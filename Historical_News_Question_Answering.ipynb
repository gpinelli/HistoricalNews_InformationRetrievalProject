{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"DRBwc_amYdiZ\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"# Download and Inspect the Collection\\n\",\n",
    "    \"\\n\",\n",
    "    \"The dataset was created from the Chronicling America collection — over 21 million digitized newspaper pages (1756–1963) curated by the Library of Congress and NEH. They used 39,330 pages (1800–1920), representing 53 US states, to ensure wide geographic and temporal coverage.\\n\",\n",
    "    \"\\n\",\n",
    "    \"Source: https://dl.acm.org/doi/pdf/10.1145/3626772.3657891\\n\",\n",
    "    \"\\n\",\n",
    "    \"GitHub: https://github.com/DataScienceUIBK/ChroniclingAmericaQA?tab=readme-ov-file\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"%pip install -r requirements.txt\\n\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Imports\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import pyterrier as pt\\n\",\n",
    "    \"import transformers\\n\",\n",
    "    \"import torch\\n\",\n",
    "    \"import nltk\\n\",\n",
    "    \"import spacy\\n\",\n",
    "    \"import shutil\\n\",\n",
    "    \"import matplotlib.pyplot as plt  # plotting library\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"from collections import defaultdict\\n\",\n",
    "    \"import re\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {\n",
    "    \"colab\": {\n",
    "     \"base_uri\": \"https://localhost:8080/\"\n",
    "    },\n",
    "    \"executionInfo\": {\n",
    "     \"elapsed\": 12366,\n",
    "     \"status\": \"ok\",\n",
    "     \"timestamp\": 1762962835550,\n",
    "     \"user\": {\n",
    "      \"displayName\": \"Georgios Peikos\",\n",
    "      \"userId\": \"04834132442165285194\"\n",
    "     },\n",
    "     \"user_tz\": -60\n",
    "    },\n",
    "    \"id\": \"4xBdfDsPYdLA\",\n",
    "    \"outputId\": \"32103be7-1880-4ccb-ea70-6800e841dec1\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"import os\\n\",\n",
    "    \"os.makedirs(\\\"data\\\", exist_ok=True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"!curl -L \\\"https://huggingface.co/datasets/Bhawna/ChroniclingAmericaQA/resolve/main/test.json?download=true\\\" -o data/test.json\\n\",\n",
    "    \"!curl -L \\\"https://huggingface.co/datasets/Bhawna/ChroniclingAmericaQA/resolve/main/train.json?download=true\\\" -o data/train.json\\n\",\n",
    "    \"!curl -L \\\"https://huggingface.co/datasets/Bhawna/ChroniclingAmericaQA/resolve/main/dev.json?download=true\\\" -o data/validation.json\\n\",\n",
    "    \"\\n\",\n",
    "    \"import json\\n\",\n",
    "    \"\\n\",\n",
    "    \"files = [\\\"data/train.json\\\", \\\"data/validation.json\\\", \\\"data/test.json\\\"]\\n\",\n",
    "    \"\\n\",\n",
    "    \"for path in files:\\n\",\n",
    "    \"    print(f\\\"\\\\n===== {path} =====\\\")\\n\",\n",
    "    \"    try:\\n\",\n",
    "    \"        with open(path, \\\"r\\\", encoding=\\\"utf-8\\\") as f:\\n\",\n",
    "    \"            # Read a few hundred characters to see what kind of JSON it is\\n\",\n",
    "    \"            head = f.read(500)\\n\",\n",
    "    \"            print(\\\"Preview of first 500 characters:\\\\n\\\")\\n\",\n",
    "    \"            print(head[:500])\\n\",\n",
    "    \"        # Try to load only part of the file\\n\",\n",
    "    \"        with open(path, \\\"r\\\", encoding=\\\"utf-8\\\") as f:\\n\",\n",
    "    \"            data = json.load(f)\\n\",\n",
    "    \"        if isinstance(data, list):\\n\",\n",
    "    \"            print(f\\\"\\\\nLoaded {len(data)} items (list).\\\")\\n\",\n",
    "    \"            print(\\\"Dictionary keys:\\\", list(data[0].keys()))\\n\",\n",
    "    \"            print(json.dumps(data[0], indent=2)[:600])\\n\",\n",
    "    \"        elif isinstance(data, dict):\\n\",\n",
    "    \"            print(\\\"\\\\nTop-level is a dictionary. Keys:\\\", list(data.keys()))\\n\",\n",
    "    \"            for k, v in data.items():\\n\",\n",
    "    \"                if isinstance(v, list):\\n\",\n",
    "    \"                    print(f\\\"Key '{k}' contains a list of {len(v)} items.\\\")\\n\",\n",
    "    \"                    if v:\\n\",\n",
    "    \"                        print(\\\"First item keys:\\\", list(v[0].keys()))\\n\",\n",
    "    \"                        print(json.dumps(v[0], indent=2)[:600])\\n\",\n",
    "    \"                        break\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            print(f\\\"Unexpected top-level type: {type(data)}\\\")\\n\",\n",
    "    \"    except Exception as e:\\n\",\n",
    "    \"        print(f\\\"Could not parse {path} as JSON: {e}\\\")\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"mylmVIP9bu8y\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"# Create the Document Collection\\n\",\n",
    "    \"\\n\",\n",
    "    \"To do that, we create a new json file that contains the 'para_id', 'context', 'raw_ocr', 'publication_date' keys, for all para_id in the collection.\\n\",\n",
    "    \"\\n\",\n",
    "    \"para_id: is the id of a paragraph of a news paper page.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {\n",
    "    \"colab\": {\n",
    "     \"base_uri\": \"https://localhost:8080/\"\n",
    "    },\n",
    "    \"executionInfo\": {\n",
    "     \"elapsed\": 17568,\n",
    "     \"status\": \"ok\",\n",
    "     \"timestamp\": 1762962853135,\n",
    "     \"user\": {\n",
    "      \"displayName\": \"Georgios Peikos\",\n",
    "      \"userId\": \"04834132442165285194\"\n",
    "     },\n",
    "     \"user_tz\": -60\n",
    "    },\n",
    "    \"id\": \"nxch4FUUbxRw\",\n",
    "    \"outputId\": \"d86e4179-defd-49d8-8b68-172c577ed825\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"import json\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"\\n\",\n",
    "    \"inputs = [\\\"data/train.json\\\", \\\"data/validation.json\\\", \\\"data/test.json\\\"]\\n\",\n",
    "    \"output = \\\"data/document_collection.json\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"def load_list_or_empty(path):\\n\",\n",
    "    \"    if not os.path.exists(path) or os.path.getsize(path) == 0:\\n\",\n",
    "    \"        print(f\\\"Skipping {path} because it is missing or empty\\\")\\n\",\n",
    "    \"        return []\\n\",\n",
    "    \"    try:\\n\",\n",
    "    \"        with open(path, \\\"r\\\", encoding=\\\"utf-8\\\") as f:\\n\",\n",
    "    \"            data = json.load(f)\\n\",\n",
    "    \"        if isinstance(data, list):\\n\",\n",
    "    \"            return data\\n\",\n",
    "    \"        print(f\\\"Skipping {path} because it is not a list at the top level\\\")\\n\",\n",
    "    \"        return []\\n\",\n",
    "    \"    except json.JSONDecodeError:\\n\",\n",
    "    \"        print(f\\\"Skipping {path} because it is not valid JSON\\\")\\n\",\n",
    "    \"        return []\\n\",\n",
    "    \"\\n\",\n",
    "    \"def project(recs):\\n\",\n",
    "    \"    out = []\\n\",\n",
    "    \"    for r in recs:\\n\",\n",
    "    \"        out.append({\\n\",\n",
    "    \"            \\\"para_id\\\": r.get(\\\"para_id\\\", \\\"\\\"),\\n\",\n",
    "    \"            \\\"context\\\": r.get(\\\"context\\\", \\\"\\\"),\\n\",\n",
    "    \"            \\\"raw_ocr\\\": r.get(\\\"raw_ocr\\\", \\\"\\\"),\\n\",\n",
    "    \"            \\\"publication_date\\\": r.get(\\\"publication_date\\\", \\\"\\\")\\n\",\n",
    "    \"        })\\n\",\n",
    "    \"    return out\\n\",\n",
    "    \"\\n\",\n",
    "    \"all_recs = []\\n\",\n",
    "    \"for p in inputs:\\n\",\n",
    "    \"    recs = load_list_or_empty(p)\\n\",\n",
    "    \"    print(f\\\"Loaded {len(recs)} records from {p}\\\")\\n\",\n",
    "    \"    all_recs.extend(project(recs))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# deduplicate by para_id keeping the first one seen\\n\",\n",
    "    \"uniq = {}\\n\",\n",
    "    \"for rec in all_recs:\\n\",\n",
    "    \"    pid = rec.get(\\\"para_id\\\", \\\"\\\")\\n\",\n",
    "    \"    if pid and pid not in uniq:\\n\",\n",
    "    \"        uniq[pid] = rec\\n\",\n",
    "    \"\\n\",\n",
    "    \"result = list(uniq.values())\\n\",\n",
    "    \"\\n\",\n",
    "    \"with open(output, \\\"w\\\", encoding=\\\"utf-8\\\") as f:\\n\",\n",
    "    \"    json.dump(result, f, ensure_ascii=False, indent=2)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Wrote {len(result)} records to {output}\\\")\\n\",\n",
    "    \"print(json.dumps(result[:3], indent=2))\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"O-9wljtri-XX\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## You should check that the collection you have matches that of the paper!\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"for path in inputs:\\n\",\n",
    "    \"    with open(path, \\\"r\\\", encoding=\\\"utf-8\\\") as f:\\n\",\n",
    "    \"        data = json.load(f)\\n\",\n",
    "    \"        df_check = pd.read_json(path)\\n\",\n",
    "    \"        print(f'Shape of {path}: {df_check.shape}')\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"The dimensions match the ones of the paper at https://github.com/DataScienceUIBK/ChroniclingAmericaQA\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"snY9dkltgMts\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"# Create the Test Queries Data Structure\\n\",\n",
    "    \"\\n\",\n",
    "    \"We keep the first 10.000 queries due to memory errors in the free colab version.\\n\",\n",
    "    \"\\n\",\n",
    "    \"To be comparable, please keep the top 10.000 queries for evaluation.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {\n",
    "    \"colab\": {\n",
    "     \"base_uri\": \"https://localhost:8080/\"\n",
    "    },\n",
    "    \"executionInfo\": {\n",
    "     \"elapsed\": 1151,\n",
    "     \"status\": \"ok\",\n",
    "     \"timestamp\": 1762962872929,\n",
    "     \"user\": {\n",
    "      \"displayName\": \"Georgios Peikos\",\n",
    "      \"userId\": \"04834132442165285194\"\n",
    "     },\n",
    "     \"user_tz\": -60\n",
    "    },\n",
    "    \"id\": \"7ZOmr1qBgRxi\",\n",
    "    \"outputId\": \"1a4cbaaa-2813-4814-e0de-aee5aab98f7c\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"import json\\n\",\n",
    "    \"import re\\n\",\n",
    "    \"import unicodedata\\n\",\n",
    "    \"import string\\n\",\n",
    "    \"\\n\",\n",
    "    \"input_file = \\\"data/test.json\\\"\\n\",\n",
    "    \"output_file = \\\"data/test_queries.json\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load the data\\n\",\n",
    "    \"with open(input_file, \\\"r\\\", encoding=\\\"utf-8\\\") as f:\\n\",\n",
    "    \"    data = json.load(f)\\n\",\n",
    "    \"\\n\",\n",
    "    \"def clean_question(text):\\n\",\n",
    "    \"    if not isinstance(text, str):\\n\",\n",
    "    \"        return \\\"\\\"\\n\",\n",
    "    \"    text = unicodedata.normalize(\\\"NFKC\\\", text)\\n\",\n",
    "    \"    text = re.sub(rf\\\"[{re.escape(string.punctuation)}]\\\", \\\" \\\", text)  # remove punctuation\\n\",\n",
    "    \"    text = re.sub(r\\\"\\\\s+\\\", \\\" \\\", text)  # collapse multiple spaces\\n\",\n",
    "    \"    return text.strip()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Extract and clean\\n\",\n",
    "    \"queries = [\\n\",\n",
    "    \"    {\\n\",\n",
    "    \"        \\\"query_id\\\": item.get(\\\"query_id\\\", \\\"\\\"),\\n\",\n",
    "    \"        \\\"question\\\": clean_question(item.get(\\\"question\\\", \\\"\\\")),\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"    for item in data\\n\",\n",
    "    \"]\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Sort by query_id (assuming numeric)\\n\",\n",
    "    \"queries = sorted(queries, key=lambda x: int(x[\\\"query_id\\\"]) if str(x[\\\"query_id\\\"]).isdigit() else x[\\\"query_id\\\"])\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Keep only the first 10,000\\n\",\n",
    "    \"queries = queries[:10000]\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Save new JSON\\n\",\n",
    "    \"with open(output_file, \\\"w\\\", encoding=\\\"utf-8\\\") as f:\\n\",\n",
    "    \"    json.dump(queries, f, ensure_ascii=False, indent=2)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Saved {len(queries)} entries to {output_file}\\\")\\n\",\n",
    "    \"print(json.dumps(queries[:3], indent=2))\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"6NyCV6oqjFS0\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"# Create the Qrels for the test set\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {\n",
    "    \"colab\": {\n",
    "     \"base_uri\": \"https://localhost:8080/\"\n",
    "    },\n",
    "    \"executionInfo\": {\n",
    "     \"elapsed\": 742,\n",
    "     \"status\": \"ok\",\n",
    "     \"timestamp\": 1762962873672,\n",
    "     \"user\": {\n",
    "      \"displayName\": \"Georgios Peikos\",\n",
    "      \"userId\": \"04834132442165285194\"\n",
    "     },\n",
    "     \"user_tz\": -60\n",
    "    },\n",
    "    \"id\": \"Lxms9bHpjIcn\",\n",
    "    \"outputId\": \"26e9db71-b590-4f5f-94db-484d857db80c\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"input_file = \\\"data/test.json\\\"\\n\",\n",
    "    \"qrels_file = \\\"data/test_qrels.json\\\"\\n\",\n",
    "    \"answers_file = \\\"data/test_query_answers.json\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load the data\\n\",\n",
    "    \"with open(input_file, \\\"r\\\", encoding=\\\"utf-8\\\") as f:\\n\",\n",
    "    \"    data = json.load(f)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Build the qrels file: query_id, iteration=0, para_id, relevance=1\\n\",\n",
    "    \"qrels = [\\n\",\n",
    "    \"    {\\n\",\n",
    "    \"        \\\"query_id\\\": item.get(\\\"query_id\\\", \\\"\\\"),\\n\",\n",
    "    \"        \\\"iteration\\\": 0,\\n\",\n",
    "    \"        \\\"para_id\\\": item.get(\\\"para_id\\\", \\\"\\\"),\\n\",\n",
    "    \"        \\\"relevance\\\": 1\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"    for item in data\\n\",\n",
    "    \"]\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Build the query_answers file: same plus answer and org_answer\\n\",\n",
    "    \"query_answers = [\\n\",\n",
    "    \"    {\\n\",\n",
    "    \"        \\\"query_id\\\": item.get(\\\"query_id\\\", \\\"\\\"),\\n\",\n",
    "    \"        \\\"iteration\\\": 0,\\n\",\n",
    "    \"        \\\"para_id\\\": item.get(\\\"para_id\\\", \\\"\\\"),\\n\",\n",
    "    \"        \\\"relevance\\\": 1,\\n\",\n",
    "    \"        \\\"answer\\\": item.get(\\\"answer\\\", \\\"\\\"),\\n\",\n",
    "    \"        \\\"org_answer\\\": item.get(\\\"org_answer\\\", \\\"\\\")\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"    for item in data\\n\",\n",
    "    \"]\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Save both files\\n\",\n",
    "    \"with open(qrels_file, \\\"w\\\", encoding=\\\"utf-8\\\") as f:\\n\",\n",
    "    \"    json.dump(qrels, f, ensure_ascii=False, indent=2)\\n\",\n",
    "    \"\\n\",\n",
    "    \"with open(answers_file, \\\"w\\\", encoding=\\\"utf-8\\\") as f:\\n\",\n",
    "    \"    json.dump(query_answers, f, ensure_ascii=False, indent=2)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Saved {len(qrels)} entries to {qrels_file}\\\")\\n\",\n",
    "    \"print(f\\\"Saved {len(query_answers)} entries to {answers_file}\\\")\\n\",\n",
    "    \"print(\\\"Sample qrels entry:\\\", qrels[0])\\n\",\n",
    "    \"print(\\\"Sample query_answers entry:\\\", query_answers[0])\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"M7vkoP010nIF\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"# Retrieval\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Extract data from json files\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"input_files = ['data/document_collection.json', 'data/test.json', 'data/test_qrels.json', 'data/test_queries.json', 'data/test_query_answers.json', 'data/train.json', 'data/validation.json']\\n\",\n",
    "    \"\\n\",\n",
    "    \"dataframes = {}\\n\",\n",
    "    \"for input_file in input_files:\\n\",\n",
    "    \"    with open(input_file, \\\"r\\\", encoding=\\\"utf-8\\\") as f:\\n\",\n",
    "    \"        data = json.load(f)\\n\",\n",
    "    \"        dataframes[input_file] = pd.read_json(input_file)\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"Let's visualize data and analyze them\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"dataframes['data/document_collection.json']\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"dataframes['data/train.json']\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"**NOTE: in `data/document_collection.json` the rows are already deduplicated**\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### _Preprocessing_\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"#### **Linguistic Processing**\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"##### Normalization\\n\",\n",
    "    \"We lowercase everything and remove all special characters/tags\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"--> 1st step normalization\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"def normalize_text1(text):\\n\",\n",
    "    \"    if not isinstance(text, str):\\n\",\n",
    "    \"        return text\\n\",\n",
    "    \"    text = unicodedata.normalize('NFKC', text)\\n\",\n",
    "    \"    text = re.sub(r'<[^>]+>', ' ', text) # HTML\\n\",\n",
    "    \"    text = re.sub(r'\\\\s+', ' ', text).strip() # multiple white spaces\\n\",\n",
    "    \"    return text\\n\",\n",
    "    \"\\n\",\n",
    "    \"# in caso togliessimo la NER vanno tolti i commenti nella funzione qui sopra\\n\",\n",
    "    \"\\n\",\n",
    "    \"docColl = dataframes['data/document_collection.json']\\n\",\n",
    "    \"docColl_contNorm1 = docColl['context'].apply(normalize_text1)\\n\",\n",
    "    \"docColl_ocrNorm1 = docColl['raw_ocr'].apply(normalize_text1)\\n\",\n",
    "    \"docColl_Norm1 = docColl.copy()\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"docColl_Norm1['context'] = docColl_contNorm1\\n\",\n",
    "    \"docColl_Norm1['raw_ocr'] = docColl_ocrNorm1\\n\",\n",
    "    \"docColl_Norm1.head(25)\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"docColl['context'].compare(docColl_Norm1['context'])\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"print(docColl['context'].iloc[2])\\n\",\n",
    "    \"print(docColl_Norm1['context'].iloc[2])\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"##### NER\\n\",\n",
    "    \"We want to identify named-entities before lemmatizing the text, so that we do not lose any entity by \\\"shrinking\\\" words to their base forms.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"import os\\n\",\n",
    "    \"import torch\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"from transformers import AutoTokenizer, pipeline\\n\",\n",
    "    \"from tqdm import tqdm\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Settings per far runnare su gpu (se possibile)\\n\",\n",
    "    \"os.environ[\\\"PYTORCH_ENABLE_MPS_FALLBACK\\\"] = \\\"1\\\"\\n\",\n",
    "    \"device = \\\"mps\\\" if torch.backends.mps.is_available() else \\\"cpu\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"MODEL_NAME = \\\"impresso-project/ner-stacked-bert-multilingual-light\\\"\\n\",\n",
    "    \"tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\\n\",\n",
    "    \"\\n\",\n",
    "    \"ner_pipeline = pipeline(\\n\",\n",
    "    \"    model=MODEL_NAME,\\n\",\n",
    "    \"    tokenizer=tokenizer,\\n\",\n",
    "    \"    trust_remote_code=True,\\n\",\n",
    "    \"    device=device)\\n\",\n",
    "    \"\\n\",\n",
    "    \"def run_impresso_ner(text_series):\\n\",\n",
    "    \"    results = []\\n\",\n",
    "    \"    for text in tqdm(text_series): # tqdm per vedere i progressi nelle ore di run\\n\",\n",
    "    \"        text_str = str(text)\\n\",\n",
    "    \"        if not text_str.strip(): # per testi vuoti\\n\",\n",
    "    \"            results.append([])\\n\",\n",
    "    \"            continue\\n\",\n",
    "    \"\\n\",\n",
    "    \"        words = text_str.split()\\n\",\n",
    "    \"\\n\",\n",
    "    \"        try:\\n\",\n",
    "    \"            entities = ner_pipeline(text_str, tokens=words)\\n\",\n",
    "    \"            results.append(entities)\\n\",\n",
    "    \"        except Exception as e:\\n\",\n",
    "    \"            print(f\\\"Errore su un documento: {e}\\\")\\n\",\n",
    "    \"            results.append([]) # per non farlo bloccare se ha un errore\\n\",\n",
    "    \"    return results\\n\",\n",
    "    \"\\n\",\n",
    "    \"OUTPUT_FILE = \\\"data/ner_results_cache.parquet\\\"\\n\",\n",
    "    \"if os.path.exists(OUTPUT_FILE):\\n\",\n",
    "    \"    cached_data = pd.read_parquet(OUTPUT_FILE)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    docColl_Norm1['ner_entities_context'] = cached_data['ner_entities_context']\\n\",\n",
    "    \"    docColl_Norm1['ner_entities_ocr'] = cached_data['ner_entities_ocr']\\n\",\n",
    "    \"\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    # context\\n\",\n",
    "    \"    docColl_Norm1['ner_entities_context'] = run_impresso_ner(docColl_Norm1['context'])\\n\",\n",
    "    \"    # OCR\\n\",\n",
    "    \"    docColl_Norm1['ner_entities_ocr'] = run_impresso_ner(docColl_Norm1['raw_ocr'])\\n\",\n",
    "    \"    # salvataggio su file esterno\\n\",\n",
    "    \"    docColl_Norm1[['ner_entities_context', 'ner_entities_ocr']].to_parquet(OUTPUT_FILE)\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"docColl_Norm1\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"docColl_ner = docColl_Norm1.copy()\\n\",\n",
    "    \"docColl_ner[['context', 'raw_ocr', 'ner_entities_context', 'ner_entities_ocr']]\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"--> 2nd step normalization\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"def normalize_text2(text):\\n\",\n",
    "    \"    if not isinstance(text, str):\\n\",\n",
    "    \"        return text\\n\",\n",
    "    \"    text = text.lower() # lowercase\\n\",\n",
    "    \"    text = re.sub(r'[^a-z0-9\\\\s]', '', text) # punctuations\\n\",\n",
    "    \"    text = re.sub(r'[^\\\\w\\\\s]', ' ', text) # any other punctuation mark\\n\",\n",
    "    \"    text = re.sub(r'\\\\s+', ' ', text).strip() # white spaces again\\n\",\n",
    "    \"    return text\\n\",\n",
    "    \"\\n\",\n",
    "    \"docColl_ner['context'] = docColl_ner['context'].apply(normalize_text2)\\n\",\n",
    "    \"docColl_ner['raw_ocr'] = docColl_ner['raw_ocr'].apply(normalize_text2)\\n\",\n",
    "    \"docColl_Norm2 = docColl_ner.copy()\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": \"docColl_Norm2['context'].iloc[2]\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"##### Lemmatization\\n\",\n",
    "    \"Placed here to standardize semantically the sentences in the documents\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": [\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import spacy\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"import gc\\n\",\n",
    "    \"from tqdm import tqdm\\n\",\n",
    "    \"\\n\",\n",
    "    \"OUTPUT_FILE_LEMM = \\\"data/lemmatization_results_cache.parquet\\\"\\n\",\n",
    "    \"columns_to_process = ['context', 'raw_ocr']\\n\",\n",
    "    \"\\n\",\n",
    "    \"# 1. Caricamento ottimizzato: disabilitiamo tutto ciò che non serve alla lemmatizzazione\\n\",\n",
    "    \"# Teniamo il 'tagger' e 'attribute_ruler' perché necessari per lemmi accurati\\n\",\n",
    "    \"try:\\n\",\n",
    "    \"    nlp = spacy.load(\\\"en_core_web_sm\\\", disable=['parser', 'ner'])\\n\",\n",
    "    \"except OSError:\\n\",\n",
    "    \"    from spacy.cli import download\\n\",\n",
    "    \"    download(\\\"en_core_web_sm\\\")\\n\",\n",
    "    \"    nlp = spacy.load(\\\"en_core_web_sm\\\", disable=['parser', 'ner'])\\n\",\n",
    "    \"\\n\",\n",
    "    \"def run_lemmatization(df, columns):\\n\",\n",
    "    \"    temp_df = pd.DataFrame(index=df.index)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    for col in columns:\\n\",\n",
    "    \"        if col in df.columns:\\n\",\n",
    "    \"            print(f\\\"\\\\n--- Elaborazione colonna: '{col}' ---\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"            # 2. Utilizziamo un generatore per non duplicare i dati in memoria\\n\",\n",
    "    \"            def text_generator():\\n\",\n",
    "    \"                for text in df[col]:\\n\",\n",
    "    \"                    yield str(text) if pd.notnull(text) else \\\"\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"            processed_texts = []\\n\",\n",
    "    \"\\n\",\n",
    "    \"            # 3. Riduciamo n_process o rimuoviamolo se la RAM è poca.\\n\",\n",
    "    \"            # Aumentiamo il batch_size per compensare la velocità.\\n\",\n",
    "    \"            for doc in tqdm(nlp.pipe(text_generator(), batch_size=1000),\\n\",\n",
    "    \"                            total=len(df),\\n\",\n",
    "    \"                            desc=f\\\"Lemmatizing {col}\\\"):\\n\",\n",
    "    \"                # Usiamo una list comprehension rapida\\n\",\n",
    "    \"                lemmas = \\\" \\\".join([token.lemma_ for token in doc if not token.is_space])\\n\",\n",
    "    \"                processed_texts.append(lemmas)\\n\",\n",
    "    \"\\n\",\n",
    "    \"            temp_df[f\\\"{col}_lemma\\\"] = processed_texts\\n\",\n",
    "    \"\\n\",\n",
    "    \"            # 4. Pulizia manuale della memoria tra una colonna e l'altra\\n\",\n",
    "    \"            del processed_texts\\n\",\n",
    "    \"            gc.collect()\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            print(f\\\" Avviso: Colonna '{col}' non trovata.\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"    return temp_df\\n\",\n",
    "    \"\\n\",\n",
    "    \"if os.path.exists(OUTPUT_FILE_LEMM):\\n\",\n",
    "    \"    cached_lemm = pd.read_parquet(OUTPUT_FILE_LEMM)\\n\",\n",
    "    \"    docColl_Norm2['context_lemma'] = cached_lemm['context_lemma']\\n\",\n",
    "    \"    docColl_Norm2['raw_ocr_lemma'] = cached_lemm['raw_ocr_lemma']\\n\",\n",
    "    \"\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    # context\\n\",\n",
    "    \"    lemmatized_df = run_lemmatization(docColl_Norm2[['context', 'raw_ocr']], columns_to_process)\\n\",\n",
    "    \"    docColl_Norm2['context_lemma'] = lemmatized_df['context_lemma']\\n\",\n",
    "    \"    docColl_Norm2['raw_ocr_lemma'] = lemmatized_df['raw_ocr_lemma']\\n\",\n",
    "    \"    # salvataggio su file esterno\\n\",\n",
    "    \"    docColl_Norm2[['context_lemma', 'raw_ocr_lemma']].to_parquet(OUTPUT_FILE_LEMM)\\n\",\n",
    "    \"\\n\",\n",
    "    \"docColl_Lemm = docColl_Norm2\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": \"docColl_Lemm['context_lemma'].iloc[0]\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": \"docColl_Lemm['context'].iloc[0]\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"##### N-gram based tokenization\\n\",\n",
    "    \"Important to place it after normalization, in this tokenization can be integrated a NER-aware part so that \\\"the tokenization is also entity-guided\\\"\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": [\n",
    "    \"def ner_aware_string_processor(row, text_col, ner_col):\\n\",\n",
    "    \"\\n\",\n",
    "    \"    text = row.get(text_col, \\\"\\\")\\n\",\n",
    "    \"    entities = row.get(ner_col, [])\\n\",\n",
    "    \"\\n\",\n",
    "    \"    if not isinstance(text, str) or not text.strip():\\n\",\n",
    "    \"        return \\\"\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"    if isinstance(entities, list) and len(entities) > 0:\\n\",\n",
    "    \"        entity_texts = sorted(\\n\",\n",
    "    \"            [ent['surface'].lower().strip() for ent in entities if ent.get('confidence_ner', 0) >= 0.5],\\n\",\n",
    "    \"            key=len,\\n\",\n",
    "    \"            reverse=True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"        for ent_text in entity_texts:\\n\",\n",
    "    \"            if \\\" \\\" in ent_text:\\n\",\n",
    "    \"                glued_ent = ent_text.replace(\\\" \\\", \\\"_\\\")\\n\",\n",
    "    \"                text = text.replace(ent_text, glued_ent)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    tokens = text.split()\\n\",\n",
    "    \"    return \\\" \\\".join(tokens)\\n\",\n",
    "    \"\\n\",\n",
    "    \"docColl_tok = docColl_Lemm.copy()\\n\",\n",
    "    \"\\n\",\n",
    "    \"columns_map = [\\n\",\n",
    "    \"    ('context_lemma', 'ner_entities_context', 'context_bigrams'),\\n\",\n",
    "    \"    ('raw_ocr_lemma', 'ner_entities_ocr',     'raw_ocr_bigrams')\\n\",\n",
    "    \"]\\n\",\n",
    "    \"\\n\",\n",
    "    \"for text_col, ner_col, new_col in columns_map:\\n\",\n",
    "    \"    if text_col in docColl_tok.columns and ner_col in docColl_tok.columns:\\n\",\n",
    "    \"\\n\",\n",
    "    \"        docColl_tok[new_col] = docColl_tok.progress_apply(\\n\",\n",
    "    \"            lambda row: ner_aware_string_processor(row, text_col, ner_col),\\n\",\n",
    "    \"            axis=1 )\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": \"docColl_tok['context_bigrams'].iloc[0]\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"da qui dovrebbe uscire il dataframe chiamato docColl_tok\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### _Multi-field Indexing_\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"--> Indexing con PyTerrier usando un generator\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# qui assumiamo che le celle create dal NER siano oggetti di tipo dizionario\\n\",\n",
    "    \"def createGenerator(df, context=True):\\n\",\n",
    "    \"    # context\\n\",\n",
    "    \"    if context:\\n\",\n",
    "    \"        for _, row in df.iterrows():\\n\",\n",
    "    \"            # togliamo lOffset and rOffset\\n\",\n",
    "    \"            clean_ents = []\\n\",\n",
    "    \"            for ent in row['ner_entities_context']:\\n\",\n",
    "    \"                cleaned = {k: v for k, v in ent.items() if k not in ['lOffset', 'rOffset']}\\n\",\n",
    "    \"                clean_ents.append(cleaned)\\n\",\n",
    "    \"\\n\",\n",
    "    \"            search_terms = []\\n\",\n",
    "    \"            for e in clean_ents:\\n\",\n",
    "    \"                #search_terms.append(e.get('name', ''))\\n\",\n",
    "    \"                #search_terms.append(e.get('title', ''))\\n\",\n",
    "    \"                # da capire se vogliamo che siano searchable, dato che surface contiene già il testo a cui è associata la entity\\n\",\n",
    "    \"                search_terms.append(e.get('surface', ''))\\n\",\n",
    "    \"\\n\",\n",
    "    \"            ent_text = \\\" \\\".join(filter(None, search_terms)) # questa riga ha senso solo se prendiamo anche 'name' e 'title'\\n\",\n",
    "    \"                                                                                           # se no ent_text va assegnato a e.get('surface', ' ')\\n\",\n",
    "    \"\\n\",\n",
    "    \"            meta_json = json.dumps(clean_ents) # facciamo diventare tutti i metadati una stringa in forma json (non un oggetto dizionario, proprio una stringa)\\n\",\n",
    "    \"\\n\",\n",
    "    \"            yield { # serve per lo stream dei dati quando viene chiamata createGenerator dentro indexer.index(•)\\n\",\n",
    "    \"                \\\"docno\\\": str(row['para_id']),\\n\",\n",
    "    \"                \\\"text\\\": row['context_bigrams'],\\n\",\n",
    "    \"                \\\"entities\\\": ent_text, # entità searchable\\n\",\n",
    "    \"                \\\"entity_json\\\": meta_json}\\n\",\n",
    "    \"    # OCR\\n\",\n",
    "    \"    if not context:\\n\",\n",
    "    \"        for _, row in df.iterrows():\\n\",\n",
    "    \"            # togliamo lOffset and rOffset\\n\",\n",
    "    \"            clean_ents = []\\n\",\n",
    "    \"            for ent in row['ner_entities_ocr']:\\n\",\n",
    "    \"                cleaned = {k: v for k, v in ent.items() if k not in ['lOffset', 'rOffset']}\\n\",\n",
    "    \"                clean_ents.append(cleaned)\\n\",\n",
    "    \"\\n\",\n",
    "    \"            search_terms = []\\n\",\n",
    "    \"            for e in clean_ents:\\n\",\n",
    "    \"                #search_terms.append(e.get('name', ''))\\n\",\n",
    "    \"                #search_terms.append(e.get('title', ''))\\n\",\n",
    "    \"                # da capire se vogliamo che siano searchable, dato che surface contiene già il testo a cui è associata la entity\\n\",\n",
    "    \"                search_terms.append(e.get('surface', ''))\\n\",\n",
    "    \"\\n\",\n",
    "    \"            ent_text = \\\" \\\".join(filter(None, search_terms)) # questa riga ha senso solo se prendiamo anche 'name' e 'title'\\n\",\n",
    "    \"                                                                                           # se no ent_text va assegnato a e.get('surface', ' ')\\n\",\n",
    "    \"\\n\",\n",
    "    \"            meta_json = json.dumps(clean_ents) # facciamo diventare tutti i metadati una stringa in forma json (non un oggetto dizionario, proprio una stringa)\\n\",\n",
    "    \"\\n\",\n",
    "    \"            yield { # serve per lo stream dei dati quando viene chiamata createGenerator dentro indexer.index(•)\\n\",\n",
    "    \"                \\\"docno\\\": str(row['para_id']),\\n\",\n",
    "    \"                \\\"text\\\": row['raw_ocr_bigrams'],\\n\",\n",
    "    \"                \\\"entities\\\": ent_text, # entità searchable\\n\",\n",
    "    \"                \\\"entity_json\\\": meta_json}\\n\",\n",
    "    \"\\n\",\n",
    "    \"contextIndex_path = 'data/docColl_context-index'\\n\",\n",
    "    \"ocrIndex_path = 'data/docColl_ocr-index'\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"if os.path.exists(contextIndex_path):\\n\",\n",
    "    \"    shutil.rmtree(contextIndex_path)\\n\",\n",
    "    \"\\n\",\n",
    "    \"indexerCont = pt.IterDictIndexer(\\n\",\n",
    "    \"    'entity_index',\\n\",\n",
    "    \"    fields=['text', 'entities'],\\n\",\n",
    "    \"    meta={'docno', 'entity_json'})\\n\",\n",
    "    \"\\n\",\n",
    "    \"indexrefCont = indexerCont.index(createGenerator(docColl_tok, context=True))\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"if os.path.exists(ocrIndex_path):\\n\",\n",
    "    \"    shutil.rmtree(ocrIndex_path)\\n\",\n",
    "    \"\\n\",\n",
    "    \"indexerOCR = pt.IterDictIndexer(\\n\",\n",
    "    \"    'entity_index',\\n\",\n",
    "    \"    fields=['text', 'entities'],\\n\",\n",
    "    \"    meta={'docno', 'entity_json'})\\n\",\n",
    "    \"\\n\",\n",
    "    \"indexrefOCR = indexerOCR.index(createGenerator(docColl_tok, context=False))\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"#### Statistics about the indexed documents\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"indexCont = pt.IndexFactory.of(indexrefCont)\\n\",\n",
    "    \"stats = indexCont.getCollectionStatistics()\\n\",\n",
    "    \"print('Index folder:', contextIndex_path)\\n\",\n",
    "    \"print('Number of documents:', stats.getNumberOfDocuments())\\n\",\n",
    "    \"print('Number of postings:', stats.getNumberOfPostings())\\n\",\n",
    "    \"print('Number of tokens:', stats.getNumberOfTokens())\\n\",\n",
    "    \"print('Number of unique terms:', stats.getNumberOfUniqueTerms())\\n\",\n",
    "    \"print('Average document length:', stats.getAverageDocumentLength())\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"indexOCR = pt.IndexFactory.of(indexrefOCR)\\n\",\n",
    "    \"stats = indexOCR.getCollectionStatistics()\\n\",\n",
    "    \"print('Index folder:', contextIndex_path)\\n\",\n",
    "    \"print('Number of documents:', stats.getNumberOfDocuments())\\n\",\n",
    "    \"print('Number of postings:', stats.getNumberOfPostings())\\n\",\n",
    "    \"print('Number of tokens:', stats.getNumberOfTokens())\\n\",\n",
    "    \"print('Number of unique terms:', stats.getNumberOfUniqueTerms())\\n\",\n",
    "    \"print('Average document length:', stats.getAverageDocumentLength())\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"#### Query analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"display(queries.head(10))\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"--> da scrivere commento riguardo l'analisi delle queries\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"#### Qrels analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"display(qrels.sample(10))\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# stats for the qrels\\n\",\n",
    "    \"# Count how many relevance assessments each query has\\n\",\n",
    "    \"counts = qrels.groupby(\\\"query_id\\\")[\\\"para_id\\\"].count()  # group by query id and count documents\\n\",\n",
    "    \"print('Overall Statistics')\\n\",\n",
    "    \"print(counts.describe())  # show a summary of the count distribution\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Plot how many relevance assessments each query received\\n\",\n",
    "    \"plt.figure()  # create a new figure\\n\",\n",
    "    \"counts.plot(kind='hist')  # histogram showing distribution of judgment counts\\n\",\n",
    "    \"plt.xlabel('Number of relevance assessments per query')  # label for x-axis\\n\",\n",
    "    \"plt.ylabel('Number of queries')  # label for y-axis\\n\",\n",
    "    \"plt.title('Relevance assessment distribution')  # title of the plot\\n\",\n",
    "    \"plt.show()  # display the plot\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Show the queries with the highest number of relevance assessments\\n\",\n",
    "    \"counts.sort_values(ascending=False).head()  # top queries by number of judgments\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Count how many times each relevance label occurs overall\\n\",\n",
    "    \"qrels['relevance'].value_counts()  # distribution of relevance scores (e.g., 0, 1, 2, etc.)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Plot the label distribution as a histogram\\n\",\n",
    "    \"plt.figure()  # create a new figure\\n\",\n",
    "    \"qrels['relevance'].plot(kind='hist')  # histogram of relevance labels\\n\",\n",
    "    \"plt.xlabel('Relevance score')  # label for x-axis\\n\",\n",
    "    \"plt.ylabel('Frequency')  # label for y-axis\\n\",\n",
    "    \"plt.title('Relevance score distribution')  # title of the plot\\n\",\n",
    "    \"plt.show()  # display the plot\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"--> commento riguardo l'analisi delle qrels\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Phase I - Topical relevance-based retrieval\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### **BM25 Retrieval from raw OCR (baseline 1)**\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"bm25ocr = pt.terrier.Retriever(indexrefOCR, wmodel='BM25', ) # dovremmo usare un BM25F? per dividere i fields di ricerca (secondo me si)\\n\",\n",
    "    \"res_bm25ocr = bm25ocr.transform()\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### **BM25 Retrieval from corrected OCR (baseline 2)**\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### **BM25 Retrieval from both raw and corrected OCR using RRF formula (baseline 3)**\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"colab\": {\n",
    "   \"authorship_tag\": \"ABX9TyNUuc82OtGicqd8vHTH8YSN\",\n",
    "   \"provenance\": [],\n",
    "   \"toc_visible\": true\n",
    "  },\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3 (ipykernel)\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.11.12\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 0\n",
    "}\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T17:02:32.245792Z",
     "start_time": "2026-01-11T17:02:31.831720Z"
    }
   },
   "cell_type": "code",
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"DRBwc_amYdiZ\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"# Download and Inspect the Collection\\n\",\n",
    "    \"\\n\",\n",
    "    \"The dataset was created from the Chronicling America collection — over 21 million digitized newspaper pages (1756–1963) curated by the Library of Congress and NEH. They used 39,330 pages (1800–1920), representing 53 US states, to ensure wide geographic and temporal coverage.\\n\",\n",
    "    \"\\n\",\n",
    "    \"Source: https://dl.acm.org/doi/pdf/10.1145/3626772.3657891\\n\",\n",
    "    \"\\n\",\n",
    "    \"GitHub: https://github.com/DataScienceUIBK/ChroniclingAmericaQA?tab=readme-ov-file\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"%pip install -r requirements.txt\\n\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Imports\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import pyterrier as pt\\n\",\n",
    "    \"import transformers\\n\",\n",
    "    \"import torch\\n\",\n",
    "    \"import nltk\\n\",\n",
    "    \"import spacy\\n\",\n",
    "    \"import shutil\\n\",\n",
    "    \"import matplotlib.pyplot as plt  # plotting library\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"from collections import defaultdict\\n\",\n",
    "    \"import re\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {\n",
    "    \"colab\": {\n",
    "     \"base_uri\": \"https://localhost:8080/\"\n",
    "    },\n",
    "    \"executionInfo\": {\n",
    "     \"elapsed\": 12366,\n",
    "     \"status\": \"ok\",\n",
    "     \"timestamp\": 1762962835550,\n",
    "     \"user\": {\n",
    "      \"displayName\": \"Georgios Peikos\",\n",
    "      \"userId\": \"04834132442165285194\"\n",
    "     },\n",
    "     \"user_tz\": -60\n",
    "    },\n",
    "    \"id\": \"4xBdfDsPYdLA\",\n",
    "    \"outputId\": \"32103be7-1880-4ccb-ea70-6800e841dec1\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"import os\\n\",\n",
    "    \"os.makedirs(\\\"data\\\", exist_ok=True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"!curl -L \\\"https://huggingface.co/datasets/Bhawna/ChroniclingAmericaQA/resolve/main/test.json?download=true\\\" -o data/test.json\\n\",\n",
    "    \"!curl -L \\\"https://huggingface.co/datasets/Bhawna/ChroniclingAmericaQA/resolve/main/train.json?download=true\\\" -o data/train.json\\n\",\n",
    "    \"!curl -L \\\"https://huggingface.co/datasets/Bhawna/ChroniclingAmericaQA/resolve/main/dev.json?download=true\\\" -o data/validation.json\\n\",\n",
    "    \"\\n\",\n",
    "    \"import json\\n\",\n",
    "    \"\\n\",\n",
    "    \"files = [\\\"data/train.json\\\", \\\"data/validation.json\\\", \\\"data/test.json\\\"]\\n\",\n",
    "    \"\\n\",\n",
    "    \"for path in files:\\n\",\n",
    "    \"    print(f\\\"\\\\n===== {path} =====\\\")\\n\",\n",
    "    \"    try:\\n\",\n",
    "    \"        with open(path, \\\"r\\\", encoding=\\\"utf-8\\\") as f:\\n\",\n",
    "    \"            # Read a few hundred characters to see what kind of JSON it is\\n\",\n",
    "    \"            head = f.read(500)\\n\",\n",
    "    \"            print(\\\"Preview of first 500 characters:\\\\n\\\")\\n\",\n",
    "    \"            print(head[:500])\\n\",\n",
    "    \"        # Try to load only part of the file\\n\",\n",
    "    \"        with open(path, \\\"r\\\", encoding=\\\"utf-8\\\") as f:\\n\",\n",
    "    \"            data = json.load(f)\\n\",\n",
    "    \"        if isinstance(data, list):\\n\",\n",
    "    \"            print(f\\\"\\\\nLoaded {len(data)} items (list).\\\")\\n\",\n",
    "    \"            print(\\\"Dictionary keys:\\\", list(data[0].keys()))\\n\",\n",
    "    \"            print(json.dumps(data[0], indent=2)[:600])\\n\",\n",
    "    \"        elif isinstance(data, dict):\\n\",\n",
    "    \"            print(\\\"\\\\nTop-level is a dictionary. Keys:\\\", list(data.keys()))\\n\",\n",
    "    \"            for k, v in data.items():\\n\",\n",
    "    \"                if isinstance(v, list):\\n\",\n",
    "    \"                    print(f\\\"Key '{k}' contains a list of {len(v)} items.\\\")\\n\",\n",
    "    \"                    if v:\\n\",\n",
    "    \"                        print(\\\"First item keys:\\\", list(v[0].keys()))\\n\",\n",
    "    \"                        print(json.dumps(v[0], indent=2)[:600])\\n\",\n",
    "    \"                        break\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            print(f\\\"Unexpected top-level type: {type(data)}\\\")\\n\",\n",
    "    \"    except Exception as e:\\n\",\n",
    "    \"        print(f\\\"Could not parse {path} as JSON: {e}\\\")\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"mylmVIP9bu8y\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"# Create the Document Collection\\n\",\n",
    "    \"\\n\",\n",
    "    \"To do that, we create a new json file that contains the 'para_id', 'context', 'raw_ocr', 'publication_date' keys, for all para_id in the collection.\\n\",\n",
    "    \"\\n\",\n",
    "    \"para_id: is the id of a paragraph of a news paper page.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {\n",
    "    \"colab\": {\n",
    "     \"base_uri\": \"https://localhost:8080/\"\n",
    "    },\n",
    "    \"executionInfo\": {\n",
    "     \"elapsed\": 17568,\n",
    "     \"status\": \"ok\",\n",
    "     \"timestamp\": 1762962853135,\n",
    "     \"user\": {\n",
    "      \"displayName\": \"Georgios Peikos\",\n",
    "      \"userId\": \"04834132442165285194\"\n",
    "     },\n",
    "     \"user_tz\": -60\n",
    "    },\n",
    "    \"id\": \"nxch4FUUbxRw\",\n",
    "    \"outputId\": \"d86e4179-defd-49d8-8b68-172c577ed825\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"import json\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"\\n\",\n",
    "    \"inputs = [\\\"data/train.json\\\", \\\"data/validation.json\\\", \\\"data/test.json\\\"]\\n\",\n",
    "    \"output = \\\"data/document_collection.json\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"def load_list_or_empty(path):\\n\",\n",
    "    \"    if not os.path.exists(path) or os.path.getsize(path) == 0:\\n\",\n",
    "    \"        print(f\\\"Skipping {path} because it is missing or empty\\\")\\n\",\n",
    "    \"        return []\\n\",\n",
    "    \"    try:\\n\",\n",
    "    \"        with open(path, \\\"r\\\", encoding=\\\"utf-8\\\") as f:\\n\",\n",
    "    \"            data = json.load(f)\\n\",\n",
    "    \"        if isinstance(data, list):\\n\",\n",
    "    \"            return data\\n\",\n",
    "    \"        print(f\\\"Skipping {path} because it is not a list at the top level\\\")\\n\",\n",
    "    \"        return []\\n\",\n",
    "    \"    except json.JSONDecodeError:\\n\",\n",
    "    \"        print(f\\\"Skipping {path} because it is not valid JSON\\\")\\n\",\n",
    "    \"        return []\\n\",\n",
    "    \"\\n\",\n",
    "    \"def project(recs):\\n\",\n",
    "    \"    out = []\\n\",\n",
    "    \"    for r in recs:\\n\",\n",
    "    \"        out.append({\\n\",\n",
    "    \"            \\\"para_id\\\": r.get(\\\"para_id\\\", \\\"\\\"),\\n\",\n",
    "    \"            \\\"context\\\": r.get(\\\"context\\\", \\\"\\\"),\\n\",\n",
    "    \"            \\\"raw_ocr\\\": r.get(\\\"raw_ocr\\\", \\\"\\\"),\\n\",\n",
    "    \"            \\\"publication_date\\\": r.get(\\\"publication_date\\\", \\\"\\\")\\n\",\n",
    "    \"        })\\n\",\n",
    "    \"    return out\\n\",\n",
    "    \"\\n\",\n",
    "    \"all_recs = []\\n\",\n",
    "    \"for p in inputs:\\n\",\n",
    "    \"    recs = load_list_or_empty(p)\\n\",\n",
    "    \"    print(f\\\"Loaded {len(recs)} records from {p}\\\")\\n\",\n",
    "    \"    all_recs.extend(project(recs))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# deduplicate by para_id keeping the first one seen\\n\",\n",
    "    \"uniq = {}\\n\",\n",
    "    \"for rec in all_recs:\\n\",\n",
    "    \"    pid = rec.get(\\\"para_id\\\", \\\"\\\")\\n\",\n",
    "    \"    if pid and pid not in uniq:\\n\",\n",
    "    \"        uniq[pid] = rec\\n\",\n",
    "    \"\\n\",\n",
    "    \"result = list(uniq.values())\\n\",\n",
    "    \"\\n\",\n",
    "    \"with open(output, \\\"w\\\", encoding=\\\"utf-8\\\") as f:\\n\",\n",
    "    \"    json.dump(result, f, ensure_ascii=False, indent=2)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Wrote {len(result)} records to {output}\\\")\\n\",\n",
    "    \"print(json.dumps(result[:3], indent=2))\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"O-9wljtri-XX\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## You should check that the collection you have matches that of the paper!\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"for path in inputs:\\n\",\n",
    "    \"    with open(path, \\\"r\\\", encoding=\\\"utf-8\\\") as f:\\n\",\n",
    "    \"        data = json.load(f)\\n\",\n",
    "    \"        df_check = pd.read_json(path)\\n\",\n",
    "    \"        print(f'Shape of {path}: {df_check.shape}')\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"The dimensions match the ones of the paper at https://github.com/DataScienceUIBK/ChroniclingAmericaQA\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"snY9dkltgMts\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"# Create the Test Queries Data Structure\\n\",\n",
    "    \"\\n\",\n",
    "    \"We keep the first 10.000 queries due to memory errors in the free colab version.\\n\",\n",
    "    \"\\n\",\n",
    "    \"To be comparable, please keep the top 10.000 queries for evaluation.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {\n",
    "    \"colab\": {\n",
    "     \"base_uri\": \"https://localhost:8080/\"\n",
    "    },\n",
    "    \"executionInfo\": {\n",
    "     \"elapsed\": 1151,\n",
    "     \"status\": \"ok\",\n",
    "     \"timestamp\": 1762962872929,\n",
    "     \"user\": {\n",
    "      \"displayName\": \"Georgios Peikos\",\n",
    "      \"userId\": \"04834132442165285194\"\n",
    "     },\n",
    "     \"user_tz\": -60\n",
    "    },\n",
    "    \"id\": \"7ZOmr1qBgRxi\",\n",
    "    \"outputId\": \"1a4cbaaa-2813-4814-e0de-aee5aab98f7c\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"import json\\n\",\n",
    "    \"import re\\n\",\n",
    "    \"import unicodedata\\n\",\n",
    "    \"import string\\n\",\n",
    "    \"\\n\",\n",
    "    \"input_file = \\\"data/test.json\\\"\\n\",\n",
    "    \"output_file = \\\"data/test_queries.json\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load the data\\n\",\n",
    "    \"with open(input_file, \\\"r\\\", encoding=\\\"utf-8\\\") as f:\\n\",\n",
    "    \"    data = json.load(f)\\n\",\n",
    "    \"\\n\",\n",
    "    \"def clean_question(text):\\n\",\n",
    "    \"    if not isinstance(text, str):\\n\",\n",
    "    \"        return \\\"\\\"\\n\",\n",
    "    \"    text = unicodedata.normalize(\\\"NFKC\\\", text)\\n\",\n",
    "    \"    text = re.sub(rf\\\"[{re.escape(string.punctuation)}]\\\", \\\" \\\", text)  # remove punctuation\\n\",\n",
    "    \"    text = re.sub(r\\\"\\\\s+\\\", \\\" \\\", text)  # collapse multiple spaces\\n\",\n",
    "    \"    return text.strip()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Extract and clean\\n\",\n",
    "    \"queries = [\\n\",\n",
    "    \"    {\\n\",\n",
    "    \"        \\\"query_id\\\": item.get(\\\"query_id\\\", \\\"\\\"),\\n\",\n",
    "    \"        \\\"question\\\": clean_question(item.get(\\\"question\\\", \\\"\\\")),\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"    for item in data\\n\",\n",
    "    \"]\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Sort by query_id (assuming numeric)\\n\",\n",
    "    \"queries = sorted(queries, key=lambda x: int(x[\\\"query_id\\\"]) if str(x[\\\"query_id\\\"]).isdigit() else x[\\\"query_id\\\"])\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Keep only the first 10,000\\n\",\n",
    "    \"queries = queries[:10000]\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Save new JSON\\n\",\n",
    "    \"with open(output_file, \\\"w\\\", encoding=\\\"utf-8\\\") as f:\\n\",\n",
    "    \"    json.dump(queries, f, ensure_ascii=False, indent=2)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Saved {len(queries)} entries to {output_file}\\\")\\n\",\n",
    "    \"print(json.dumps(queries[:3], indent=2))\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"6NyCV6oqjFS0\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"# Create the Qrels for the test set\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {\n",
    "    \"colab\": {\n",
    "     \"base_uri\": \"https://localhost:8080/\"\n",
    "    },\n",
    "    \"executionInfo\": {\n",
    "     \"elapsed\": 742,\n",
    "     \"status\": \"ok\",\n",
    "     \"timestamp\": 1762962873672,\n",
    "     \"user\": {\n",
    "      \"displayName\": \"Georgios Peikos\",\n",
    "      \"userId\": \"04834132442165285194\"\n",
    "     },\n",
    "     \"user_tz\": -60\n",
    "    },\n",
    "    \"id\": \"Lxms9bHpjIcn\",\n",
    "    \"outputId\": \"26e9db71-b590-4f5f-94db-484d857db80c\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"input_file = \\\"data/test.json\\\"\\n\",\n",
    "    \"qrels_file = \\\"data/test_qrels.json\\\"\\n\",\n",
    "    \"answers_file = \\\"data/test_query_answers.json\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load the data\\n\",\n",
    "    \"with open(input_file, \\\"r\\\", encoding=\\\"utf-8\\\") as f:\\n\",\n",
    "    \"    data = json.load(f)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Build the qrels file: query_id, iteration=0, para_id, relevance=1\\n\",\n",
    "    \"qrels = [\\n\",\n",
    "    \"    {\\n\",\n",
    "    \"        \\\"query_id\\\": item.get(\\\"query_id\\\", \\\"\\\"),\\n\",\n",
    "    \"        \\\"iteration\\\": 0,\\n\",\n",
    "    \"        \\\"para_id\\\": item.get(\\\"para_id\\\", \\\"\\\"),\\n\",\n",
    "    \"        \\\"relevance\\\": 1\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"    for item in data\\n\",\n",
    "    \"]\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Build the query_answers file: same plus answer and org_answer\\n\",\n",
    "    \"query_answers = [\\n\",\n",
    "    \"    {\\n\",\n",
    "    \"        \\\"query_id\\\": item.get(\\\"query_id\\\", \\\"\\\"),\\n\",\n",
    "    \"        \\\"iteration\\\": 0,\\n\",\n",
    "    \"        \\\"para_id\\\": item.get(\\\"para_id\\\", \\\"\\\"),\\n\",\n",
    "    \"        \\\"relevance\\\": 1,\\n\",\n",
    "    \"        \\\"answer\\\": item.get(\\\"answer\\\", \\\"\\\"),\\n\",\n",
    "    \"        \\\"org_answer\\\": item.get(\\\"org_answer\\\", \\\"\\\")\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"    for item in data\\n\",\n",
    "    \"]\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Save both files\\n\",\n",
    "    \"with open(qrels_file, \\\"w\\\", encoding=\\\"utf-8\\\") as f:\\n\",\n",
    "    \"    json.dump(qrels, f, ensure_ascii=False, indent=2)\\n\",\n",
    "    \"\\n\",\n",
    "    \"with open(answers_file, \\\"w\\\", encoding=\\\"utf-8\\\") as f:\\n\",\n",
    "    \"    json.dump(query_answers, f, ensure_ascii=False, indent=2)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Saved {len(qrels)} entries to {qrels_file}\\\")\\n\",\n",
    "    \"print(f\\\"Saved {len(query_answers)} entries to {answers_file}\\\")\\n\",\n",
    "    \"print(\\\"Sample qrels entry:\\\", qrels[0])\\n\",\n",
    "    \"print(\\\"Sample query_answers entry:\\\", query_answers[0])\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"M7vkoP010nIF\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"# Retrieval\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Extract data from json files\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"input_files = ['data/document_collection.json', 'data/test.json', 'data/test_qrels.json', 'data/test_queries.json', 'data/test_query_answers.json', 'data/train.json', 'data/validation.json']\\n\",\n",
    "    \"\\n\",\n",
    "    \"dataframes = {}\\n\",\n",
    "    \"for input_file in input_files:\\n\",\n",
    "    \"    with open(input_file, \\\"r\\\", encoding=\\\"utf-8\\\") as f:\\n\",\n",
    "    \"        data = json.load(f)\\n\",\n",
    "    \"        dataframes[input_file] = pd.read_json(input_file)\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"Let's visualize data and analyze them\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"dataframes['data/document_collection.json']\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"dataframes['data/train.json']\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"**NOTE: in `data/document_collection.json` the rows are already deduplicated**\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### _Preprocessing_\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"#### **Linguistic Processing**\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"##### Normalization\\n\",\n",
    "    \"We lowercase everything and remove all special characters/tags\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"--> 1st step normalization\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"def normalize_text1(text):\\n\",\n",
    "    \"    if not isinstance(text, str):\\n\",\n",
    "    \"        return text\\n\",\n",
    "    \"    text = unicodedata.normalize('NFKC', text)\\n\",\n",
    "    \"    text = re.sub(r'<[^>]+>', ' ', text) # HTML\\n\",\n",
    "    \"    text = re.sub(r'\\\\s+', ' ', text).strip() # multiple white spaces\\n\",\n",
    "    \"    return text\\n\",\n",
    "    \"\\n\",\n",
    "    \"# in caso togliessimo la NER vanno tolti i commenti nella funzione qui sopra\\n\",\n",
    "    \"\\n\",\n",
    "    \"docColl = dataframes['data/document_collection.json']\\n\",\n",
    "    \"docColl_contNorm1 = docColl['context'].apply(normalize_text1)\\n\",\n",
    "    \"docColl_ocrNorm1 = docColl['raw_ocr'].apply(normalize_text1)\\n\",\n",
    "    \"docColl_Norm1 = docColl.copy()\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"docColl_Norm1['context'] = docColl_contNorm1\\n\",\n",
    "    \"docColl_Norm1['raw_ocr'] = docColl_ocrNorm1\\n\",\n",
    "    \"docColl_Norm1.head(25)\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"docColl['context'].compare(docColl_Norm1['context'])\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"print(docColl['context'].iloc[2])\\n\",\n",
    "    \"print(docColl_Norm1['context'].iloc[2])\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"##### NER\\n\",\n",
    "    \"We want to identify named-entities before lemmatizing the text, so that we do not lose any entity by \\\"shrinking\\\" words to their base forms.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"import os\\n\",\n",
    "    \"import torch\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"from transformers import AutoTokenizer, pipeline\\n\",\n",
    "    \"from tqdm import tqdm\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Settings per far runnare su gpu (se possibile)\\n\",\n",
    "    \"os.environ[\\\"PYTORCH_ENABLE_MPS_FALLBACK\\\"] = \\\"1\\\"\\n\",\n",
    "    \"device = \\\"mps\\\" if torch.backends.mps.is_available() else \\\"cpu\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"MODEL_NAME = \\\"impresso-project/ner-stacked-bert-multilingual-light\\\"\\n\",\n",
    "    \"tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\\n\",\n",
    "    \"\\n\",\n",
    "    \"ner_pipeline = pipeline(\\n\",\n",
    "    \"    model=MODEL_NAME,\\n\",\n",
    "    \"    tokenizer=tokenizer,\\n\",\n",
    "    \"    trust_remote_code=True,\\n\",\n",
    "    \"    device=device)\\n\",\n",
    "    \"\\n\",\n",
    "    \"def run_impresso_ner(text_series):\\n\",\n",
    "    \"    results = []\\n\",\n",
    "    \"    for text in tqdm(text_series): # tqdm per vedere i progressi nelle ore di run\\n\",\n",
    "    \"        text_str = str(text)\\n\",\n",
    "    \"        if not text_str.strip(): # per testi vuoti\\n\",\n",
    "    \"            results.append([])\\n\",\n",
    "    \"            continue\\n\",\n",
    "    \"\\n\",\n",
    "    \"        words = text_str.split()\\n\",\n",
    "    \"\\n\",\n",
    "    \"        try:\\n\",\n",
    "    \"            entities = ner_pipeline(text_str, tokens=words)\\n\",\n",
    "    \"            results.append(entities)\\n\",\n",
    "    \"        except Exception as e:\\n\",\n",
    "    \"            print(f\\\"Errore su un documento: {e}\\\")\\n\",\n",
    "    \"            results.append([]) # per non farlo bloccare se ha un errore\\n\",\n",
    "    \"    return results\\n\",\n",
    "    \"\\n\",\n",
    "    \"OUTPUT_FILE = \\\"data/ner_results_cache.parquet\\\"\\n\",\n",
    "    \"if os.path.exists(OUTPUT_FILE):\\n\",\n",
    "    \"    cached_data = pd.read_parquet(OUTPUT_FILE)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    docColl_Norm1['ner_entities_context'] = cached_data['ner_entities_context']\\n\",\n",
    "    \"    docColl_Norm1['ner_entities_ocr'] = cached_data['ner_entities_ocr']\\n\",\n",
    "    \"\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    # context\\n\",\n",
    "    \"    docColl_Norm1['ner_entities_context'] = run_impresso_ner(docColl_Norm1['context'])\\n\",\n",
    "    \"    # OCR\\n\",\n",
    "    \"    docColl_Norm1['ner_entities_ocr'] = run_impresso_ner(docColl_Norm1['raw_ocr'])\\n\",\n",
    "    \"    # salvataggio su file esterno\\n\",\n",
    "    \"    docColl_Norm1[['ner_entities_context', 'ner_entities_ocr']].to_parquet(OUTPUT_FILE)\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"docColl_Norm1\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"docColl_ner = docColl_Norm1.copy()\\n\",\n",
    "    \"docColl_ner[['context', 'raw_ocr', 'ner_entities_context', 'ner_entities_ocr']]\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"--> 2nd step normalization\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"def normalize_text2(text):\\n\",\n",
    "    \"    if not isinstance(text, str):\\n\",\n",
    "    \"        return text\\n\",\n",
    "    \"    text = text.lower() # lowercase\\n\",\n",
    "    \"    text = re.sub(r'[^a-z0-9\\\\s]', '', text) # punctuations\\n\",\n",
    "    \"    text = re.sub(r'[^\\\\w\\\\s]', ' ', text) # any other punctuation mark\\n\",\n",
    "    \"    text = re.sub(r'\\\\s+', ' ', text).strip() # white spaces again\\n\",\n",
    "    \"    return text\\n\",\n",
    "    \"\\n\",\n",
    "    \"docColl_ner['context'] = docColl_ner['context'].apply(normalize_text2)\\n\",\n",
    "    \"docColl_ner['raw_ocr'] = docColl_ner['raw_ocr'].apply(normalize_text2)\\n\",\n",
    "    \"docColl_Norm2 = docColl_ner.copy()\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": \"docColl_Norm2['context'].iloc[2]\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"##### Lemmatization\\n\",\n",
    "    \"Placed here to standardize semantically the sentences in the documents\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": [\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import spacy\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"import gc\\n\",\n",
    "    \"from tqdm import tqdm\\n\",\n",
    "    \"\\n\",\n",
    "    \"OUTPUT_FILE_LEMM = \\\"data/lemmatization_results_cache.parquet\\\"\\n\",\n",
    "    \"columns_to_process = ['context', 'raw_ocr']\\n\",\n",
    "    \"\\n\",\n",
    "    \"# 1. Caricamento ottimizzato: disabilitiamo tutto ciò che non serve alla lemmatizzazione\\n\",\n",
    "    \"# Teniamo il 'tagger' e 'attribute_ruler' perché necessari per lemmi accurati\\n\",\n",
    "    \"try:\\n\",\n",
    "    \"    nlp = spacy.load(\\\"en_core_web_sm\\\", disable=['parser', 'ner'])\\n\",\n",
    "    \"except OSError:\\n\",\n",
    "    \"    from spacy.cli import download\\n\",\n",
    "    \"    download(\\\"en_core_web_sm\\\")\\n\",\n",
    "    \"    nlp = spacy.load(\\\"en_core_web_sm\\\", disable=['parser', 'ner'])\\n\",\n",
    "    \"\\n\",\n",
    "    \"def run_lemmatization(df, columns):\\n\",\n",
    "    \"    temp_df = pd.DataFrame(index=df.index)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    for col in columns:\\n\",\n",
    "    \"        if col in df.columns:\\n\",\n",
    "    \"            print(f\\\"\\\\n--- Elaborazione colonna: '{col}' ---\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"            # 2. Utilizziamo un generatore per non duplicare i dati in memoria\\n\",\n",
    "    \"            def text_generator():\\n\",\n",
    "    \"                for text in df[col]:\\n\",\n",
    "    \"                    yield str(text) if pd.notnull(text) else \\\"\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"            processed_texts = []\\n\",\n",
    "    \"\\n\",\n",
    "    \"            # 3. Riduciamo n_process o rimuoviamolo se la RAM è poca.\\n\",\n",
    "    \"            # Aumentiamo il batch_size per compensare la velocità.\\n\",\n",
    "    \"            for doc in tqdm(nlp.pipe(text_generator(), batch_size=1000),\\n\",\n",
    "    \"                            total=len(df),\\n\",\n",
    "    \"                            desc=f\\\"Lemmatizing {col}\\\"):\\n\",\n",
    "    \"                # Usiamo una list comprehension rapida\\n\",\n",
    "    \"                lemmas = \\\" \\\".join([token.lemma_ for token in doc if not token.is_space])\\n\",\n",
    "    \"                processed_texts.append(lemmas)\\n\",\n",
    "    \"\\n\",\n",
    "    \"            temp_df[f\\\"{col}_lemma\\\"] = processed_texts\\n\",\n",
    "    \"\\n\",\n",
    "    \"            # 4. Pulizia manuale della memoria tra una colonna e l'altra\\n\",\n",
    "    \"            del processed_texts\\n\",\n",
    "    \"            gc.collect()\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            print(f\\\" Avviso: Colonna '{col}' non trovata.\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"    return temp_df\\n\",\n",
    "    \"\\n\",\n",
    "    \"if os.path.exists(OUTPUT_FILE_LEMM):\\n\",\n",
    "    \"    cached_lemm = pd.read_parquet(OUTPUT_FILE_LEMM)\\n\",\n",
    "    \"    docColl_Norm2['context_lemma'] = cached_lemm['context_lemma']\\n\",\n",
    "    \"    docColl_Norm2['raw_ocr_lemma'] = cached_lemm['raw_ocr_lemma']\\n\",\n",
    "    \"\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    # context\\n\",\n",
    "    \"    lemmatized_df = run_lemmatization(docColl_Norm2[['context', 'raw_ocr']], columns_to_process)\\n\",\n",
    "    \"    docColl_Norm2['context_lemma'] = lemmatized_df['context_lemma']\\n\",\n",
    "    \"    docColl_Norm2['raw_ocr_lemma'] = lemmatized_df['raw_ocr_lemma']\\n\",\n",
    "    \"    # salvataggio su file esterno\\n\",\n",
    "    \"    docColl_Norm2[['context_lemma', 'raw_ocr_lemma']].to_parquet(OUTPUT_FILE_LEMM)\\n\",\n",
    "    \"\\n\",\n",
    "    \"docColl_Lemm = docColl_Norm2\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": \"docColl_Lemm['context_lemma'].iloc[0]\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": \"docColl_Lemm['context'].iloc[0]\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"##### N-gram based tokenization\\n\",\n",
    "    \"Important to place it after normalization, in this tokenization can be integrated a NER-aware part so that \\\"the tokenization is also entity-guided\\\"\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": [\n",
    "    \"def ner_aware_string_processor(row, text_col, ner_col):\\n\",\n",
    "    \"\\n\",\n",
    "    \"    text = row.get(text_col, \\\"\\\")\\n\",\n",
    "    \"    entities = row.get(ner_col, [])\\n\",\n",
    "    \"\\n\",\n",
    "    \"    if not isinstance(text, str) or not text.strip():\\n\",\n",
    "    \"        return \\\"\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"    if isinstance(entities, list) and len(entities) > 0:\\n\",\n",
    "    \"        entity_texts = sorted(\\n\",\n",
    "    \"            [ent['surface'].lower().strip() for ent in entities if ent.get('confidence_ner', 0) >= 0.5],\\n\",\n",
    "    \"            key=len,\\n\",\n",
    "    \"            reverse=True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"        for ent_text in entity_texts:\\n\",\n",
    "    \"            if \\\" \\\" in ent_text:\\n\",\n",
    "    \"                glued_ent = ent_text.replace(\\\" \\\", \\\"_\\\")\\n\",\n",
    "    \"                text = text.replace(ent_text, glued_ent)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    tokens = text.split()\\n\",\n",
    "    \"    return \\\" \\\".join(tokens)\\n\",\n",
    "    \"\\n\",\n",
    "    \"docColl_tok = docColl_Lemm.copy()\\n\",\n",
    "    \"\\n\",\n",
    "    \"columns_map = [\\n\",\n",
    "    \"    ('context_lemma', 'ner_entities_context', 'context_bigrams'),\\n\",\n",
    "    \"    ('raw_ocr_lemma', 'ner_entities_ocr',     'raw_ocr_bigrams')\\n\",\n",
    "    \"]\\n\",\n",
    "    \"\\n\",\n",
    "    \"for text_col, ner_col, new_col in columns_map:\\n\",\n",
    "    \"    if text_col in docColl_tok.columns and ner_col in docColl_tok.columns:\\n\",\n",
    "    \"\\n\",\n",
    "    \"        docColl_tok[new_col] = docColl_tok.progress_apply(\\n\",\n",
    "    \"            lambda row: ner_aware_string_processor(row, text_col, ner_col),\\n\",\n",
    "    \"            axis=1 )\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": \"docColl_tok['context_bigrams'].iloc[0]\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"da qui dovrebbe uscire il dataframe chiamato docColl_tok\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### _Multi-field Indexing_\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"--> Indexing con PyTerrier usando un generator\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# qui assumiamo che le celle create dal NER siano oggetti di tipo dizionario\\n\",\n",
    "    \"def createGenerator(df, context=True):\\n\",\n",
    "    \"    # context\\n\",\n",
    "    \"    if context:\\n\",\n",
    "    \"        for _, row in df.iterrows():\\n\",\n",
    "    \"            # togliamo lOffset and rOffset\\n\",\n",
    "    \"            clean_ents = []\\n\",\n",
    "    \"            for ent in row['ner_entities_context']:\\n\",\n",
    "    \"                cleaned = {k: v for k, v in ent.items() if k not in ['lOffset', 'rOffset']}\\n\",\n",
    "    \"                clean_ents.append(cleaned)\\n\",\n",
    "    \"\\n\",\n",
    "    \"            search_terms = []\\n\",\n",
    "    \"            for e in clean_ents:\\n\",\n",
    "    \"                #search_terms.append(e.get('name', ''))\\n\",\n",
    "    \"                #search_terms.append(e.get('title', ''))\\n\",\n",
    "    \"                # da capire se vogliamo che siano searchable, dato che surface contiene già il testo a cui è associata la entity\\n\",\n",
    "    \"                search_terms.append(e.get('surface', ''))\\n\",\n",
    "    \"\\n\",\n",
    "    \"            ent_text = \\\" \\\".join(filter(None, search_terms)) # questa riga ha senso solo se prendiamo anche 'name' e 'title'\\n\",\n",
    "    \"                                                                                           # se no ent_text va assegnato a e.get('surface', ' ')\\n\",\n",
    "    \"\\n\",\n",
    "    \"            meta_json = json.dumps(clean_ents) # facciamo diventare tutti i metadati una stringa in forma json (non un oggetto dizionario, proprio una stringa)\\n\",\n",
    "    \"\\n\",\n",
    "    \"            yield { # serve per lo stream dei dati quando viene chiamata createGenerator dentro indexer.index(•)\\n\",\n",
    "    \"                \\\"docno\\\": str(row['para_id']),\\n\",\n",
    "    \"                \\\"text\\\": row['context_bigrams'],\\n\",\n",
    "    \"                \\\"entities\\\": ent_text, # entità searchable\\n\",\n",
    "    \"                \\\"entity_json\\\": meta_json}\\n\",\n",
    "    \"    # OCR\\n\",\n",
    "    \"    if not context:\\n\",\n",
    "    \"        for _, row in df.iterrows():\\n\",\n",
    "    \"            # togliamo lOffset and rOffset\\n\",\n",
    "    \"            clean_ents = []\\n\",\n",
    "    \"            for ent in row['ner_entities_ocr']:\\n\",\n",
    "    \"                cleaned = {k: v for k, v in ent.items() if k not in ['lOffset', 'rOffset']}\\n\",\n",
    "    \"                clean_ents.append(cleaned)\\n\",\n",
    "    \"\\n\",\n",
    "    \"            search_terms = []\\n\",\n",
    "    \"            for e in clean_ents:\\n\",\n",
    "    \"                #search_terms.append(e.get('name', ''))\\n\",\n",
    "    \"                #search_terms.append(e.get('title', ''))\\n\",\n",
    "    \"                # da capire se vogliamo che siano searchable, dato che surface contiene già il testo a cui è associata la entity\\n\",\n",
    "    \"                search_terms.append(e.get('surface', ''))\\n\",\n",
    "    \"\\n\",\n",
    "    \"            ent_text = \\\" \\\".join(filter(None, search_terms)) # questa riga ha senso solo se prendiamo anche 'name' e 'title'\\n\",\n",
    "    \"                                                                                           # se no ent_text va assegnato a e.get('surface', ' ')\\n\",\n",
    "    \"\\n\",\n",
    "    \"            meta_json = json.dumps(clean_ents) # facciamo diventare tutti i metadati una stringa in forma json (non un oggetto dizionario, proprio una stringa)\\n\",\n",
    "    \"\\n\",\n",
    "    \"            yield { # serve per lo stream dei dati quando viene chiamata createGenerator dentro indexer.index(•)\\n\",\n",
    "    \"                \\\"docno\\\": str(row['para_id']),\\n\",\n",
    "    \"                \\\"text\\\": row['raw_ocr_bigrams'],\\n\",\n",
    "    \"                \\\"entities\\\": ent_text, # entità searchable\\n\",\n",
    "    \"                \\\"entity_json\\\": meta_json}\\n\",\n",
    "    \"\\n\",\n",
    "    \"contextIndex_path = 'data/docColl_context-index'\\n\",\n",
    "    \"ocrIndex_path = 'data/docColl_ocr-index'\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"if os.path.exists(contextIndex_path):\\n\",\n",
    "    \"    shutil.rmtree(contextIndex_path)\\n\",\n",
    "    \"\\n\",\n",
    "    \"indexerCont = pt.IterDictIndexer(\\n\",\n",
    "    \"    'entity_index',\\n\",\n",
    "    \"    fields=['text', 'entities'],\\n\",\n",
    "    \"    meta={'docno', 'entity_json'})\\n\",\n",
    "    \"\\n\",\n",
    "    \"indexrefCont = indexerCont.index(createGenerator(docColl_tok, context=True))\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"if os.path.exists(ocrIndex_path):\\n\",\n",
    "    \"    shutil.rmtree(ocrIndex_path)\\n\",\n",
    "    \"\\n\",\n",
    "    \"indexerOCR = pt.IterDictIndexer(\\n\",\n",
    "    \"    'entity_index',\\n\",\n",
    "    \"    fields=['text', 'entities'],\\n\",\n",
    "    \"    meta={'docno', 'entity_json'})\\n\",\n",
    "    \"\\n\",\n",
    "    \"indexrefOCR = indexerOCR.index(createGenerator(docColl_tok, context=False))\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"#### Statistics about the indexed documents\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"indexCont = pt.IndexFactory.of(indexrefCont)\\n\",\n",
    "    \"stats = indexCont.getCollectionStatistics()\\n\",\n",
    "    \"print('Index folder:', contextIndex_path)\\n\",\n",
    "    \"print('Number of documents:', stats.getNumberOfDocuments())\\n\",\n",
    "    \"print('Number of postings:', stats.getNumberOfPostings())\\n\",\n",
    "    \"print('Number of tokens:', stats.getNumberOfTokens())\\n\",\n",
    "    \"print('Number of unique terms:', stats.getNumberOfUniqueTerms())\\n\",\n",
    "    \"print('Average document length:', stats.getAverageDocumentLength())\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"indexOCR = pt.IndexFactory.of(indexrefOCR)\\n\",\n",
    "    \"stats = indexOCR.getCollectionStatistics()\\n\",\n",
    "    \"print('Index folder:', contextIndex_path)\\n\",\n",
    "    \"print('Number of documents:', stats.getNumberOfDocuments())\\n\",\n",
    "    \"print('Number of postings:', stats.getNumberOfPostings())\\n\",\n",
    "    \"print('Number of tokens:', stats.getNumberOfTokens())\\n\",\n",
    "    \"print('Number of unique terms:', stats.getNumberOfUniqueTerms())\\n\",\n",
    "    \"print('Average document length:', stats.getAverageDocumentLength())\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"#### Query analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"display(queries.head(10))\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"--> da scrivere commento riguardo l'analisi delle queries\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"#### Qrels analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"display(qrels.sample(10))\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# stats for the qrels\\n\",\n",
    "    \"# Count how many relevance assessments each query has\\n\",\n",
    "    \"counts = qrels.groupby(\\\"query_id\\\")[\\\"para_id\\\"].count()  # group by query id and count documents\\n\",\n",
    "    \"print('Overall Statistics')\\n\",\n",
    "    \"print(counts.describe())  # show a summary of the count distribution\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Plot how many relevance assessments each query received\\n\",\n",
    "    \"plt.figure()  # create a new figure\\n\",\n",
    "    \"counts.plot(kind='hist')  # histogram showing distribution of judgment counts\\n\",\n",
    "    \"plt.xlabel('Number of relevance assessments per query')  # label for x-axis\\n\",\n",
    "    \"plt.ylabel('Number of queries')  # label for y-axis\\n\",\n",
    "    \"plt.title('Relevance assessment distribution')  # title of the plot\\n\",\n",
    "    \"plt.show()  # display the plot\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Show the queries with the highest number of relevance assessments\\n\",\n",
    "    \"counts.sort_values(ascending=False).head()  # top queries by number of judgments\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Count how many times each relevance label occurs overall\\n\",\n",
    "    \"qrels['relevance'].value_counts()  # distribution of relevance scores (e.g., 0, 1, 2, etc.)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Plot the label distribution as a histogram\\n\",\n",
    "    \"plt.figure()  # create a new figure\\n\",\n",
    "    \"qrels['relevance'].plot(kind='hist')  # histogram of relevance labels\\n\",\n",
    "    \"plt.xlabel('Relevance score')  # label for x-axis\\n\",\n",
    "    \"plt.ylabel('Frequency')  # label for y-axis\\n\",\n",
    "    \"plt.title('Relevance score distribution')  # title of the plot\\n\",\n",
    "    \"plt.show()  # display the plot\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"--> commento riguardo l'analisi delle qrels\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Phase I - Topical relevance-based retrieval\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### **BM25 Retrieval from raw OCR (baseline 1)**\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"bm25ocr = pt.terrier.Retriever(indexrefOCR, wmodel='BM25', ) # dovremmo usare un BM25F? per dividere i fields di ricerca (secondo me si)\\n\",\n",
    "    \"res_bm25ocr = bm25ocr.transform()\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### **BM25 Retrieval from corrected OCR (baseline 2)**\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### **BM25 Retrieval from both raw and corrected OCR using RRF formula (baseline 3)**\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"colab\": {\n",
    "   \"authorship_tag\": \"ABX9TyNUuc82OtGicqd8vHTH8YSN\",\n",
    "   \"provenance\": [],\n",
    "   \"toc_visible\": true\n",
    "  },\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3 (ipykernel)\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.11.12\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 0\n",
    "}\n"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'null' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 25\u001B[39m\n\u001B[32m      1\u001B[39m {\n\u001B[32m      2\u001B[39m  \u001B[33m\"\u001B[39m\u001B[33mcells\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m      3\u001B[39m   {\n\u001B[32m      4\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mmarkdown\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m      5\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {\n\u001B[32m      6\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mid\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mDRBwc_amYdiZ\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m      7\u001B[39m    },\n\u001B[32m      8\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m      9\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m# Download and Inspect the Collection\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m     10\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m     11\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mThe dataset was created from the Chronicling America collection — over 21 million digitized newspaper pages (1756–1963) curated by the Library of Congress and NEH. They used 39,330 pages (1800–1920), representing 53 US states, to ensure wide geographic and temporal coverage.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m     12\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m     13\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mSource: https://dl.acm.org/doi/pdf/10.1145/3626772.3657891\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m     14\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m     15\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mGitHub: https://github.com/DataScienceUIBK/ChroniclingAmericaQA?tab=readme-ov-file\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     16\u001B[39m    ]\n\u001B[32m     17\u001B[39m   },\n\u001B[32m     18\u001B[39m   {\n\u001B[32m     19\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mcode\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     20\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {},\n\u001B[32m     21\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m     22\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m%\u001B[39m\u001B[33mpip install -r requirements.txt\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m     23\u001B[39m    ],\n\u001B[32m     24\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33moutputs\u001B[39m\u001B[33m\"\u001B[39m: [],\n\u001B[32m---> \u001B[39m\u001B[32m25\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mexecution_count\u001B[39m\u001B[33m\"\u001B[39m: \u001B[43mnull\u001B[49m\n\u001B[32m     26\u001B[39m   },\n\u001B[32m     27\u001B[39m   {\n\u001B[32m     28\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mcode\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     29\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {},\n\u001B[32m     30\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m     31\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m# Imports\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m     32\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mimport os\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m     33\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mimport pandas as pd\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m     34\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mimport pyterrier as pt\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m     35\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mimport transformers\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m     36\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mimport torch\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m     37\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mimport nltk\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m     38\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mimport spacy\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m     39\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mimport shutil\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m     40\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mimport matplotlib.pyplot as plt  # plotting library\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m     41\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mimport numpy as np\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m     42\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mfrom collections import defaultdict\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m     43\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mimport re\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     44\u001B[39m    ],\n\u001B[32m     45\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33moutputs\u001B[39m\u001B[33m\"\u001B[39m: [],\n\u001B[32m     46\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mexecution_count\u001B[39m\u001B[33m\"\u001B[39m: null\n\u001B[32m     47\u001B[39m   },\n\u001B[32m     48\u001B[39m   {\n\u001B[32m     49\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mcode\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     50\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {\n\u001B[32m     51\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mcolab\u001B[39m\u001B[33m\"\u001B[39m: {\n\u001B[32m     52\u001B[39m      \u001B[33m\"\u001B[39m\u001B[33mbase_uri\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mhttps://localhost:8080/\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     53\u001B[39m     },\n\u001B[32m     54\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mexecutionInfo\u001B[39m\u001B[33m\"\u001B[39m: {\n\u001B[32m     55\u001B[39m      \u001B[33m\"\u001B[39m\u001B[33melapsed\u001B[39m\u001B[33m\"\u001B[39m: \u001B[32m12366\u001B[39m,\n\u001B[32m     56\u001B[39m      \u001B[33m\"\u001B[39m\u001B[33mstatus\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mok\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     57\u001B[39m      \u001B[33m\"\u001B[39m\u001B[33mtimestamp\u001B[39m\u001B[33m\"\u001B[39m: \u001B[32m1762962835550\u001B[39m,\n\u001B[32m     58\u001B[39m      \u001B[33m\"\u001B[39m\u001B[33muser\u001B[39m\u001B[33m\"\u001B[39m: {\n\u001B[32m     59\u001B[39m       \u001B[33m\"\u001B[39m\u001B[33mdisplayName\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mGeorgios Peikos\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     60\u001B[39m       \u001B[33m\"\u001B[39m\u001B[33muserId\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33m04834132442165285194\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     61\u001B[39m      },\n\u001B[32m     62\u001B[39m      \u001B[33m\"\u001B[39m\u001B[33muser_tz\u001B[39m\u001B[33m\"\u001B[39m: -\u001B[32m60\u001B[39m\n\u001B[32m     63\u001B[39m     },\n\u001B[32m     64\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mid\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33m4xBdfDsPYdLA\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     65\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33moutputId\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33m32103be7-1880-4ccb-ea70-6800e841dec1\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     66\u001B[39m    },\n\u001B[32m     67\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m     68\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mimport os\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m     69\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mos.makedirs(\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mdata\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m, exist_ok=True)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m     70\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m     71\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m!curl -L \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mhttps://huggingface.co/datasets/Bhawna/ChroniclingAmericaQA/resolve/main/test.json?download=true\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m -o data/test.json\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m     72\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m!curl -L \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mhttps://huggingface.co/datasets/Bhawna/ChroniclingAmericaQA/resolve/main/train.json?download=true\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m -o data/train.json\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m     73\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m!curl -L \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mhttps://huggingface.co/datasets/Bhawna/ChroniclingAmericaQA/resolve/main/dev.json?download=true\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m -o data/validation.json\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m     74\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m     75\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mimport json\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m     76\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m     77\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mfiles = [\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mdata/train.json\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m, \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mdata/validation.json\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m, \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mdata/test.json\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m]\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m     78\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m     79\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mfor path in files:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m     80\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    print(f\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[33mn===== \u001B[39m\u001B[38;5;132;01m{path}\u001B[39;00m\u001B[33m =====\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m     81\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    try:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m     82\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m        with open(path, \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mr\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m, encoding=\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mutf-8\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m) as f:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m     83\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m            # Read a few hundred characters to see what kind of JSON it is\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m     84\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m            head = f.read(500)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m     85\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m            print(\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mPreview of first 500 characters:\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[33mn\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m     86\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m            print(head[:500])\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m     87\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m        # Try to load only part of the file\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m     88\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m        with open(path, \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mr\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m, encoding=\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mutf-8\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m) as f:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m     89\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m            data = json.load(f)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m     90\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m        if isinstance(data, list):\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m     91\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m            print(f\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[33mnLoaded \u001B[39m\u001B[33m{\u001B[39m\u001B[33mlen(data)} items (list).\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m     92\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m            print(\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mDictionary keys:\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m, list(data[0].keys()))\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m     93\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m            print(json.dumps(data[0], indent=2)[:600])\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m     94\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m        elif isinstance(data, dict):\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m     95\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m            print(\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[33mnTop-level is a dictionary. Keys:\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m, list(data.keys()))\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m     96\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m            for k, v in data.items():\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m     97\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m                if isinstance(v, list):\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m     98\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m                    print(f\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mKey \u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m{k}\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m contains a list of \u001B[39m\u001B[33m{\u001B[39m\u001B[33mlen(v)} items.\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m     99\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m                    if v:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    100\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m                        print(\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mFirst item keys:\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m, list(v[0].keys()))\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    101\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m                        print(json.dumps(v[0], indent=2)[:600])\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    102\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m                        break\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    103\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m        else:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    104\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m            print(f\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mUnexpected top-level type: \u001B[39m\u001B[33m{\u001B[39m\u001B[33mtype(data)}\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    105\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    except Exception as e:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    106\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m        print(f\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mCould not parse \u001B[39m\u001B[38;5;132;01m{path}\u001B[39;00m\u001B[33m as JSON: \u001B[39m\u001B[38;5;132;01m{e}\u001B[39;00m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m)\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    107\u001B[39m    ],\n\u001B[32m    108\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33moutputs\u001B[39m\u001B[33m\"\u001B[39m: [],\n\u001B[32m    109\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mexecution_count\u001B[39m\u001B[33m\"\u001B[39m: null\n\u001B[32m    110\u001B[39m   },\n\u001B[32m    111\u001B[39m   {\n\u001B[32m    112\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mmarkdown\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    113\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {\n\u001B[32m    114\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mid\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mmylmVIP9bu8y\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    115\u001B[39m    },\n\u001B[32m    116\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m    117\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m# Create the Document Collection\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    118\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    119\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mTo do that, we create a new json file that contains the \u001B[39m\u001B[33m'\u001B[39m\u001B[33mpara_id\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m\u001B[33mcontext\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m\u001B[33mraw_ocr\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m\u001B[33mpublication_date\u001B[39m\u001B[33m'\u001B[39m\u001B[33m keys, for all para_id in the collection.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    120\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    121\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mpara_id: is the id of a paragraph of a news paper page.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    122\u001B[39m    ]\n\u001B[32m    123\u001B[39m   },\n\u001B[32m    124\u001B[39m   {\n\u001B[32m    125\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mcode\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    126\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {\n\u001B[32m    127\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mcolab\u001B[39m\u001B[33m\"\u001B[39m: {\n\u001B[32m    128\u001B[39m      \u001B[33m\"\u001B[39m\u001B[33mbase_uri\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mhttps://localhost:8080/\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    129\u001B[39m     },\n\u001B[32m    130\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mexecutionInfo\u001B[39m\u001B[33m\"\u001B[39m: {\n\u001B[32m    131\u001B[39m      \u001B[33m\"\u001B[39m\u001B[33melapsed\u001B[39m\u001B[33m\"\u001B[39m: \u001B[32m17568\u001B[39m,\n\u001B[32m    132\u001B[39m      \u001B[33m\"\u001B[39m\u001B[33mstatus\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mok\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    133\u001B[39m      \u001B[33m\"\u001B[39m\u001B[33mtimestamp\u001B[39m\u001B[33m\"\u001B[39m: \u001B[32m1762962853135\u001B[39m,\n\u001B[32m    134\u001B[39m      \u001B[33m\"\u001B[39m\u001B[33muser\u001B[39m\u001B[33m\"\u001B[39m: {\n\u001B[32m    135\u001B[39m       \u001B[33m\"\u001B[39m\u001B[33mdisplayName\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mGeorgios Peikos\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    136\u001B[39m       \u001B[33m\"\u001B[39m\u001B[33muserId\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33m04834132442165285194\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    137\u001B[39m      },\n\u001B[32m    138\u001B[39m      \u001B[33m\"\u001B[39m\u001B[33muser_tz\u001B[39m\u001B[33m\"\u001B[39m: -\u001B[32m60\u001B[39m\n\u001B[32m    139\u001B[39m     },\n\u001B[32m    140\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mid\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mnxch4FUUbxRw\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    141\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33moutputId\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33md86e4179-defd-49d8-8b68-172c577ed825\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    142\u001B[39m    },\n\u001B[32m    143\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m    144\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mimport json\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    145\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mimport os\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    146\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    147\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33minputs = [\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mdata/train.json\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m, \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mdata/validation.json\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m, \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mdata/test.json\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m]\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    148\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33moutput = \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mdata/document_collection.json\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    149\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    150\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mdef load_list_or_empty(path):\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    151\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    if not os.path.exists(path) or os.path.getsize(path) == 0:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    152\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m        print(f\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mSkipping \u001B[39m\u001B[38;5;132;01m{path}\u001B[39;00m\u001B[33m because it is missing or empty\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    153\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m        return []\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    154\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    try:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    155\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m        with open(path, \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mr\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m, encoding=\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mutf-8\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m) as f:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    156\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m            data = json.load(f)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    157\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m        if isinstance(data, list):\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    158\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m            return data\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    159\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m        print(f\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mSkipping \u001B[39m\u001B[38;5;132;01m{path}\u001B[39;00m\u001B[33m because it is not a list at the top level\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    160\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m        return []\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    161\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    except json.JSONDecodeError:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    162\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m        print(f\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mSkipping \u001B[39m\u001B[38;5;132;01m{path}\u001B[39;00m\u001B[33m because it is not valid JSON\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    163\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m        return []\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    164\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    165\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mdef project(recs):\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    166\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    out = []\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    167\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    for r in recs:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    168\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m        out.append(\u001B[39m\u001B[33m{\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    169\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m            \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mpara_id\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m: r.get(\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mpara_id\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m, \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m),\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    170\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m            \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mcontext\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m: r.get(\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mcontext\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m, \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m),\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    171\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m            \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mraw_ocr\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m: r.get(\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mraw_ocr\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m, \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m),\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    172\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m            \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mpublication_date\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m: r.get(\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mpublication_date\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m, \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    173\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m        })\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    174\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    return out\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    175\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    176\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mall_recs = []\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    177\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mfor p in inputs:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    178\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    recs = load_list_or_empty(p)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    179\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    print(f\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mLoaded \u001B[39m\u001B[33m{\u001B[39m\u001B[33mlen(recs)} records from \u001B[39m\u001B[38;5;132;01m{p}\u001B[39;00m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    180\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    all_recs.extend(project(recs))\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    181\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    182\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m# deduplicate by para_id keeping the first one seen\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    183\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33muniq = \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    184\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mfor rec in all_recs:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    185\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    pid = rec.get(\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mpara_id\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m, \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    186\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    if pid and pid not in uniq:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    187\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m        uniq[pid] = rec\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    188\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    189\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mresult = list(uniq.values())\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    190\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    191\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mwith open(output, \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mw\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m, encoding=\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mutf-8\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m) as f:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    192\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    json.dump(result, f, ensure_ascii=False, indent=2)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    193\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    194\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mprint(f\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mWrote \u001B[39m\u001B[33m{\u001B[39m\u001B[33mlen(result)} records to \u001B[39m\u001B[38;5;132;01m{output}\u001B[39;00m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    195\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mprint(json.dumps(result[:3], indent=2))\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    196\u001B[39m    ],\n\u001B[32m    197\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33moutputs\u001B[39m\u001B[33m\"\u001B[39m: [],\n\u001B[32m    198\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mexecution_count\u001B[39m\u001B[33m\"\u001B[39m: null\n\u001B[32m    199\u001B[39m   },\n\u001B[32m    200\u001B[39m   {\n\u001B[32m    201\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mmarkdown\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    202\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {\n\u001B[32m    203\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mid\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mO-9wljtri-XX\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    204\u001B[39m    },\n\u001B[32m    205\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m    206\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m## You should check that the collection you have matches that of the paper!\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    207\u001B[39m    ]\n\u001B[32m    208\u001B[39m   },\n\u001B[32m    209\u001B[39m   {\n\u001B[32m    210\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mcode\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    211\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {},\n\u001B[32m    212\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m    213\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mimport pandas as pd\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    214\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mfor path in inputs:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    215\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    with open(path, \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mr\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m, encoding=\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mutf-8\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m) as f:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    216\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m        data = json.load(f)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    217\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m        df_check = pd.read_json(path)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    218\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m        print(f\u001B[39m\u001B[33m'\u001B[39m\u001B[33mShape of \u001B[39m\u001B[38;5;132;01m{path}\u001B[39;00m\u001B[33m: \u001B[39m\u001B[38;5;132;01m{df_check.shape}\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m)\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    219\u001B[39m    ],\n\u001B[32m    220\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33moutputs\u001B[39m\u001B[33m\"\u001B[39m: [],\n\u001B[32m    221\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mexecution_count\u001B[39m\u001B[33m\"\u001B[39m: null\n\u001B[32m    222\u001B[39m   },\n\u001B[32m    223\u001B[39m   {\n\u001B[32m    224\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mmarkdown\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    225\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {},\n\u001B[32m    226\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m    227\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mThe dimensions match the ones of the paper at https://github.com/DataScienceUIBK/ChroniclingAmericaQA\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    228\u001B[39m    ]\n\u001B[32m    229\u001B[39m   },\n\u001B[32m    230\u001B[39m   {\n\u001B[32m    231\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mmarkdown\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    232\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {\n\u001B[32m    233\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mid\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33msnY9dkltgMts\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    234\u001B[39m    },\n\u001B[32m    235\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m    236\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m# Create the Test Queries Data Structure\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    237\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    238\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mWe keep the first 10.000 queries due to memory errors in the free colab version.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    239\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    240\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mTo be comparable, please keep the top 10.000 queries for evaluation.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    241\u001B[39m    ]\n\u001B[32m    242\u001B[39m   },\n\u001B[32m    243\u001B[39m   {\n\u001B[32m    244\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mcode\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    245\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {\n\u001B[32m    246\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mcolab\u001B[39m\u001B[33m\"\u001B[39m: {\n\u001B[32m    247\u001B[39m      \u001B[33m\"\u001B[39m\u001B[33mbase_uri\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mhttps://localhost:8080/\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    248\u001B[39m     },\n\u001B[32m    249\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mexecutionInfo\u001B[39m\u001B[33m\"\u001B[39m: {\n\u001B[32m    250\u001B[39m      \u001B[33m\"\u001B[39m\u001B[33melapsed\u001B[39m\u001B[33m\"\u001B[39m: \u001B[32m1151\u001B[39m,\n\u001B[32m    251\u001B[39m      \u001B[33m\"\u001B[39m\u001B[33mstatus\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mok\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    252\u001B[39m      \u001B[33m\"\u001B[39m\u001B[33mtimestamp\u001B[39m\u001B[33m\"\u001B[39m: \u001B[32m1762962872929\u001B[39m,\n\u001B[32m    253\u001B[39m      \u001B[33m\"\u001B[39m\u001B[33muser\u001B[39m\u001B[33m\"\u001B[39m: {\n\u001B[32m    254\u001B[39m       \u001B[33m\"\u001B[39m\u001B[33mdisplayName\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mGeorgios Peikos\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    255\u001B[39m       \u001B[33m\"\u001B[39m\u001B[33muserId\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33m04834132442165285194\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    256\u001B[39m      },\n\u001B[32m    257\u001B[39m      \u001B[33m\"\u001B[39m\u001B[33muser_tz\u001B[39m\u001B[33m\"\u001B[39m: -\u001B[32m60\u001B[39m\n\u001B[32m    258\u001B[39m     },\n\u001B[32m    259\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mid\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33m7ZOmr1qBgRxi\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    260\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33moutputId\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33m1a4cbaaa-2813-4814-e0de-aee5aab98f7c\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    261\u001B[39m    },\n\u001B[32m    262\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m    263\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mimport json\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    264\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mimport re\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    265\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mimport unicodedata\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    266\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mimport string\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    267\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    268\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33minput_file = \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mdata/test.json\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    269\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33moutput_file = \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mdata/test_queries.json\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    270\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    271\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m# Load the data\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    272\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mwith open(input_file, \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mr\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m, encoding=\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mutf-8\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m) as f:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    273\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    data = json.load(f)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    274\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    275\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mdef clean_question(text):\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    276\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    if not isinstance(text, str):\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    277\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m        return \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    278\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    text = unicodedata.normalize(\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mNFKC\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m, text)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    279\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    text = re.sub(rf\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m[\u001B[39m\u001B[33m{\u001B[39m\u001B[33mre.escape(string.punctuation)}]\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m, \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m, text)  # remove punctuation\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    280\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    text = re.sub(r\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[33ms+\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m, \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m, text)  # collapse multiple spaces\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    281\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    return text.strip()\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    282\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    283\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m# Extract and clean\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    284\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mqueries = [\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    285\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    \u001B[39m\u001B[33m{\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    286\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m        \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mquery_id\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m: item.get(\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mquery_id\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m, \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m),\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    287\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m        \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mquestion\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m: clean_question(item.get(\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mquestion\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m, \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m)),\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    288\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    }\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    289\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    for item in data\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    290\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m]\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    291\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    292\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m# Sort by query_id (assuming numeric)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    293\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mqueries = sorted(queries, key=lambda x: int(x[\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mquery_id\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m]) if str(x[\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mquery_id\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m]).isdigit() else x[\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mquery_id\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m])\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    294\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    295\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m# Keep only the first 10,000\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    296\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mqueries = queries[:10000]\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    297\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    298\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m# Save new JSON\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    299\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mwith open(output_file, \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mw\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m, encoding=\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mutf-8\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m) as f:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    300\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    json.dump(queries, f, ensure_ascii=False, indent=2)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    301\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    302\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mprint(f\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mSaved \u001B[39m\u001B[33m{\u001B[39m\u001B[33mlen(queries)} entries to \u001B[39m\u001B[38;5;132;01m{output_file}\u001B[39;00m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    303\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mprint(json.dumps(queries[:3], indent=2))\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    304\u001B[39m    ],\n\u001B[32m    305\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33moutputs\u001B[39m\u001B[33m\"\u001B[39m: [],\n\u001B[32m    306\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mexecution_count\u001B[39m\u001B[33m\"\u001B[39m: null\n\u001B[32m    307\u001B[39m   },\n\u001B[32m    308\u001B[39m   {\n\u001B[32m    309\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mmarkdown\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    310\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {\n\u001B[32m    311\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mid\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33m6NyCV6oqjFS0\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    312\u001B[39m    },\n\u001B[32m    313\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m    314\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m# Create the Qrels for the test set\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    315\u001B[39m    ]\n\u001B[32m    316\u001B[39m   },\n\u001B[32m    317\u001B[39m   {\n\u001B[32m    318\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mcode\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    319\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {\n\u001B[32m    320\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mcolab\u001B[39m\u001B[33m\"\u001B[39m: {\n\u001B[32m    321\u001B[39m      \u001B[33m\"\u001B[39m\u001B[33mbase_uri\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mhttps://localhost:8080/\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    322\u001B[39m     },\n\u001B[32m    323\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mexecutionInfo\u001B[39m\u001B[33m\"\u001B[39m: {\n\u001B[32m    324\u001B[39m      \u001B[33m\"\u001B[39m\u001B[33melapsed\u001B[39m\u001B[33m\"\u001B[39m: \u001B[32m742\u001B[39m,\n\u001B[32m    325\u001B[39m      \u001B[33m\"\u001B[39m\u001B[33mstatus\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mok\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    326\u001B[39m      \u001B[33m\"\u001B[39m\u001B[33mtimestamp\u001B[39m\u001B[33m\"\u001B[39m: \u001B[32m1762962873672\u001B[39m,\n\u001B[32m    327\u001B[39m      \u001B[33m\"\u001B[39m\u001B[33muser\u001B[39m\u001B[33m\"\u001B[39m: {\n\u001B[32m    328\u001B[39m       \u001B[33m\"\u001B[39m\u001B[33mdisplayName\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mGeorgios Peikos\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    329\u001B[39m       \u001B[33m\"\u001B[39m\u001B[33muserId\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33m04834132442165285194\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    330\u001B[39m      },\n\u001B[32m    331\u001B[39m      \u001B[33m\"\u001B[39m\u001B[33muser_tz\u001B[39m\u001B[33m\"\u001B[39m: -\u001B[32m60\u001B[39m\n\u001B[32m    332\u001B[39m     },\n\u001B[32m    333\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mid\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mLxms9bHpjIcn\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    334\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33moutputId\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33m26e9db71-b590-4f5f-94db-484d857db80c\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    335\u001B[39m    },\n\u001B[32m    336\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m    337\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33minput_file = \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mdata/test.json\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    338\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mqrels_file = \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mdata/test_qrels.json\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    339\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33manswers_file = \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mdata/test_query_answers.json\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    340\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    341\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m# Load the data\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    342\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mwith open(input_file, \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mr\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m, encoding=\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mutf-8\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m) as f:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    343\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    data = json.load(f)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    344\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    345\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m# Build the qrels file: query_id, iteration=0, para_id, relevance=1\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    346\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mqrels = [\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    347\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    \u001B[39m\u001B[33m{\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    348\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m        \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mquery_id\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m: item.get(\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mquery_id\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m, \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m),\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    349\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m        \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33miteration\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m: 0,\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    350\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m        \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mpara_id\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m: item.get(\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mpara_id\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m, \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m),\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    351\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m        \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mrelevance\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m: 1\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    352\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    }\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    353\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    for item in data\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    354\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m]\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    355\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    356\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m# Build the query_answers file: same plus answer and org_answer\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    357\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mquery_answers = [\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    358\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    \u001B[39m\u001B[33m{\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    359\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m        \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mquery_id\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m: item.get(\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mquery_id\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m, \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m),\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    360\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m        \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33miteration\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m: 0,\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    361\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m        \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mpara_id\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m: item.get(\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mpara_id\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m, \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m),\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    362\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m        \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mrelevance\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m: 1,\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    363\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m        \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33manswer\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m: item.get(\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33manswer\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m, \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m),\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    364\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m        \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33morg_answer\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m: item.get(\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33morg_answer\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m, \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    365\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    }\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    366\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    for item in data\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    367\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m]\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    368\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    369\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m# Save both files\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    370\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mwith open(qrels_file, \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mw\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m, encoding=\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mutf-8\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m) as f:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    371\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    json.dump(qrels, f, ensure_ascii=False, indent=2)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    372\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    373\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mwith open(answers_file, \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mw\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m, encoding=\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mutf-8\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m) as f:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    374\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    json.dump(query_answers, f, ensure_ascii=False, indent=2)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    375\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    376\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mprint(f\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mSaved \u001B[39m\u001B[33m{\u001B[39m\u001B[33mlen(qrels)} entries to \u001B[39m\u001B[38;5;132;01m{qrels_file}\u001B[39;00m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    377\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mprint(f\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mSaved \u001B[39m\u001B[33m{\u001B[39m\u001B[33mlen(query_answers)} entries to \u001B[39m\u001B[38;5;132;01m{answers_file}\u001B[39;00m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    378\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mprint(\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mSample qrels entry:\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m, qrels[0])\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    379\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mprint(\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mSample query_answers entry:\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m, query_answers[0])\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    380\u001B[39m    ],\n\u001B[32m    381\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33moutputs\u001B[39m\u001B[33m\"\u001B[39m: [],\n\u001B[32m    382\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mexecution_count\u001B[39m\u001B[33m\"\u001B[39m: null\n\u001B[32m    383\u001B[39m   },\n\u001B[32m    384\u001B[39m   {\n\u001B[32m    385\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mmarkdown\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    386\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {\n\u001B[32m    387\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mid\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mM7vkoP010nIF\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    388\u001B[39m    },\n\u001B[32m    389\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m    390\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m# Retrieval\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    391\u001B[39m    ]\n\u001B[32m    392\u001B[39m   },\n\u001B[32m    393\u001B[39m   {\n\u001B[32m    394\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mmarkdown\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    395\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {},\n\u001B[32m    396\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m    397\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m### Extract data from json files\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    398\u001B[39m    ]\n\u001B[32m    399\u001B[39m   },\n\u001B[32m    400\u001B[39m   {\n\u001B[32m    401\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mcode\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    402\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {},\n\u001B[32m    403\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m    404\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33minput_files = [\u001B[39m\u001B[33m'\u001B[39m\u001B[33mdata/document_collection.json\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m\u001B[33mdata/test.json\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m\u001B[33mdata/test_qrels.json\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m\u001B[33mdata/test_queries.json\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m\u001B[33mdata/test_query_answers.json\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m\u001B[33mdata/train.json\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m\u001B[33mdata/validation.json\u001B[39m\u001B[33m'\u001B[39m\u001B[33m]\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    405\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    406\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mdataframes = \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    407\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mfor input_file in input_files:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    408\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    with open(input_file, \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mr\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m, encoding=\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mutf-8\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m) as f:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    409\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m        data = json.load(f)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    410\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m        dataframes[input_file] = pd.read_json(input_file)\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    411\u001B[39m    ],\n\u001B[32m    412\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33moutputs\u001B[39m\u001B[33m\"\u001B[39m: [],\n\u001B[32m    413\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mexecution_count\u001B[39m\u001B[33m\"\u001B[39m: null\n\u001B[32m    414\u001B[39m   },\n\u001B[32m    415\u001B[39m   {\n\u001B[32m    416\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mmarkdown\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    417\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {},\n\u001B[32m    418\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m    419\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mLet\u001B[39m\u001B[33m'\u001B[39m\u001B[33ms visualize data and analyze them\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    420\u001B[39m    ]\n\u001B[32m    421\u001B[39m   },\n\u001B[32m    422\u001B[39m   {\n\u001B[32m    423\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mcode\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    424\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {},\n\u001B[32m    425\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m    426\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mdataframes[\u001B[39m\u001B[33m'\u001B[39m\u001B[33mdata/document_collection.json\u001B[39m\u001B[33m'\u001B[39m\u001B[33m]\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    427\u001B[39m    ],\n\u001B[32m    428\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33moutputs\u001B[39m\u001B[33m\"\u001B[39m: [],\n\u001B[32m    429\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mexecution_count\u001B[39m\u001B[33m\"\u001B[39m: null\n\u001B[32m    430\u001B[39m   },\n\u001B[32m    431\u001B[39m   {\n\u001B[32m    432\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mcode\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    433\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {},\n\u001B[32m    434\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m    435\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mdataframes[\u001B[39m\u001B[33m'\u001B[39m\u001B[33mdata/train.json\u001B[39m\u001B[33m'\u001B[39m\u001B[33m]\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    436\u001B[39m    ],\n\u001B[32m    437\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33moutputs\u001B[39m\u001B[33m\"\u001B[39m: [],\n\u001B[32m    438\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mexecution_count\u001B[39m\u001B[33m\"\u001B[39m: null\n\u001B[32m    439\u001B[39m   },\n\u001B[32m    440\u001B[39m   {\n\u001B[32m    441\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mmarkdown\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    442\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {},\n\u001B[32m    443\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m    444\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m**NOTE: in `data/document_collection.json` the rows are already deduplicated**\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    445\u001B[39m    ]\n\u001B[32m    446\u001B[39m   },\n\u001B[32m    447\u001B[39m   {\n\u001B[32m    448\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mmarkdown\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    449\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {},\n\u001B[32m    450\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m    451\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m### _Preprocessing_\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    452\u001B[39m    ]\n\u001B[32m    453\u001B[39m   },\n\u001B[32m    454\u001B[39m   {\n\u001B[32m    455\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mmarkdown\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    456\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {},\n\u001B[32m    457\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m    458\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m#### **Linguistic Processing**\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    459\u001B[39m    ]\n\u001B[32m    460\u001B[39m   },\n\u001B[32m    461\u001B[39m   {\n\u001B[32m    462\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mmarkdown\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    463\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {},\n\u001B[32m    464\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m    465\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m##### Normalization\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    466\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mWe lowercase everything and remove all special characters/tags\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    467\u001B[39m    ]\n\u001B[32m    468\u001B[39m   },\n\u001B[32m    469\u001B[39m   {\n\u001B[32m    470\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mmarkdown\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    471\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {},\n\u001B[32m    472\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m    473\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m--> 1st step normalization\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    474\u001B[39m    ]\n\u001B[32m    475\u001B[39m   },\n\u001B[32m    476\u001B[39m   {\n\u001B[32m    477\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mcode\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    478\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {},\n\u001B[32m    479\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m    480\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mdef normalize_text1(text):\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    481\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    if not isinstance(text, str):\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    482\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m        return text\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    483\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    text = unicodedata.normalize(\u001B[39m\u001B[33m'\u001B[39m\u001B[33mNFKC\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, text)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    484\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    text = re.sub(r\u001B[39m\u001B[33m'\u001B[39m\u001B[33m<[^>]+>\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m\u001B[33m \u001B[39m\u001B[33m'\u001B[39m\u001B[33m, text) # HTML\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    485\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    text = re.sub(r\u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[33ms+\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m\u001B[33m \u001B[39m\u001B[33m'\u001B[39m\u001B[33m, text).strip() # multiple white spaces\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    486\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    return text\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    487\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    488\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m# in caso togliessimo la NER vanno tolti i commenti nella funzione qui sopra\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    489\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    490\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mdocColl = dataframes[\u001B[39m\u001B[33m'\u001B[39m\u001B[33mdata/document_collection.json\u001B[39m\u001B[33m'\u001B[39m\u001B[33m]\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    491\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mdocColl_contNorm1 = docColl[\u001B[39m\u001B[33m'\u001B[39m\u001B[33mcontext\u001B[39m\u001B[33m'\u001B[39m\u001B[33m].apply(normalize_text1)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    492\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mdocColl_ocrNorm1 = docColl[\u001B[39m\u001B[33m'\u001B[39m\u001B[33mraw_ocr\u001B[39m\u001B[33m'\u001B[39m\u001B[33m].apply(normalize_text1)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    493\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mdocColl_Norm1 = docColl.copy()\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    494\u001B[39m    ],\n\u001B[32m    495\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33moutputs\u001B[39m\u001B[33m\"\u001B[39m: [],\n\u001B[32m    496\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mexecution_count\u001B[39m\u001B[33m\"\u001B[39m: null\n\u001B[32m    497\u001B[39m   },\n\u001B[32m    498\u001B[39m   {\n\u001B[32m    499\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mcode\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    500\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {},\n\u001B[32m    501\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m    502\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mdocColl_Norm1[\u001B[39m\u001B[33m'\u001B[39m\u001B[33mcontext\u001B[39m\u001B[33m'\u001B[39m\u001B[33m] = docColl_contNorm1\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    503\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mdocColl_Norm1[\u001B[39m\u001B[33m'\u001B[39m\u001B[33mraw_ocr\u001B[39m\u001B[33m'\u001B[39m\u001B[33m] = docColl_ocrNorm1\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    504\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mdocColl_Norm1.head(25)\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    505\u001B[39m    ],\n\u001B[32m    506\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33moutputs\u001B[39m\u001B[33m\"\u001B[39m: [],\n\u001B[32m    507\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mexecution_count\u001B[39m\u001B[33m\"\u001B[39m: null\n\u001B[32m    508\u001B[39m   },\n\u001B[32m    509\u001B[39m   {\n\u001B[32m    510\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mcode\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    511\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {},\n\u001B[32m    512\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m    513\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mdocColl[\u001B[39m\u001B[33m'\u001B[39m\u001B[33mcontext\u001B[39m\u001B[33m'\u001B[39m\u001B[33m].compare(docColl_Norm1[\u001B[39m\u001B[33m'\u001B[39m\u001B[33mcontext\u001B[39m\u001B[33m'\u001B[39m\u001B[33m])\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    514\u001B[39m    ],\n\u001B[32m    515\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33moutputs\u001B[39m\u001B[33m\"\u001B[39m: [],\n\u001B[32m    516\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mexecution_count\u001B[39m\u001B[33m\"\u001B[39m: null\n\u001B[32m    517\u001B[39m   },\n\u001B[32m    518\u001B[39m   {\n\u001B[32m    519\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mcode\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    520\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {},\n\u001B[32m    521\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m    522\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mprint(docColl[\u001B[39m\u001B[33m'\u001B[39m\u001B[33mcontext\u001B[39m\u001B[33m'\u001B[39m\u001B[33m].iloc[2])\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    523\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mprint(docColl_Norm1[\u001B[39m\u001B[33m'\u001B[39m\u001B[33mcontext\u001B[39m\u001B[33m'\u001B[39m\u001B[33m].iloc[2])\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    524\u001B[39m    ],\n\u001B[32m    525\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33moutputs\u001B[39m\u001B[33m\"\u001B[39m: [],\n\u001B[32m    526\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mexecution_count\u001B[39m\u001B[33m\"\u001B[39m: null\n\u001B[32m    527\u001B[39m   },\n\u001B[32m    528\u001B[39m   {\n\u001B[32m    529\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mmarkdown\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    530\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {},\n\u001B[32m    531\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m    532\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m##### NER\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    533\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mWe want to identify named-entities before lemmatizing the text, so that we do not lose any entity by \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mshrinking\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m words to their base forms.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    534\u001B[39m    ]\n\u001B[32m    535\u001B[39m   },\n\u001B[32m    536\u001B[39m   {\n\u001B[32m    537\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mcode\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    538\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {},\n\u001B[32m    539\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m    540\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mimport os\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    541\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mimport torch\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    542\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mimport pandas as pd\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    543\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mfrom transformers import AutoTokenizer, pipeline\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    544\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mfrom tqdm import tqdm\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    545\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    546\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m# Settings per far runnare su gpu (se possibile)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    547\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mos.environ[\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mPYTORCH_ENABLE_MPS_FALLBACK\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m] = \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m1\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    548\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mdevice = \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mmps\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m if torch.backends.mps.is_available() else \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mcpu\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    549\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    550\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mMODEL_NAME = \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mimpresso-project/ner-stacked-bert-multilingual-light\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    551\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mtokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    552\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    553\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mner_pipeline = pipeline(\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    554\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    model=MODEL_NAME,\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    555\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    tokenizer=tokenizer,\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    556\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    trust_remote_code=True,\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    557\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    device=device)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    558\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    559\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mdef run_impresso_ner(text_series):\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    560\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    results = []\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    561\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    for text in tqdm(text_series): # tqdm per vedere i progressi nelle ore di run\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    562\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m        text_str = str(text)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    563\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m        if not text_str.strip(): # per testi vuoti\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    564\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m            results.append([])\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    565\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m            continue\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    566\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    567\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m        words = text_str.split()\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    568\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    569\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m        try:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    570\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m            entities = ner_pipeline(text_str, tokens=words)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    571\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m            results.append(entities)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    572\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m        except Exception as e:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    573\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m            print(f\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mErrore su un documento: \u001B[39m\u001B[38;5;132;01m{e}\u001B[39;00m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    574\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m            results.append([]) # per non farlo bloccare se ha un errore\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    575\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    return results\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    576\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    577\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mOUTPUT_FILE = \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mdata/ner_results_cache.parquet\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    578\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mif os.path.exists(OUTPUT_FILE):\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    579\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    cached_data = pd.read_parquet(OUTPUT_FILE)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    580\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    581\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    docColl_Norm1[\u001B[39m\u001B[33m'\u001B[39m\u001B[33mner_entities_context\u001B[39m\u001B[33m'\u001B[39m\u001B[33m] = cached_data[\u001B[39m\u001B[33m'\u001B[39m\u001B[33mner_entities_context\u001B[39m\u001B[33m'\u001B[39m\u001B[33m]\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    582\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    docColl_Norm1[\u001B[39m\u001B[33m'\u001B[39m\u001B[33mner_entities_ocr\u001B[39m\u001B[33m'\u001B[39m\u001B[33m] = cached_data[\u001B[39m\u001B[33m'\u001B[39m\u001B[33mner_entities_ocr\u001B[39m\u001B[33m'\u001B[39m\u001B[33m]\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    583\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    584\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33melse:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    585\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    # context\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    586\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    docColl_Norm1[\u001B[39m\u001B[33m'\u001B[39m\u001B[33mner_entities_context\u001B[39m\u001B[33m'\u001B[39m\u001B[33m] = run_impresso_ner(docColl_Norm1[\u001B[39m\u001B[33m'\u001B[39m\u001B[33mcontext\u001B[39m\u001B[33m'\u001B[39m\u001B[33m])\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    587\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    # OCR\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    588\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    docColl_Norm1[\u001B[39m\u001B[33m'\u001B[39m\u001B[33mner_entities_ocr\u001B[39m\u001B[33m'\u001B[39m\u001B[33m] = run_impresso_ner(docColl_Norm1[\u001B[39m\u001B[33m'\u001B[39m\u001B[33mraw_ocr\u001B[39m\u001B[33m'\u001B[39m\u001B[33m])\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    589\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    # salvataggio su file esterno\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    590\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    docColl_Norm1[[\u001B[39m\u001B[33m'\u001B[39m\u001B[33mner_entities_context\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m\u001B[33mner_entities_ocr\u001B[39m\u001B[33m'\u001B[39m\u001B[33m]].to_parquet(OUTPUT_FILE)\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    591\u001B[39m    ],\n\u001B[32m    592\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33moutputs\u001B[39m\u001B[33m\"\u001B[39m: [],\n\u001B[32m    593\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mexecution_count\u001B[39m\u001B[33m\"\u001B[39m: null\n\u001B[32m    594\u001B[39m   },\n\u001B[32m    595\u001B[39m   {\n\u001B[32m    596\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mcode\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    597\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {},\n\u001B[32m    598\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m    599\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mdocColl_Norm1\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    600\u001B[39m    ],\n\u001B[32m    601\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33moutputs\u001B[39m\u001B[33m\"\u001B[39m: [],\n\u001B[32m    602\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mexecution_count\u001B[39m\u001B[33m\"\u001B[39m: null\n\u001B[32m    603\u001B[39m   },\n\u001B[32m    604\u001B[39m   {\n\u001B[32m    605\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mcode\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    606\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {},\n\u001B[32m    607\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m    608\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mdocColl_ner = docColl_Norm1.copy()\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    609\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mdocColl_ner[[\u001B[39m\u001B[33m'\u001B[39m\u001B[33mcontext\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m\u001B[33mraw_ocr\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m\u001B[33mner_entities_context\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m\u001B[33mner_entities_ocr\u001B[39m\u001B[33m'\u001B[39m\u001B[33m]]\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    610\u001B[39m    ],\n\u001B[32m    611\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33moutputs\u001B[39m\u001B[33m\"\u001B[39m: [],\n\u001B[32m    612\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mexecution_count\u001B[39m\u001B[33m\"\u001B[39m: null\n\u001B[32m    613\u001B[39m   },\n\u001B[32m    614\u001B[39m   {\n\u001B[32m    615\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mmarkdown\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    616\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {},\n\u001B[32m    617\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m    618\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m--> 2nd step normalization\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    619\u001B[39m    ]\n\u001B[32m    620\u001B[39m   },\n\u001B[32m    621\u001B[39m   {\n\u001B[32m    622\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mcode\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    623\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {},\n\u001B[32m    624\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m    625\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mdef normalize_text2(text):\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    626\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    if not isinstance(text, str):\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    627\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m        return text\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    628\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    text = text.lower() # lowercase\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    629\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    text = re.sub(r\u001B[39m\u001B[33m'\u001B[39m\u001B[33m[^a-z0-9\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[33ms]\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, text) # punctuations\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    630\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    text = re.sub(r\u001B[39m\u001B[33m'\u001B[39m\u001B[33m[^\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[33mw\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[33ms]\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m\u001B[33m \u001B[39m\u001B[33m'\u001B[39m\u001B[33m, text) # any other punctuation mark\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    631\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    text = re.sub(r\u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[33ms+\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m\u001B[33m \u001B[39m\u001B[33m'\u001B[39m\u001B[33m, text).strip() # white spaces again\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    632\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    return text\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    633\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    634\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mdocColl_ner[\u001B[39m\u001B[33m'\u001B[39m\u001B[33mcontext\u001B[39m\u001B[33m'\u001B[39m\u001B[33m] = docColl_ner[\u001B[39m\u001B[33m'\u001B[39m\u001B[33mcontext\u001B[39m\u001B[33m'\u001B[39m\u001B[33m].apply(normalize_text2)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    635\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mdocColl_ner[\u001B[39m\u001B[33m'\u001B[39m\u001B[33mraw_ocr\u001B[39m\u001B[33m'\u001B[39m\u001B[33m] = docColl_ner[\u001B[39m\u001B[33m'\u001B[39m\u001B[33mraw_ocr\u001B[39m\u001B[33m'\u001B[39m\u001B[33m].apply(normalize_text2)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    636\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mdocColl_Norm2 = docColl_ner.copy()\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    637\u001B[39m    ],\n\u001B[32m    638\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33moutputs\u001B[39m\u001B[33m\"\u001B[39m: [],\n\u001B[32m    639\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mexecution_count\u001B[39m\u001B[33m\"\u001B[39m: null\n\u001B[32m    640\u001B[39m   },\n\u001B[32m    641\u001B[39m   {\n\u001B[32m    642\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {},\n\u001B[32m    643\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mcode\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    644\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mdocColl_Norm2[\u001B[39m\u001B[33m'\u001B[39m\u001B[33mcontext\u001B[39m\u001B[33m'\u001B[39m\u001B[33m].iloc[2]\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    645\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33moutputs\u001B[39m\u001B[33m\"\u001B[39m: [],\n\u001B[32m    646\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mexecution_count\u001B[39m\u001B[33m\"\u001B[39m: null\n\u001B[32m    647\u001B[39m   },\n\u001B[32m    648\u001B[39m   {\n\u001B[32m    649\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mmarkdown\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    650\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {},\n\u001B[32m    651\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m    652\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m##### Lemmatization\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    653\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mPlaced here to standardize semantically the sentences in the documents\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    654\u001B[39m    ]\n\u001B[32m    655\u001B[39m   },\n\u001B[32m    656\u001B[39m   {\n\u001B[32m    657\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {},\n\u001B[32m    658\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mcode\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    659\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m    660\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mimport pandas as pd\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    661\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mimport spacy\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    662\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mimport os\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    663\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mimport gc\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    664\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mfrom tqdm import tqdm\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    665\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    666\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mOUTPUT_FILE_LEMM = \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mdata/lemmatization_results_cache.parquet\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    667\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mcolumns_to_process = [\u001B[39m\u001B[33m'\u001B[39m\u001B[33mcontext\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m\u001B[33mraw_ocr\u001B[39m\u001B[33m'\u001B[39m\u001B[33m]\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    668\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    669\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m# 1. Caricamento ottimizzato: disabilitiamo tutto ciò che non serve alla lemmatizzazione\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    670\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m# Teniamo il \u001B[39m\u001B[33m'\u001B[39m\u001B[33mtagger\u001B[39m\u001B[33m'\u001B[39m\u001B[33m e \u001B[39m\u001B[33m'\u001B[39m\u001B[33mattribute_ruler\u001B[39m\u001B[33m'\u001B[39m\u001B[33m perché necessari per lemmi accurati\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    671\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mtry:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    672\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    nlp = spacy.load(\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33men_core_web_sm\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m, disable=[\u001B[39m\u001B[33m'\u001B[39m\u001B[33mparser\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m\u001B[33mner\u001B[39m\u001B[33m'\u001B[39m\u001B[33m])\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    673\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mexcept OSError:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    674\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    from spacy.cli import download\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    675\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    download(\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33men_core_web_sm\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    676\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    nlp = spacy.load(\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33men_core_web_sm\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m, disable=[\u001B[39m\u001B[33m'\u001B[39m\u001B[33mparser\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m\u001B[33mner\u001B[39m\u001B[33m'\u001B[39m\u001B[33m])\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    677\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    678\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mdef run_lemmatization(df, columns):\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    679\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    temp_df = pd.DataFrame(index=df.index)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    680\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    681\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    for col in columns:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    682\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m        if col in df.columns:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    683\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m            print(f\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[33mn--- Elaborazione colonna: \u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m{col}\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m ---\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    684\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    685\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m            # 2. Utilizziamo un generatore per non duplicare i dati in memoria\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    686\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m            def text_generator():\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    687\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m                for text in df[col]:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    688\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m                    yield str(text) if pd.notnull(text) else \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    689\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    690\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m            processed_texts = []\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    691\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    692\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m            # 3. Riduciamo n_process o rimuoviamolo se la RAM è poca.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    693\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m            # Aumentiamo il batch_size per compensare la velocità.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    694\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m            for doc in tqdm(nlp.pipe(text_generator(), batch_size=1000),\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    695\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m                            total=len(df),\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    696\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m                            desc=f\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mLemmatizing \u001B[39m\u001B[38;5;132;01m{col}\u001B[39;00m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m):\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    697\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m                # Usiamo una list comprehension rapida\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    698\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m                lemmas = \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m.join([token.lemma_ for token in doc if not token.is_space])\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    699\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m                processed_texts.append(lemmas)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    700\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    701\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m            temp_df[f\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[38;5;132;01m{col}\u001B[39;00m\u001B[33m_lemma\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m] = processed_texts\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    702\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    703\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m            # 4. Pulizia manuale della memoria tra una colonna e l\u001B[39m\u001B[33m'\u001B[39m\u001B[33maltra\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    704\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m            del processed_texts\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    705\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m            gc.collect()\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    706\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m        else:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    707\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m            print(f\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m Avviso: Colonna \u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m{col}\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m non trovata.\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    708\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    709\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    return temp_df\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    710\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    711\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mif os.path.exists(OUTPUT_FILE_LEMM):\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    712\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    cached_lemm = pd.read_parquet(OUTPUT_FILE_LEMM)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    713\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    docColl_Norm2[\u001B[39m\u001B[33m'\u001B[39m\u001B[33mcontext_lemma\u001B[39m\u001B[33m'\u001B[39m\u001B[33m] = cached_lemm[\u001B[39m\u001B[33m'\u001B[39m\u001B[33mcontext_lemma\u001B[39m\u001B[33m'\u001B[39m\u001B[33m]\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    714\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    docColl_Norm2[\u001B[39m\u001B[33m'\u001B[39m\u001B[33mraw_ocr_lemma\u001B[39m\u001B[33m'\u001B[39m\u001B[33m] = cached_lemm[\u001B[39m\u001B[33m'\u001B[39m\u001B[33mraw_ocr_lemma\u001B[39m\u001B[33m'\u001B[39m\u001B[33m]\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    715\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    716\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33melse:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    717\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    # context\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    718\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    lemmatized_df = run_lemmatization(docColl_Norm2[[\u001B[39m\u001B[33m'\u001B[39m\u001B[33mcontext\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m\u001B[33mraw_ocr\u001B[39m\u001B[33m'\u001B[39m\u001B[33m]], columns_to_process)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    719\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    docColl_Norm2[\u001B[39m\u001B[33m'\u001B[39m\u001B[33mcontext_lemma\u001B[39m\u001B[33m'\u001B[39m\u001B[33m] = lemmatized_df[\u001B[39m\u001B[33m'\u001B[39m\u001B[33mcontext_lemma\u001B[39m\u001B[33m'\u001B[39m\u001B[33m]\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    720\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    docColl_Norm2[\u001B[39m\u001B[33m'\u001B[39m\u001B[33mraw_ocr_lemma\u001B[39m\u001B[33m'\u001B[39m\u001B[33m] = lemmatized_df[\u001B[39m\u001B[33m'\u001B[39m\u001B[33mraw_ocr_lemma\u001B[39m\u001B[33m'\u001B[39m\u001B[33m]\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    721\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    # salvataggio su file esterno\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    722\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    docColl_Norm2[[\u001B[39m\u001B[33m'\u001B[39m\u001B[33mcontext_lemma\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m\u001B[33mraw_ocr_lemma\u001B[39m\u001B[33m'\u001B[39m\u001B[33m]].to_parquet(OUTPUT_FILE_LEMM)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    723\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    724\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mdocColl_Lemm = docColl_Norm2\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    725\u001B[39m    ],\n\u001B[32m    726\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33moutputs\u001B[39m\u001B[33m\"\u001B[39m: [],\n\u001B[32m    727\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mexecution_count\u001B[39m\u001B[33m\"\u001B[39m: null\n\u001B[32m    728\u001B[39m   },\n\u001B[32m    729\u001B[39m   {\n\u001B[32m    730\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {},\n\u001B[32m    731\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mcode\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    732\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mdocColl_Lemm[\u001B[39m\u001B[33m'\u001B[39m\u001B[33mcontext_lemma\u001B[39m\u001B[33m'\u001B[39m\u001B[33m].iloc[0]\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    733\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33moutputs\u001B[39m\u001B[33m\"\u001B[39m: [],\n\u001B[32m    734\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mexecution_count\u001B[39m\u001B[33m\"\u001B[39m: null\n\u001B[32m    735\u001B[39m   },\n\u001B[32m    736\u001B[39m   {\n\u001B[32m    737\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {},\n\u001B[32m    738\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mcode\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    739\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mdocColl_Lemm[\u001B[39m\u001B[33m'\u001B[39m\u001B[33mcontext\u001B[39m\u001B[33m'\u001B[39m\u001B[33m].iloc[0]\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    740\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33moutputs\u001B[39m\u001B[33m\"\u001B[39m: [],\n\u001B[32m    741\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mexecution_count\u001B[39m\u001B[33m\"\u001B[39m: null\n\u001B[32m    742\u001B[39m   },\n\u001B[32m    743\u001B[39m   {\n\u001B[32m    744\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mmarkdown\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    745\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {},\n\u001B[32m    746\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m    747\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m##### N-gram based tokenization\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    748\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mImportant to place it after normalization, in this tokenization can be integrated a NER-aware part so that \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mthe tokenization is also entity-guided\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m    749\u001B[39m    ]\n\u001B[32m    750\u001B[39m   },\n\u001B[32m    751\u001B[39m   {\n\u001B[32m    752\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {},\n\u001B[32m    753\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mcode\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    754\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m    755\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mdef ner_aware_string_processor(row, text_col, ner_col):\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    756\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    757\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    text = row.get(text_col, \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    758\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    entities = row.get(ner_col, [])\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    759\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    760\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    if not isinstance(text, str) or not text.strip():\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    761\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m        return \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    762\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    763\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    if isinstance(entities, list) and len(entities) > 0:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    764\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m        entity_texts = sorted(\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    765\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m            [ent[\u001B[39m\u001B[33m'\u001B[39m\u001B[33msurface\u001B[39m\u001B[33m'\u001B[39m\u001B[33m].lower().strip() for ent in entities if ent.get(\u001B[39m\u001B[33m'\u001B[39m\u001B[33mconfidence_ner\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, 0) >= 0.5],\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    766\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m            key=len,\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    767\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m            reverse=True)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    768\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    769\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m        for ent_text in entity_texts:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    770\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m            if \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m in ent_text:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    771\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m                glued_ent = ent_text.replace(\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m, \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m_\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    772\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m                text = text.replace(ent_text, glued_ent)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    773\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    774\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    tokens = text.split()\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    775\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    return \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m.join(tokens)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    776\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    777\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mdocColl_tok = docColl_Lemm.copy()\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    778\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    779\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mcolumns_map = [\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    780\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    (\u001B[39m\u001B[33m'\u001B[39m\u001B[33mcontext_lemma\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m\u001B[33mner_entities_context\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m\u001B[33mcontext_bigrams\u001B[39m\u001B[33m'\u001B[39m\u001B[33m),\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    781\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    (\u001B[39m\u001B[33m'\u001B[39m\u001B[33mraw_ocr_lemma\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m\u001B[33mner_entities_ocr\u001B[39m\u001B[33m'\u001B[39m\u001B[33m,     \u001B[39m\u001B[33m'\u001B[39m\u001B[33mraw_ocr_bigrams\u001B[39m\u001B[33m'\u001B[39m\u001B[33m)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    782\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m]\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    783\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    784\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mfor text_col, ner_col, new_col in columns_map:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    785\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    if text_col in docColl_tok.columns and ner_col in docColl_tok.columns:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    786\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    787\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m        docColl_tok[new_col] = docColl_tok.progress_apply(\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    788\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m            lambda row: ner_aware_string_processor(row, text_col, ner_col),\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    789\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m            axis=1 )\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    790\u001B[39m    ],\n\u001B[32m    791\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33moutputs\u001B[39m\u001B[33m\"\u001B[39m: [],\n\u001B[32m    792\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mexecution_count\u001B[39m\u001B[33m\"\u001B[39m: null\n\u001B[32m    793\u001B[39m   },\n\u001B[32m    794\u001B[39m   {\n\u001B[32m    795\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {},\n\u001B[32m    796\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mcode\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    797\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mdocColl_tok[\u001B[39m\u001B[33m'\u001B[39m\u001B[33mcontext_bigrams\u001B[39m\u001B[33m'\u001B[39m\u001B[33m].iloc[0]\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    798\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33moutputs\u001B[39m\u001B[33m\"\u001B[39m: [],\n\u001B[32m    799\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mexecution_count\u001B[39m\u001B[33m\"\u001B[39m: null\n\u001B[32m    800\u001B[39m   },\n\u001B[32m    801\u001B[39m   {\n\u001B[32m    802\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mmarkdown\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    803\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {},\n\u001B[32m    804\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m    805\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mda qui dovrebbe uscire il dataframe chiamato docColl_tok\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    806\u001B[39m    ]\n\u001B[32m    807\u001B[39m   },\n\u001B[32m    808\u001B[39m   {\n\u001B[32m    809\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mmarkdown\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    810\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {},\n\u001B[32m    811\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m    812\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m### _Multi-field Indexing_\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    813\u001B[39m    ]\n\u001B[32m    814\u001B[39m   },\n\u001B[32m    815\u001B[39m   {\n\u001B[32m    816\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mmarkdown\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    817\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {},\n\u001B[32m    818\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m    819\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m--> Indexing con PyTerrier usando un generator\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    820\u001B[39m    ]\n\u001B[32m    821\u001B[39m   },\n\u001B[32m    822\u001B[39m   {\n\u001B[32m    823\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mcode\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    824\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {},\n\u001B[32m    825\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m    826\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m# qui assumiamo che le celle create dal NER siano oggetti di tipo dizionario\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    827\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mdef createGenerator(df, context=True):\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    828\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    # context\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    829\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    if context:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    830\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m        for _, row in df.iterrows():\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    831\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m            # togliamo lOffset and rOffset\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    832\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m            clean_ents = []\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    833\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m            for ent in row[\u001B[39m\u001B[33m'\u001B[39m\u001B[33mner_entities_context\u001B[39m\u001B[33m'\u001B[39m\u001B[33m]:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    834\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m                cleaned = \u001B[39m\u001B[33m{\u001B[39m\u001B[33mk: v for k, v in ent.items() if k not in [\u001B[39m\u001B[33m'\u001B[39m\u001B[33mlOffset\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m\u001B[33mrOffset\u001B[39m\u001B[33m'\u001B[39m\u001B[33m]}\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    835\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m                clean_ents.append(cleaned)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    836\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    837\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m            search_terms = []\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    838\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m            for e in clean_ents:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    839\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m                #search_terms.append(e.get(\u001B[39m\u001B[33m'\u001B[39m\u001B[33mname\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m\u001B[33m'\u001B[39m\u001B[33m))\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    840\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m                #search_terms.append(e.get(\u001B[39m\u001B[33m'\u001B[39m\u001B[33mtitle\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m\u001B[33m'\u001B[39m\u001B[33m))\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    841\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m                # da capire se vogliamo che siano searchable, dato che surface contiene già il testo a cui è associata la entity\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    842\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m                search_terms.append(e.get(\u001B[39m\u001B[33m'\u001B[39m\u001B[33msurface\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m\u001B[33m'\u001B[39m\u001B[33m))\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    843\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    844\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m            ent_text = \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m.join(filter(None, search_terms)) # questa riga ha senso solo se prendiamo anche \u001B[39m\u001B[33m'\u001B[39m\u001B[33mname\u001B[39m\u001B[33m'\u001B[39m\u001B[33m e \u001B[39m\u001B[33m'\u001B[39m\u001B[33mtitle\u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    845\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m                                                                                           # se no ent_text va assegnato a e.get(\u001B[39m\u001B[33m'\u001B[39m\u001B[33msurface\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m\u001B[33m \u001B[39m\u001B[33m'\u001B[39m\u001B[33m)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    846\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    847\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m            meta_json = json.dumps(clean_ents) # facciamo diventare tutti i metadati una stringa in forma json (non un oggetto dizionario, proprio una stringa)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    848\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    849\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m            yield \u001B[39m\u001B[33m{\u001B[39m\u001B[33m # serve per lo stream dei dati quando viene chiamata createGenerator dentro indexer.index(•)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    850\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m                \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mdocno\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m: str(row[\u001B[39m\u001B[33m'\u001B[39m\u001B[33mpara_id\u001B[39m\u001B[33m'\u001B[39m\u001B[33m]),\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    851\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m                \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mtext\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m: row[\u001B[39m\u001B[33m'\u001B[39m\u001B[33mcontext_bigrams\u001B[39m\u001B[33m'\u001B[39m\u001B[33m],\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    852\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m                \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mentities\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m: ent_text, # entità searchable\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    853\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m                \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mentity_json\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m: meta_json}\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    854\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    # OCR\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    855\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    if not context:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    856\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m        for _, row in df.iterrows():\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    857\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m            # togliamo lOffset and rOffset\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    858\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m            clean_ents = []\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    859\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m            for ent in row[\u001B[39m\u001B[33m'\u001B[39m\u001B[33mner_entities_ocr\u001B[39m\u001B[33m'\u001B[39m\u001B[33m]:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    860\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m                cleaned = \u001B[39m\u001B[33m{\u001B[39m\u001B[33mk: v for k, v in ent.items() if k not in [\u001B[39m\u001B[33m'\u001B[39m\u001B[33mlOffset\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m\u001B[33mrOffset\u001B[39m\u001B[33m'\u001B[39m\u001B[33m]}\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    861\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m                clean_ents.append(cleaned)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    862\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    863\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m            search_terms = []\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    864\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m            for e in clean_ents:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    865\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m                #search_terms.append(e.get(\u001B[39m\u001B[33m'\u001B[39m\u001B[33mname\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m\u001B[33m'\u001B[39m\u001B[33m))\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    866\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m                #search_terms.append(e.get(\u001B[39m\u001B[33m'\u001B[39m\u001B[33mtitle\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m\u001B[33m'\u001B[39m\u001B[33m))\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    867\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m                # da capire se vogliamo che siano searchable, dato che surface contiene già il testo a cui è associata la entity\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    868\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m                search_terms.append(e.get(\u001B[39m\u001B[33m'\u001B[39m\u001B[33msurface\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m\u001B[33m'\u001B[39m\u001B[33m))\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    869\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    870\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m            ent_text = \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m.join(filter(None, search_terms)) # questa riga ha senso solo se prendiamo anche \u001B[39m\u001B[33m'\u001B[39m\u001B[33mname\u001B[39m\u001B[33m'\u001B[39m\u001B[33m e \u001B[39m\u001B[33m'\u001B[39m\u001B[33mtitle\u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    871\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m                                                                                           # se no ent_text va assegnato a e.get(\u001B[39m\u001B[33m'\u001B[39m\u001B[33msurface\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m\u001B[33m \u001B[39m\u001B[33m'\u001B[39m\u001B[33m)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    872\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    873\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m            meta_json = json.dumps(clean_ents) # facciamo diventare tutti i metadati una stringa in forma json (non un oggetto dizionario, proprio una stringa)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    874\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    875\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m            yield \u001B[39m\u001B[33m{\u001B[39m\u001B[33m # serve per lo stream dei dati quando viene chiamata createGenerator dentro indexer.index(•)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    876\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m                \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mdocno\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m: str(row[\u001B[39m\u001B[33m'\u001B[39m\u001B[33mpara_id\u001B[39m\u001B[33m'\u001B[39m\u001B[33m]),\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    877\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m                \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mtext\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m: row[\u001B[39m\u001B[33m'\u001B[39m\u001B[33mraw_ocr_bigrams\u001B[39m\u001B[33m'\u001B[39m\u001B[33m],\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    878\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m                \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mentities\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m: ent_text, # entità searchable\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    879\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m                \u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mentity_json\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m: meta_json}\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    880\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    881\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mcontextIndex_path = \u001B[39m\u001B[33m'\u001B[39m\u001B[33mdata/docColl_context-index\u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    882\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mocrIndex_path = \u001B[39m\u001B[33m'\u001B[39m\u001B[33mdata/docColl_ocr-index\u001B[39m\u001B[33m'\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    883\u001B[39m    ],\n\u001B[32m    884\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33moutputs\u001B[39m\u001B[33m\"\u001B[39m: [],\n\u001B[32m    885\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mexecution_count\u001B[39m\u001B[33m\"\u001B[39m: null\n\u001B[32m    886\u001B[39m   },\n\u001B[32m    887\u001B[39m   {\n\u001B[32m    888\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mcode\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    889\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {},\n\u001B[32m    890\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m    891\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mif os.path.exists(contextIndex_path):\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    892\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    shutil.rmtree(contextIndex_path)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    893\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    894\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mindexerCont = pt.IterDictIndexer(\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    895\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    \u001B[39m\u001B[33m'\u001B[39m\u001B[33mentity_index\u001B[39m\u001B[33m'\u001B[39m\u001B[33m,\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    896\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    fields=[\u001B[39m\u001B[33m'\u001B[39m\u001B[33mtext\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m\u001B[33mentities\u001B[39m\u001B[33m'\u001B[39m\u001B[33m],\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    897\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    meta=\u001B[39m\u001B[33m{\u001B[39m\u001B[33m'\u001B[39m\u001B[33mdocno\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m\u001B[33mentity_json\u001B[39m\u001B[33m'\u001B[39m\u001B[33m})\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    898\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    899\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mindexrefCont = indexerCont.index(createGenerator(docColl_tok, context=True))\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    900\u001B[39m    ],\n\u001B[32m    901\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33moutputs\u001B[39m\u001B[33m\"\u001B[39m: [],\n\u001B[32m    902\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mexecution_count\u001B[39m\u001B[33m\"\u001B[39m: null\n\u001B[32m    903\u001B[39m   },\n\u001B[32m    904\u001B[39m   {\n\u001B[32m    905\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mcode\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    906\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {},\n\u001B[32m    907\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m    908\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mif os.path.exists(ocrIndex_path):\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    909\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    shutil.rmtree(ocrIndex_path)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    910\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    911\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mindexerOCR = pt.IterDictIndexer(\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    912\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    \u001B[39m\u001B[33m'\u001B[39m\u001B[33mentity_index\u001B[39m\u001B[33m'\u001B[39m\u001B[33m,\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    913\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    fields=[\u001B[39m\u001B[33m'\u001B[39m\u001B[33mtext\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m\u001B[33mentities\u001B[39m\u001B[33m'\u001B[39m\u001B[33m],\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    914\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m    meta=\u001B[39m\u001B[33m{\u001B[39m\u001B[33m'\u001B[39m\u001B[33mdocno\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m\u001B[33mentity_json\u001B[39m\u001B[33m'\u001B[39m\u001B[33m})\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    915\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    916\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mindexrefOCR = indexerOCR.index(createGenerator(docColl_tok, context=False))\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    917\u001B[39m    ],\n\u001B[32m    918\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33moutputs\u001B[39m\u001B[33m\"\u001B[39m: [],\n\u001B[32m    919\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mexecution_count\u001B[39m\u001B[33m\"\u001B[39m: null\n\u001B[32m    920\u001B[39m   },\n\u001B[32m    921\u001B[39m   {\n\u001B[32m    922\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mmarkdown\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    923\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {},\n\u001B[32m    924\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m    925\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m#### Statistics about the indexed documents\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    926\u001B[39m    ]\n\u001B[32m    927\u001B[39m   },\n\u001B[32m    928\u001B[39m   {\n\u001B[32m    929\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mcode\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    930\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {},\n\u001B[32m    931\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m    932\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mindexCont = pt.IndexFactory.of(indexrefCont)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    933\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mstats = indexCont.getCollectionStatistics()\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    934\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mprint(\u001B[39m\u001B[33m'\u001B[39m\u001B[33mIndex folder:\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, contextIndex_path)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    935\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mprint(\u001B[39m\u001B[33m'\u001B[39m\u001B[33mNumber of documents:\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, stats.getNumberOfDocuments())\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    936\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mprint(\u001B[39m\u001B[33m'\u001B[39m\u001B[33mNumber of postings:\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, stats.getNumberOfPostings())\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    937\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mprint(\u001B[39m\u001B[33m'\u001B[39m\u001B[33mNumber of tokens:\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, stats.getNumberOfTokens())\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    938\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mprint(\u001B[39m\u001B[33m'\u001B[39m\u001B[33mNumber of unique terms:\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, stats.getNumberOfUniqueTerms())\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    939\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mprint(\u001B[39m\u001B[33m'\u001B[39m\u001B[33mAverage document length:\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, stats.getAverageDocumentLength())\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    940\u001B[39m    ],\n\u001B[32m    941\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33moutputs\u001B[39m\u001B[33m\"\u001B[39m: [],\n\u001B[32m    942\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mexecution_count\u001B[39m\u001B[33m\"\u001B[39m: null\n\u001B[32m    943\u001B[39m   },\n\u001B[32m    944\u001B[39m   {\n\u001B[32m    945\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mcode\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    946\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {},\n\u001B[32m    947\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m    948\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mindexOCR = pt.IndexFactory.of(indexrefOCR)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    949\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mstats = indexOCR.getCollectionStatistics()\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    950\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mprint(\u001B[39m\u001B[33m'\u001B[39m\u001B[33mIndex folder:\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, contextIndex_path)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    951\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mprint(\u001B[39m\u001B[33m'\u001B[39m\u001B[33mNumber of documents:\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, stats.getNumberOfDocuments())\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    952\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mprint(\u001B[39m\u001B[33m'\u001B[39m\u001B[33mNumber of postings:\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, stats.getNumberOfPostings())\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    953\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mprint(\u001B[39m\u001B[33m'\u001B[39m\u001B[33mNumber of tokens:\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, stats.getNumberOfTokens())\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    954\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mprint(\u001B[39m\u001B[33m'\u001B[39m\u001B[33mNumber of unique terms:\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, stats.getNumberOfUniqueTerms())\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    955\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mprint(\u001B[39m\u001B[33m'\u001B[39m\u001B[33mAverage document length:\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, stats.getAverageDocumentLength())\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    956\u001B[39m    ],\n\u001B[32m    957\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33moutputs\u001B[39m\u001B[33m\"\u001B[39m: [],\n\u001B[32m    958\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mexecution_count\u001B[39m\u001B[33m\"\u001B[39m: null\n\u001B[32m    959\u001B[39m   },\n\u001B[32m    960\u001B[39m   {\n\u001B[32m    961\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mmarkdown\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    962\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {},\n\u001B[32m    963\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m    964\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m#### Query analysis\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    965\u001B[39m    ]\n\u001B[32m    966\u001B[39m   },\n\u001B[32m    967\u001B[39m   {\n\u001B[32m    968\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mcode\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    969\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {},\n\u001B[32m    970\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m    971\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mdisplay(queries.head(10))\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    972\u001B[39m    ],\n\u001B[32m    973\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33moutputs\u001B[39m\u001B[33m\"\u001B[39m: [],\n\u001B[32m    974\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mexecution_count\u001B[39m\u001B[33m\"\u001B[39m: null\n\u001B[32m    975\u001B[39m   },\n\u001B[32m    976\u001B[39m   {\n\u001B[32m    977\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mmarkdown\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    978\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {},\n\u001B[32m    979\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m    980\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m--> da scrivere commento riguardo l\u001B[39m\u001B[33m'\u001B[39m\u001B[33manalisi delle queries\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    981\u001B[39m    ]\n\u001B[32m    982\u001B[39m   },\n\u001B[32m    983\u001B[39m   {\n\u001B[32m    984\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mmarkdown\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    985\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {},\n\u001B[32m    986\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m    987\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m#### Qrels analysis\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    988\u001B[39m    ]\n\u001B[32m    989\u001B[39m   },\n\u001B[32m    990\u001B[39m   {\n\u001B[32m    991\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mcode\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    992\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {},\n\u001B[32m    993\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m    994\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mdisplay(qrels.sample(10))\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    995\u001B[39m    ],\n\u001B[32m    996\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33moutputs\u001B[39m\u001B[33m\"\u001B[39m: [],\n\u001B[32m    997\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mexecution_count\u001B[39m\u001B[33m\"\u001B[39m: null\n\u001B[32m    998\u001B[39m   },\n\u001B[32m    999\u001B[39m   {\n\u001B[32m   1000\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mcode\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   1001\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {},\n\u001B[32m   1002\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m   1003\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m# stats for the qrels\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m   1004\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m# Count how many relevance assessments each query has\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m   1005\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mcounts = qrels.groupby(\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mquery_id\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m)[\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33mpara_id\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[33m].count()  # group by query id and count documents\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m   1006\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mprint(\u001B[39m\u001B[33m'\u001B[39m\u001B[33mOverall Statistics\u001B[39m\u001B[33m'\u001B[39m\u001B[33m)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m   1007\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mprint(counts.describe())  # show a summary of the count distribution\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m   1008\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m   1009\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m# Plot how many relevance assessments each query received\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m   1010\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mplt.figure()  # create a new figure\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m   1011\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mcounts.plot(kind=\u001B[39m\u001B[33m'\u001B[39m\u001B[33mhist\u001B[39m\u001B[33m'\u001B[39m\u001B[33m)  # histogram showing distribution of judgment counts\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m   1012\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mplt.xlabel(\u001B[39m\u001B[33m'\u001B[39m\u001B[33mNumber of relevance assessments per query\u001B[39m\u001B[33m'\u001B[39m\u001B[33m)  # label for x-axis\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m   1013\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mplt.ylabel(\u001B[39m\u001B[33m'\u001B[39m\u001B[33mNumber of queries\u001B[39m\u001B[33m'\u001B[39m\u001B[33m)  # label for y-axis\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m   1014\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mplt.title(\u001B[39m\u001B[33m'\u001B[39m\u001B[33mRelevance assessment distribution\u001B[39m\u001B[33m'\u001B[39m\u001B[33m)  # title of the plot\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m   1015\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mplt.show()  # display the plot\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m   1016\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m   1017\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m# Show the queries with the highest number of relevance assessments\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m   1018\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mcounts.sort_values(ascending=False).head()  # top queries by number of judgments\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m   1019\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m   1020\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m# Count how many times each relevance label occurs overall\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m   1021\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mqrels[\u001B[39m\u001B[33m'\u001B[39m\u001B[33mrelevance\u001B[39m\u001B[33m'\u001B[39m\u001B[33m].value_counts()  # distribution of relevance scores (e.g., 0, 1, 2, etc.)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m   1022\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m   1023\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m# Plot the label distribution as a histogram\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m   1024\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mplt.figure()  # create a new figure\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m   1025\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mqrels[\u001B[39m\u001B[33m'\u001B[39m\u001B[33mrelevance\u001B[39m\u001B[33m'\u001B[39m\u001B[33m].plot(kind=\u001B[39m\u001B[33m'\u001B[39m\u001B[33mhist\u001B[39m\u001B[33m'\u001B[39m\u001B[33m)  # histogram of relevance labels\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m   1026\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mplt.xlabel(\u001B[39m\u001B[33m'\u001B[39m\u001B[33mRelevance score\u001B[39m\u001B[33m'\u001B[39m\u001B[33m)  # label for x-axis\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m   1027\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mplt.ylabel(\u001B[39m\u001B[33m'\u001B[39m\u001B[33mFrequency\u001B[39m\u001B[33m'\u001B[39m\u001B[33m)  # label for y-axis\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m   1028\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mplt.title(\u001B[39m\u001B[33m'\u001B[39m\u001B[33mRelevance score distribution\u001B[39m\u001B[33m'\u001B[39m\u001B[33m)  # title of the plot\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m   1029\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mplt.show()  # display the plot\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1030\u001B[39m    ],\n\u001B[32m   1031\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33moutputs\u001B[39m\u001B[33m\"\u001B[39m: [],\n\u001B[32m   1032\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mexecution_count\u001B[39m\u001B[33m\"\u001B[39m: null\n\u001B[32m   1033\u001B[39m   },\n\u001B[32m   1034\u001B[39m   {\n\u001B[32m   1035\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mmarkdown\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   1036\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {},\n\u001B[32m   1037\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m   1038\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m--> commento riguardo l\u001B[39m\u001B[33m'\u001B[39m\u001B[33manalisi delle qrels\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1039\u001B[39m    ]\n\u001B[32m   1040\u001B[39m   },\n\u001B[32m   1041\u001B[39m   {\n\u001B[32m   1042\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mmarkdown\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   1043\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {},\n\u001B[32m   1044\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m   1045\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m## Phase I - Topical relevance-based retrieval\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1046\u001B[39m    ]\n\u001B[32m   1047\u001B[39m   },\n\u001B[32m   1048\u001B[39m   {\n\u001B[32m   1049\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mmarkdown\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   1050\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {},\n\u001B[32m   1051\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m   1052\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m### **BM25 Retrieval from raw OCR (baseline 1)**\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1053\u001B[39m    ]\n\u001B[32m   1054\u001B[39m   },\n\u001B[32m   1055\u001B[39m   {\n\u001B[32m   1056\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mcode\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   1057\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {},\n\u001B[32m   1058\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m   1059\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mbm25ocr = pt.terrier.Retriever(indexrefOCR, wmodel=\u001B[39m\u001B[33m'\u001B[39m\u001B[33mBM25\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, ) # dovremmo usare un BM25F? per dividere i fields di ricerca (secondo me si)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m   1060\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mres_bm25ocr = bm25ocr.transform()\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1061\u001B[39m    ],\n\u001B[32m   1062\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33moutputs\u001B[39m\u001B[33m\"\u001B[39m: [],\n\u001B[32m   1063\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mexecution_count\u001B[39m\u001B[33m\"\u001B[39m: null\n\u001B[32m   1064\u001B[39m   },\n\u001B[32m   1065\u001B[39m   {\n\u001B[32m   1066\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mmarkdown\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   1067\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {},\n\u001B[32m   1068\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m   1069\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m### **BM25 Retrieval from corrected OCR (baseline 2)**\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1070\u001B[39m    ]\n\u001B[32m   1071\u001B[39m   },\n\u001B[32m   1072\u001B[39m   {\n\u001B[32m   1073\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mcode\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   1074\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {},\n\u001B[32m   1075\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: [],\n\u001B[32m   1076\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33moutputs\u001B[39m\u001B[33m\"\u001B[39m: [],\n\u001B[32m   1077\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mexecution_count\u001B[39m\u001B[33m\"\u001B[39m: null\n\u001B[32m   1078\u001B[39m   },\n\u001B[32m   1079\u001B[39m   {\n\u001B[32m   1080\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mmarkdown\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   1081\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {},\n\u001B[32m   1082\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m   1083\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m### **BM25 Retrieval from both raw and corrected OCR using RRF formula (baseline 3)**\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1084\u001B[39m    ]\n\u001B[32m   1085\u001B[39m   },\n\u001B[32m   1086\u001B[39m   {\n\u001B[32m   1087\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcell_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mcode\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   1088\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {},\n\u001B[32m   1089\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33msource\u001B[39m\u001B[33m\"\u001B[39m: [],\n\u001B[32m   1090\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33moutputs\u001B[39m\u001B[33m\"\u001B[39m: [],\n\u001B[32m   1091\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mexecution_count\u001B[39m\u001B[33m\"\u001B[39m: null\n\u001B[32m   1092\u001B[39m   }\n\u001B[32m   1093\u001B[39m  ],\n\u001B[32m   1094\u001B[39m  \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: {\n\u001B[32m   1095\u001B[39m   \u001B[33m\"\u001B[39m\u001B[33mcolab\u001B[39m\u001B[33m\"\u001B[39m: {\n\u001B[32m   1096\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mauthorship_tag\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mABX9TyNUuc82OtGicqd8vHTH8YSN\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   1097\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mprovenance\u001B[39m\u001B[33m\"\u001B[39m: [],\n\u001B[32m   1098\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mtoc_visible\u001B[39m\u001B[33m\"\u001B[39m: true\n\u001B[32m   1099\u001B[39m   },\n\u001B[32m   1100\u001B[39m   \u001B[33m\"\u001B[39m\u001B[33mkernelspec\u001B[39m\u001B[33m\"\u001B[39m: {\n\u001B[32m   1101\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mdisplay_name\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mPython 3 (ipykernel)\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   1102\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mlanguage\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mpython\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   1103\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mname\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mpython3\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1104\u001B[39m   },\n\u001B[32m   1105\u001B[39m   \u001B[33m\"\u001B[39m\u001B[33mlanguage_info\u001B[39m\u001B[33m\"\u001B[39m: {\n\u001B[32m   1106\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mcodemirror_mode\u001B[39m\u001B[33m\"\u001B[39m: {\n\u001B[32m   1107\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mname\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mipython\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   1108\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mversion\u001B[39m\u001B[33m\"\u001B[39m: \u001B[32m3\u001B[39m\n\u001B[32m   1109\u001B[39m    },\n\u001B[32m   1110\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mfile_extension\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33m.py\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   1111\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mmimetype\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mtext/x-python\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   1112\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mname\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mpython\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   1113\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mnbconvert_exporter\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mpython\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   1114\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mpygments_lexer\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mipython3\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   1115\u001B[39m    \u001B[33m\"\u001B[39m\u001B[33mversion\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33m3.11.12\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1116\u001B[39m   }\n\u001B[32m   1117\u001B[39m  },\n\u001B[32m   1118\u001B[39m  \u001B[33m\"\u001B[39m\u001B[33mnbformat\u001B[39m\u001B[33m\"\u001B[39m: \u001B[32m4\u001B[39m,\n\u001B[32m   1119\u001B[39m  \u001B[33m\"\u001B[39m\u001B[33mnbformat_minor\u001B[39m\u001B[33m\"\u001B[39m: \u001B[32m0\u001B[39m\n\u001B[32m   1120\u001B[39m }\n",
      "\u001B[31mNameError\u001B[39m: name 'null' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNUuc82OtGicqd8vHTH8YSN",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

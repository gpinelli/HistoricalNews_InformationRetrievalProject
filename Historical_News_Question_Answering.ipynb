{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DRBwc_amYdiZ"
   },
   "source": [
    "# Download and Inspect the Collection\n",
    "\n",
    "The dataset was created from the Chronicling America collection — over 21 million digitized newspaper pages (1756–1963) curated by the Library of Congress and NEH. They used 39,330 pages (1800–1920), representing 53 US states, to ensure wide geographic and temporal coverage.\n",
    "\n",
    "Source: https://dl.acm.org/doi/pdf/10.1145/3626772.3657891\n",
    "\n",
    "GitHub: https://github.com/DataScienceUIBK/ChroniclingAmericaQA?tab=readme-ov-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyterrier as pt\n",
    "import transformers\n",
    "import torch\n",
    "import nltk\n",
    "import spacy\n",
    "import shutil\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12366,
     "status": "ok",
     "timestamp": 1762962835550,
     "user": {
      "displayName": "Georgios Peikos",
      "userId": "04834132442165285194"
     },
     "user_tz": -60
    },
    "id": "4xBdfDsPYdLA",
    "outputId": "32103be7-1880-4ccb-ea70-6800e841dec1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "!curl -L \"https://huggingface.co/datasets/Bhawna/ChroniclingAmericaQA/resolve/main/test.json?download=true\" -o data/test.json\n",
    "!curl -L \"https://huggingface.co/datasets/Bhawna/ChroniclingAmericaQA/resolve/main/train.json?download=true\" -o data/train.json\n",
    "!curl -L \"https://huggingface.co/datasets/Bhawna/ChroniclingAmericaQA/resolve/main/dev.json?download=true\" -o data/validation.json\n",
    "\n",
    "import json\n",
    "\n",
    "files = [\"data/train.json\", \"data/validation.json\", \"data/test.json\"]\n",
    "\n",
    "for path in files:\n",
    "    print(f\"\\n===== {path} =====\")\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            # Read a few hundred characters to see what kind of JSON it is\n",
    "            head = f.read(500)\n",
    "            print(\"Preview of first 500 characters:\\n\")\n",
    "            print(head[:500])\n",
    "        # Try to load only part of the file\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        if isinstance(data, list):\n",
    "            print(f\"\\nLoaded {len(data)} items (list).\")\n",
    "            print(\"Dictionary keys:\", list(data[0].keys()))\n",
    "            print(json.dumps(data[0], indent=2)[:600])\n",
    "        elif isinstance(data, dict):\n",
    "            print(\"\\nTop-level is a dictionary. Keys:\", list(data.keys()))\n",
    "            for k, v in data.items():\n",
    "                if isinstance(v, list):\n",
    "                    print(f\"Key '{k}' contains a list of {len(v)} items.\")\n",
    "                    if v:\n",
    "                        print(\"First item keys:\", list(v[0].keys()))\n",
    "                        print(json.dumps(v[0], indent=2)[:600])\n",
    "                        break\n",
    "        else:\n",
    "            print(f\"Unexpected top-level type: {type(data)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not parse {path} as JSON: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mylmVIP9bu8y"
   },
   "source": [
    "# Create the Document Collection\n",
    "\n",
    "To do that, we create a new json file that contains the 'para_id', 'context', 'raw_ocr', 'publication_date' keys, for all para_id in the collection.\n",
    "\n",
    "para_id: is the id of a paragraph of a news paper page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17568,
     "status": "ok",
     "timestamp": 1762962853135,
     "user": {
      "displayName": "Georgios Peikos",
      "userId": "04834132442165285194"
     },
     "user_tz": -60
    },
    "id": "nxch4FUUbxRw",
    "outputId": "d86e4179-defd-49d8-8b68-172c577ed825"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "inputs = [\"data/train.json\", \"data/validation.json\", \"data/test.json\"]\n",
    "output = \"data/document_collection.json\"\n",
    "\n",
    "def load_list_or_empty(path):\n",
    "    if not os.path.exists(path) or os.path.getsize(path) == 0:\n",
    "        print(f\"Skipping {path} because it is missing or empty\")\n",
    "        return []\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        if isinstance(data, list):\n",
    "            return data\n",
    "        print(f\"Skipping {path} because it is not a list at the top level\")\n",
    "        return []\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Skipping {path} because it is not valid JSON\")\n",
    "        return []\n",
    "\n",
    "def project(recs):\n",
    "    out = []\n",
    "    for r in recs:\n",
    "        out.append({\n",
    "            \"para_id\": r.get(\"para_id\", \"\"),\n",
    "            \"context\": r.get(\"context\", \"\"),\n",
    "            \"raw_ocr\": r.get(\"raw_ocr\", \"\"),\n",
    "            \"publication_date\": r.get(\"publication_date\", \"\")\n",
    "        })\n",
    "    return out\n",
    "\n",
    "all_recs = []\n",
    "for p in inputs:\n",
    "    recs = load_list_or_empty(p)\n",
    "    print(f\"Loaded {len(recs)} records from {p}\")\n",
    "    all_recs.extend(project(recs))\n",
    "\n",
    "# deduplicate by para_id keeping the first one seen\n",
    "uniq = {}\n",
    "for rec in all_recs:\n",
    "    pid = rec.get(\"para_id\", \"\")\n",
    "    if pid and pid not in uniq:\n",
    "        uniq[pid] = rec\n",
    "\n",
    "result = list(uniq.values())\n",
    "\n",
    "with open(output, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Wrote {len(result)} records to {output}\")\n",
    "print(json.dumps(result[:3], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-9wljtri-XX"
   },
   "source": [
    "## You should check that the collection you have matches that of the paper!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "for path in inputs:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        df_check = pd.read_json(path)\n",
    "        print(f'Shape of {path}: {df_check.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimensions match the ones of the paper at https://github.com/DataScienceUIBK/ChroniclingAmericaQA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "snY9dkltgMts"
   },
   "source": [
    "# Create the Test Queries Data Structure\n",
    "\n",
    "We keep the first 10.000 queries due to memory errors in the free colab version.\n",
    "\n",
    "To be comparable, please keep the top 10.000 queries for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1151,
     "status": "ok",
     "timestamp": 1762962872929,
     "user": {
      "displayName": "Georgios Peikos",
      "userId": "04834132442165285194"
     },
     "user_tz": -60
    },
    "id": "7ZOmr1qBgRxi",
    "outputId": "1a4cbaaa-2813-4814-e0de-aee5aab98f7c"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "input_file = \"data/test.json\"\n",
    "output_file = \"data/test_queries.json\"\n",
    "\n",
    "# Load the data\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "def clean_question(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    text = re.sub(rf\"[{re.escape(string.punctuation)}]\", \" \", text)  # remove punctuation\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # collapse multiple spaces\n",
    "    return text.strip()\n",
    "\n",
    "# Extract and clean\n",
    "queries = [\n",
    "    {\n",
    "        \"query_id\": item.get(\"query_id\", \"\"),\n",
    "        \"question\": clean_question(item.get(\"question\", \"\")),\n",
    "    }\n",
    "    for item in data\n",
    "]\n",
    "\n",
    "# Sort by query_id (assuming numeric)\n",
    "queries = sorted(queries, key=lambda x: int(x[\"query_id\"]) if str(x[\"query_id\"]).isdigit() else x[\"query_id\"])\n",
    "\n",
    "# Keep only the first 10,000\n",
    "queries = queries[:10000]\n",
    "\n",
    "# Save new JSON\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(queries, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Saved {len(queries)} entries to {output_file}\")\n",
    "print(json.dumps(queries[:3], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6NyCV6oqjFS0"
   },
   "source": [
    "# Create the Qrels for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 742,
     "status": "ok",
     "timestamp": 1762962873672,
     "user": {
      "displayName": "Georgios Peikos",
      "userId": "04834132442165285194"
     },
     "user_tz": -60
    },
    "id": "Lxms9bHpjIcn",
    "outputId": "26e9db71-b590-4f5f-94db-484d857db80c"
   },
   "outputs": [],
   "source": [
    "input_file = \"data/test.json\"\n",
    "qrels_file = \"data/test_qrels.json\"\n",
    "answers_file = \"data/test_query_answers.json\"\n",
    "\n",
    "# Load the data\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Build the qrels file: query_id, iteration=0, para_id, relevance=1\n",
    "qrels = [\n",
    "    {\n",
    "        \"query_id\": item.get(\"query_id\", \"\"),\n",
    "        \"iteration\": 0,\n",
    "        \"para_id\": item.get(\"para_id\", \"\"),\n",
    "        \"relevance\": 1\n",
    "    }\n",
    "    for item in data\n",
    "]\n",
    "\n",
    "# Build the query_answers file: same plus answer and org_answer\n",
    "query_answers = [\n",
    "    {\n",
    "        \"query_id\": item.get(\"query_id\", \"\"),\n",
    "        \"iteration\": 0,\n",
    "        \"para_id\": item.get(\"para_id\", \"\"),\n",
    "        \"relevance\": 1,\n",
    "        \"answer\": item.get(\"answer\", \"\"),\n",
    "        \"org_answer\": item.get(\"org_answer\", \"\")\n",
    "    }\n",
    "    for item in data\n",
    "]\n",
    "\n",
    "# Save both files\n",
    "with open(qrels_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(qrels, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(answers_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(query_answers, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Saved {len(qrels)} entries to {qrels_file}\")\n",
    "print(f\"Saved {len(query_answers)} entries to {answers_file}\")\n",
    "print(\"Sample qrels entry:\", qrels[0])\n",
    "print(\"Sample query_answers entry:\", query_answers[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7vkoP010nIF"
   },
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract data from json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files = ['data/document_collection.json', 'data/test.json', 'data/test_qrels.json', 'data/test_queries.json', 'data/test_query_answers.json', 'data/train.json', 'data/validation.json']\n",
    "\n",
    "dataframes = {}\n",
    "for input_file in input_files:\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        dataframes[input_file] = pd.read_json(input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize data and analyze them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes['data/document_collection.json']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes['data/train.json']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: in `data/document_collection.json` the rows are already deduplicated**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Preprocessing_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Linguistic Processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalization\n",
    "We lowercase everything and remove all special characters/tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> 1st step normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text1(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    #text = text.lower()\n",
    "    text = re.sub(r'<[^>]+>', ' ', text) # HTML\n",
    "    # text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() # multiple white spaces\n",
    "    return text\n",
    "\n",
    "# in caso togliessimo la NER vanno tolti i commenti nella funzione qui sopra\n",
    "\n",
    "docColl = dataframes['data/document_collection.json']\n",
    "docColl_contNorm1 = docColl['context'].apply(normalize_text1)\n",
    "docColl_ocrNorm1 = docColl['raw_ocr'].apply(normalize_text1)\n",
    "docColl_Norm1 = docColl.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docColl_Norm1['context'] = docColl_contNorm1\n",
    "docColl_Norm1['raw_ocr'] = docColl_ocrNorm1\n",
    "docColl_Norm1.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docColl['context'].compare(docColl_Norm1['context'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docColl['context'].iloc[2])\n",
    "print(docColl_Norm1['context'].iloc[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NER\n",
    "We want to identify named-entities before lemmatizing the text, so that we do not lose any entity by \"shrinking\" words to their base forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Settings per far runnare su gpu (se possibile)\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "MODEL_NAME = \"impresso-project/ner-stacked-bert-multilingual-light\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "ner_pipeline = pipeline(\n",
    "    model=MODEL_NAME,\n",
    "    tokenizer=tokenizer,\n",
    "    trust_remote_code=True,\n",
    "    device=device)\n",
    "\n",
    "def run_impresso_ner(text_series):\n",
    "    results = []\n",
    "    for text in tqdm(text_series): # tqdm per vedere i progressi nelle ore di run\n",
    "        text_str = str(text)\n",
    "        if not text_str.strip(): # per testi vuoti\n",
    "            results.append([])\n",
    "            continue\n",
    "\n",
    "        words = text_str.split()\n",
    "\n",
    "        try:\n",
    "            entities = ner_pipeline(text_str, tokens=words)\n",
    "            results.append(entities)\n",
    "        except Exception as e:\n",
    "            print(f\"Errore su un documento: {e}\")\n",
    "            results.append([]) # per non farlo bloccare se ha un errore\n",
    "    return results\n",
    "\n",
    "OUTPUT_FILE = \"data/ner_results_cache.parquet\"\n",
    "if os.path.exists(OUTPUT_FILE):\n",
    "    cached_data = pd.read_parquet(OUTPUT_FILE)\n",
    "\n",
    "    docColl_Norm1['ner_entities_context'] = cached_data['ner_entities_context']\n",
    "    docColl_Norm1['ner_entities_ocr'] = cached_data['ner_entities_ocr']\n",
    "\n",
    "else:\n",
    "    # context\n",
    "    docColl_Norm1['ner_entities_context'] = run_impresso_ner(docColl_Norm1['context'])\n",
    "    # OCR\n",
    "    docColl_Norm1['ner_entities_ocr'] = run_impresso_ner(docColl_Norm1['raw_ocr'])\n",
    "    # salvataggio su file esterno\n",
    "    docColl_Norm1[['ner_entities_context', 'ner_entities_ocr']].to_parquet(OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docColl_Norm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docColl_ner = docColl_Norm1.copy()\n",
    "docColl_ner[['context', 'raw_ocr', 'ner_entities_context', 'ner_entities_ocr']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> 2nd step normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text2(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    text = text.lower() # lowercase\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text) # punctuations\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() # white spaces again\n",
    "    return text\n",
    "\n",
    "# da testare così,\n",
    "# se va: cambiare anche normalize_text1\n",
    "# se non va: scrivere questo apply(normalize_text2) diviso tra context e raw_ocr e poi riunire tutto su un dataframe unico\n",
    "docColl_Norm2 = docColl_ner[['context', 'raw_ocr']].apply(normalize_text2)\n",
    "#docColl_ocrNorm2 = docColl_ner['raw_ocr'].apply(normalize_text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lemmatization\n",
    "Placed here to standardize semantically the sentences in the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "try:\n",
    "    if 'nlp' not in locals():\n",
    "        nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "except OSError:\n",
    "    from spacy.cli import download\n",
    "    download(\"en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "\n",
    "# Qui prendiamo i dati dall'ultimo step (docColl_Norm2)\n",
    "# Facciamo una copia per creare il nuovo dataframe 'docColl_Lemm'\n",
    "if 'docColl_Norm2' in locals():\n",
    "    docColl_Lemm = docColl_Norm2.copy()\n",
    "    print(\" DataFrame sorgente: 'docColl_Norm2' trovato e copiato in 'docColl_Lemm'.\")\n",
    "else:\n",
    "    print(\" Errore: 'docColl_Norm2' non trovato in memoria.\")\n",
    "\n",
    "columns_to_process = ['context', 'raw_ocr']\n",
    "\n",
    "print(f\" Avvio lemmatizzazione sulle colonne: {columns_to_process}\")\n",
    "\n",
    "for col in columns_to_process:\n",
    "    if col in docColl_Lemm.columns:\n",
    "        print(f\"\\n--- Elaborazione colonna: '{col}' ---\")\n",
    "        \n",
    "        texts = docColl_Lemm[col].astype(str).tolist()\n",
    "        processed_texts = []\n",
    "\n",
    "        print(f\"Processando {len(texts)} documenti da 'docColl_Norm2'...\")\n",
    "\n",
    "        for doc in nlp.pipe(texts, batch_size=2000, n_process=-1):\n",
    "            lemmas = [token.lemma_ for token in doc if not token.is_space]\n",
    "            processed_texts.append(\" \".join(lemmas))\n",
    "\n",
    "        new_col_name = f\"{col}_lemma\"\n",
    "        docColl_Lemm[new_col_name] = processed_texts\n",
    "\n",
    "        print(f\" Finito! Creata colonna: {new_col_name}\")\n",
    "    else:\n",
    "        print(f\" Errore: Colonna '{col}' non trovata nel dataframe.\")\n",
    "\n",
    "print(\"\\nDataFrame finale: docColl_Lemm\")\n",
    "print(docColl_Lemm[['context', 'context_lemma', 'raw_ocr', 'raw_ocr_lemma']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### N-gram based tokenization\n",
    "Important to place it after normalization, in this tokenization can be integrated a NER-aware part so that \"the tokenization is also entity-guided\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def ner_aware_ngram_tokenizer(row, text_col, ner_col, n=2):\n",
    "    \"\"\"\n",
    "    Tokenizzazione N-gram aware delle entità:\n",
    "    1. Unisce le entità composte (es. \"new york\" -> \"new_york\") nel testo.\n",
    "    2. Genera N-grams.\n",
    "    \"\"\"\n",
    "    text = row.get(text_col, \"\")\n",
    "    entities = row.get(ner_col, [])\n",
    "\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return []\n",
    "\n",
    "    if isinstance(entities, list) and len(entities) > 0:\n",
    "        entity_texts = []\n",
    "        \n",
    "        for ent in entities:\n",
    "            if isinstance(ent, dict):\n",
    "                if 'word' in ent:\n",
    "                    entity_texts.append(ent['word'])\n",
    "                elif 'entity_group' in ent:\n",
    "                    entity_texts.append(ent['entity_group'])\n",
    "                elif 'entity' in ent: \n",
    "                    entity_texts.append(ent['entity'])\n",
    "        \n",
    "        for ent_text in sorted(entity_texts, key=len, reverse=True):\n",
    "            if not isinstance(ent_text, str): continue\n",
    "            \n",
    "            clean_ent = ent_text.lower().strip()\n",
    "            if \" \" in clean_ent:\n",
    "                clean_ent_glued = clean_ent.replace(\" \", \"_\")\n",
    "                text = text.replace(clean_ent, clean_ent_glued)\n",
    "\n",
    "    tokens = text.split()\n",
    "\n",
    "    if len(tokens) < n:\n",
    "        return tokens \n",
    "\n",
    "    n_grams_tuples = zip(*[tokens[i:] for i in range(n)])\n",
    "    \n",
    "    n_grams_list = [\" \".join(ngram) for ngram in n_grams_tuples]\n",
    "\n",
    "    return n_grams_list\n",
    "\n",
    "\n",
    "if 'docColl_Lemm' in locals():\n",
    "    docColl_tok = docColl_Lemm.copy()\n",
    "    print(\" Creazione di 'docColl_tok' da 'docColl_Lemm'.\")\n",
    "else:\n",
    "    print(\" Errore: 'docColl_Lemm' non trovato. Assicurati di aver eseguito lo step precedente.\")\n",
    "\n",
    "N_VALUE = 2\n",
    "\n",
    "columns_map = [\n",
    "    ('context_lemma', 'ner_entities_context', 'context_ngrams'),\n",
    "    ('raw_ocr_lemma', 'ner_entities_ocr',     'raw_ocr_ngrams')\n",
    "]\n",
    "\n",
    "print(f\" Avvio Tokenization ({N_VALUE}-grams) Entity-Aware...\")\n",
    "\n",
    "for text_col, ner_col, new_col in columns_map:\n",
    "    \n",
    "    if text_col in docColl_tok.columns and ner_col in docColl_tok.columns:\n",
    "        print(f\"\\n--- Elaborazione: {text_col} + {ner_col} -> {new_col} ---\")\n",
    "        \n",
    "        docColl_tok[new_col] = docColl_tok.apply(\n",
    "            lambda row: ner_aware_ngram_tokenizer(\n",
    "                row, \n",
    "                text_col=text_col, \n",
    "                ner_col=ner_col, \n",
    "                n=N_VALUE\n",
    "            ), \n",
    "            axis=1\n",
    "        )\n",
    "        print(f\" Finito! Colonna creata: {new_col}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\" Saltato: Colonne '{text_col}' o '{ner_col}' non trovate nel DataFrame.\")\n",
    "\n",
    "print(\"\\nDataFrame finale: docColl_tok\")\n",
    "cols_to_show = ['context_lemma', 'context_ngrams', 'raw_ocr_lemma', 'raw_ocr_ngrams']\n",
    "cols_existing = [c for c in cols_to_show if c in docColl_tok.columns]\n",
    "print(docColl_tok[cols_existing].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "da qui dovrebbe uscire il dataframe chiamato docColl_tok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Multi-field Indexing_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def create_multi_field_index(df):\n",
    "    # The index structure: { field_name: { term: { doc_id: frequency } } }\n",
    "    inverted_index = {\n",
    "        \"raw\": defaultdict(lambda: defaultdict(int)),\n",
    "        \"clean\": defaultdict(lambda: defaultdict(int)),\n",
    "        \"entities\": defaultdict(lambda: defaultdict(int))\n",
    "    }\n",
    "    \n",
    "    # Track document frequency (how many docs a term appears in)\n",
    "    doc_counts = {\n",
    "        \"raw\": defaultdict(int),\n",
    "        \"clean\": defaultdict(int),\n",
    "        \"entities\": defaultdict(int)\n",
    "    }\n",
    "\n",
    "    num_docs = len(df)\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        doc_id = idx # Using dataframe index as Document ID\n",
    "        \n",
    "        # --- Field 1: Raw (from raw_ocr) ---\n",
    "        raw_tokens = str(row.get('raw_ocr', '')).lower().split()\n",
    "        for token in raw_tokens:\n",
    "            inverted_index[\"raw\"][token][doc_id] += 1\n",
    "            \n",
    "        # --- Field 2: Clean (from context / lemmatized_context) ---\n",
    "        clean_tokens = str(row.get('context', '')).lower().split()\n",
    "        for token in clean_tokens:\n",
    "            inverted_index[\"clean\"][token][doc_id] += 1\n",
    "            \n",
    "        # --- Field 3: Entities (from ner_entities) ---\n",
    "        # Extracts only the 'word' or 'entity' text from your NER results\n",
    "        entities_list = row.get('ner_entities', [])\n",
    "        if isinstance(entities_list, list):\n",
    "            for ent in entities_list:\n",
    "                # Handle different key structures found in your screenshots\n",
    "                ent_text = ent.get('word') or ent.get('entity_group') or ent.get('entity')\n",
    "                if ent_text:\n",
    "                    term = ent_text.lower().strip().replace(\" \", \"_\")\n",
    "                    inverted_index[\"entities\"][term][doc_id] += 1\n",
    "\n",
    "    return inverted_index, num_docs\n",
    "\n",
    "# Execute Indexing\n",
    "df_target = dataframes['data/document_collection.json']\n",
    "my_index, total_docs = create_multi_field_index(df_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> Indexing con PyTerrier usando un generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qui assumiamo che le celle create dal NER siano oggetti di tipo dizionario\n",
    "def createGenerator(df, context=True):\n",
    "    # context\n",
    "    if context:\n",
    "        for _, row in df.iterrows():\n",
    "            # togliamo lOffset and rOffset\n",
    "            clean_ents = []\n",
    "            for ent in row['ner_entities_context']:\n",
    "                cleaned = {k: v for k, v in ent.items() if k not in ['lOffset', 'rOffset']}\n",
    "                clean_ents.append(cleaned)\n",
    "\n",
    "            search_terms = []\n",
    "            for e in clean_ents:\n",
    "                #search_terms.append(e.get('name', ''))\n",
    "                #search_terms.append(e.get('title', ''))\n",
    "                # da capire se vogliamo che siano searchable, dato che surface contiene già il testo a cui è associata la entity\n",
    "                search_terms.append(e.get('surface', ''))\n",
    "\n",
    "            ent_text = \" \".join(filter(None, search_terms)) # questa riga ha senso solo se prendiamo anche 'name' e 'title'\n",
    "                                                                                           # se no ent_text va assegnato a e.get('surface', ' ')\n",
    "\n",
    "            meta_json = json.dumps(clean_ents) # facciamo diventare tutti i metadati una stringa in forma json (non un oggetto dizionario, proprio una stringa)\n",
    "\n",
    "            yield { # serve per lo stream dei dati quando viene chiamata createGenerator dentro indexer.index(•)\n",
    "                \"docno\": str(row['para_id']),\n",
    "                \"text\": row['context'],\n",
    "                \"entities\": ent_text, # entità searchable\n",
    "                \"entity_json\": meta_json}\n",
    "    # OCR\n",
    "    if not context:\n",
    "        for _, row in df.iterrows():\n",
    "            # togliamo lOffset and rOffset\n",
    "            clean_ents = []\n",
    "            for ent in row['ner_entities_ocr']:\n",
    "                cleaned = {k: v for k, v in ent.items() if k not in ['lOffset', 'rOffset']}\n",
    "                clean_ents.append(cleaned)\n",
    "\n",
    "            search_terms = []\n",
    "            for e in clean_ents:\n",
    "                #search_terms.append(e.get('name', ''))\n",
    "                #search_terms.append(e.get('title', ''))\n",
    "                # da capire se vogliamo che siano searchable, dato che surface contiene già il testo a cui è associata la entity\n",
    "                search_terms.append(e.get('surface', ''))\n",
    "\n",
    "            ent_text = \" \".join(filter(None, search_terms)) # questa riga ha senso solo se prendiamo anche 'name' e 'title'\n",
    "                                                                                           # se no ent_text va assegnato a e.get('surface', ' ')\n",
    "\n",
    "            meta_json = json.dumps(clean_ents) # facciamo diventare tutti i metadati una stringa in forma json (non un oggetto dizionario, proprio una stringa)\n",
    "\n",
    "            yield { # serve per lo stream dei dati quando viene chiamata createGenerator dentro indexer.index(•)\n",
    "                \"docno\": str(row['para_id']),\n",
    "                \"text\": row['raw_ocr'],\n",
    "                \"entities\": ent_text, # entità searchable\n",
    "                \"entity_json\": meta_json}\n",
    "\n",
    "contextIndex_path = 'data/docColl_context-index'\n",
    "ocrIndex_path = 'data/docColl_ocr-index'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(contextIndex_path):\n",
    "    shutil.rmtree(contextIndex_path)\n",
    "\n",
    "indexerCont = pt.IterDictIndexer(\n",
    "    'entity_index',\n",
    "    fields=['text', 'entities'],\n",
    "    meta={'docno', 'entity_json'})\n",
    "\n",
    "indexrefCont = indexerCont.index(createGenerator(docColl_tok, context=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(ocrIndex_path):\n",
    "    shutil.rmtree(ocrIndex_path)\n",
    "\n",
    "indexerOCR = pt.IterDictIndexer(\n",
    "    'entity_index',\n",
    "    fields=['text', 'entities'],\n",
    "    meta={'docno', 'entity_json'})\n",
    "\n",
    "indexrefOCR = indexerOCR.index(createGenerator(docColl_tok, context=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistics about the indexed documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexCont = pt.IndexFactory.of(indexrefCont)\n",
    "stats = indexCont.getCollectionStatistics()\n",
    "print('Index folder:', contextIndex_path)\n",
    "print('Number of documents:', stats.getNumberOfDocuments())\n",
    "print('Number of postings:', stats.getNumberOfPostings())\n",
    "print('Number of tokens:', stats.getNumberOfTokens())\n",
    "print('Number of unique terms:', stats.getNumberOfUniqueTerms())\n",
    "print('Average document length:', stats.getAverageDocumentLength())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexOCR = pt.IndexFactory.of(indexrefOCR)\n",
    "stats = indexOCR.getCollectionStatistics()\n",
    "print('Index folder:', contextIndex_path)\n",
    "print('Number of documents:', stats.getNumberOfDocuments())\n",
    "print('Number of postings:', stats.getNumberOfPostings())\n",
    "print('Number of tokens:', stats.getNumberOfTokens())\n",
    "print('Number of unique terms:', stats.getNumberOfUniqueTerms())\n",
    "print('Average document length:', stats.getAverageDocumentLength())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(queries.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> da scrivere commento riguardo l'analisi delle queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qrels analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(qrels.sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats for the qrels\n",
    "# Count how many relevance assessments each query has\n",
    "counts = qrels.groupby(\"query_id\")[\"para_id\"].count()  # group by query id and count documents\n",
    "print('Overall Statistics')\n",
    "print(counts.describe())  # show a summary of the count distribution\n",
    "\n",
    "import matplotlib.pyplot as plt  # plotting library\n",
    "\n",
    "# Plot how many relevance assessments each query received\n",
    "plt.figure()  # create a new figure\n",
    "counts.plot(kind='hist')  # histogram showing distribution of judgment counts\n",
    "plt.xlabel('Number of relevance assessments per query')  # label for x-axis\n",
    "plt.ylabel('Number of queries')  # label for y-axis\n",
    "plt.title('Relevance assessment distribution')  # title of the plot\n",
    "plt.show()  # display the plot\n",
    "\n",
    "# Show the queries with the highest number of relevance assessments\n",
    "counts.sort_values(ascending=False).head()  # top queries by number of judgments\n",
    "\n",
    "# Count how many times each relevance label occurs overall\n",
    "qrels['relevance'].value_counts()  # distribution of relevance scores (e.g., 0, 1, 2, etc.)\n",
    "\n",
    "# Plot the label distribution as a histogram\n",
    "plt.figure()  # create a new figure\n",
    "qrels['relevance'].plot(kind='hist')  # histogram of relevance labels\n",
    "plt.xlabel('Relevance score')  # label for x-axis\n",
    "plt.ylabel('Frequency')  # label for y-axis\n",
    "plt.title('Relevance score distribution')  # title of the plot\n",
    "plt.show()  # display the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> commento riguardo l'analisi delle qrels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase I - Topical relevance-based retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **BM25 Retrieval from raw OCR (baseline 1)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25ocr = pt.terrier.Retriever(indexrefOCR, wmodel='BM25', ) # dovremmo usare un BM25F? per dividere i fields di ricerca (secondo me si)\n",
    "res_bm25ocr = bm25ocr.transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **BM25 Retrieval from corrected OCR (baseline 2)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **BM25 Retrieval from both raw and corrected OCR using RRF formula (baseline 3)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNUuc82OtGicqd8vHTH8YSN",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DRBwc_amYdiZ"
   },
   "source": [
    "# Download and Inspect the Collection\n",
    "\n",
    "The dataset was created from the Chronicling America collection — over 21 million digitized newspaper pages (1756–1963) curated by the Library of Congress and NEH. They used 39,330 pages (1800–1920), representing 53 US states, to ensure wide geographic and temporal coverage.\n",
    "\n",
    "Source: https://dl.acm.org/doi/pdf/10.1145/3626772.3657891\n",
    "\n",
    "GitHub: https://github.com/DataScienceUIBK/ChroniclingAmericaQA?tab=readme-ov-file"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-09T20:46:59.639636Z",
     "start_time": "2026-01-09T20:46:58.504572Z"
    }
   },
   "source": [
    "%pip install -r requirements.txt\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing line 1 of /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages/distutils-precedence.pth:\r\n",
      "\r\n",
      "  Traceback (most recent call last):\r\n",
      "    File \"/Users/gabrielepinelli/miniconda3/lib/python3.10/site.py\", line 195, in addpackage\r\n",
      "      exec(line)\r\n",
      "    File \"<string>\", line 1, in <module>\r\n",
      "  ModuleNotFoundError: No module named '_distutils_hack'\r\n",
      "\r\n",
      "Remainder of file ignored\r\n",
      "Requirement already satisfied: pyterrier in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (1.0.0)\r\n",
      "Requirement already satisfied: pandas in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (2.2.2)\r\n",
      "Requirement already satisfied: transformers in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (4.57.3)\r\n",
      "Requirement already satisfied: torch in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (2.9.1)\r\n",
      "Requirement already satisfied: nltk in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (3.9.2)\r\n",
      "Requirement already satisfied: spacy in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (3.8.11)\r\n",
      "Requirement already satisfied: numpy in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from pyterrier->-r requirements.txt (line 1)) (1.26.4)\r\n",
      "Requirement already satisfied: more_itertools in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from pyterrier->-r requirements.txt (line 1)) (10.8.0)\r\n",
      "Requirement already satisfied: tqdm in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from pyterrier->-r requirements.txt (line 1)) (4.65.0)\r\n",
      "Requirement already satisfied: requests in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from pyterrier->-r requirements.txt (line 1)) (2.32.5)\r\n",
      "Requirement already satisfied: ir_datasets>=0.3.2 in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from pyterrier->-r requirements.txt (line 1)) (0.5.11)\r\n",
      "Requirement already satisfied: deprecated in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from pyterrier->-r requirements.txt (line 1)) (1.3.1)\r\n",
      "Requirement already satisfied: scipy in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from pyterrier->-r requirements.txt (line 1)) (1.15.3)\r\n",
      "Requirement already satisfied: ir_measures>=0.4.1 in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from pyterrier->-r requirements.txt (line 1)) (0.4.2)\r\n",
      "Requirement already satisfied: pytrec_eval_terrier>=0.5.3 in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from pyterrier->-r requirements.txt (line 1)) (0.5.10)\r\n",
      "Requirement already satisfied: jinja2 in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from pyterrier->-r requirements.txt (line 1)) (3.1.3)\r\n",
      "Requirement already satisfied: statsmodels in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from pyterrier->-r requirements.txt (line 1)) (0.14.5)\r\n",
      "Requirement already satisfied: dill in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from pyterrier->-r requirements.txt (line 1)) (0.4.0)\r\n",
      "Requirement already satisfied: lz4 in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from pyterrier->-r requirements.txt (line 1)) (4.4.5)\r\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from pyterrier->-r requirements.txt (line 1)) (4.12.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from pandas->-r requirements.txt (line 2)) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from pandas->-r requirements.txt (line 2)) (2024.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from pandas->-r requirements.txt (line 2)) (2024.1)\r\n",
      "Requirement already satisfied: filelock in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 3)) (3.20.2)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 3)) (0.36.0)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 3)) (23.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 3)) (6.0.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 3)) (2025.11.3)\r\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 3)) (0.22.2)\r\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 3)) (0.7.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from torch->-r requirements.txt (line 4)) (4.15.0)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from torch->-r requirements.txt (line 4)) (1.14.0)\r\n",
      "Requirement already satisfied: networkx>=2.5.1 in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from torch->-r requirements.txt (line 4)) (3.4.2)\r\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from torch->-r requirements.txt (line 4)) (2025.12.0)\r\n",
      "Requirement already satisfied: click in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from nltk->-r requirements.txt (line 5)) (8.3.1)\r\n",
      "Requirement already satisfied: joblib in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from nltk->-r requirements.txt (line 5)) (1.5.2)\r\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from spacy->-r requirements.txt (line 6)) (3.0.12)\r\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from spacy->-r requirements.txt (line 6)) (1.0.5)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from spacy->-r requirements.txt (line 6)) (1.0.15)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from spacy->-r requirements.txt (line 6)) (2.0.13)\r\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from spacy->-r requirements.txt (line 6)) (3.0.12)\r\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from spacy->-r requirements.txt (line 6)) (8.3.10)\r\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from spacy->-r requirements.txt (line 6)) (1.1.3)\r\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from spacy->-r requirements.txt (line 6)) (2.5.2)\r\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from spacy->-r requirements.txt (line 6)) (2.0.10)\r\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from spacy->-r requirements.txt (line 6)) (0.4.3)\r\n",
      "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from spacy->-r requirements.txt (line 6)) (0.21.1)\r\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from spacy->-r requirements.txt (line 6)) (2.12.5)\r\n",
      "Requirement already satisfied: setuptools in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from spacy->-r requirements.txt (line 6)) (44.1.1)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers->-r requirements.txt (line 3)) (1.2.0)\r\n",
      "Requirement already satisfied: inscriptis>=2.2.0 in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from ir_datasets>=0.3.2->pyterrier->-r requirements.txt (line 1)) (2.7.0)\r\n",
      "Requirement already satisfied: lxml>=4.5.2 in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from ir_datasets>=0.3.2->pyterrier->-r requirements.txt (line 1)) (5.4.0)\r\n",
      "Requirement already satisfied: trec-car-tools>=2.5.4 in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from ir_datasets>=0.3.2->pyterrier->-r requirements.txt (line 1)) (2.6)\r\n",
      "Requirement already satisfied: warc3-wet>=0.2.3 in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from ir_datasets>=0.3.2->pyterrier->-r requirements.txt (line 1)) (0.2.5)\r\n",
      "Requirement already satisfied: warc3-wet-clueweb09>=0.2.5 in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from ir_datasets>=0.3.2->pyterrier->-r requirements.txt (line 1)) (0.2.5)\r\n",
      "Requirement already satisfied: zlib-state>=0.1.3 in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from ir_datasets>=0.3.2->pyterrier->-r requirements.txt (line 1)) (0.1.10)\r\n",
      "Requirement already satisfied: ijson>=3.1.3 in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from ir_datasets>=0.3.2->pyterrier->-r requirements.txt (line 1)) (3.4.0.post0)\r\n",
      "Requirement already satisfied: unlzw3>=0.2.1 in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from ir_datasets>=0.3.2->pyterrier->-r requirements.txt (line 1)) (0.2.3)\r\n",
      "Requirement already satisfied: pyarrow>=16.1.0 in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from ir_datasets>=0.3.2->pyterrier->-r requirements.txt (line 1)) (22.0.0)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from beautifulsoup4->pyterrier->-r requirements.txt (line 1)) (2.5)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->-r requirements.txt (line 6)) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->-r requirements.txt (line 6)) (2.41.5)\r\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->-r requirements.txt (line 6)) (0.4.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 2)) (1.16.0)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from requests->pyterrier->-r requirements.txt (line 1)) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from requests->pyterrier->-r requirements.txt (line 1)) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from requests->pyterrier->-r requirements.txt (line 1)) (1.26.15)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from requests->pyterrier->-r requirements.txt (line 1)) (2024.2.2)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from sympy>=1.13.3->torch->-r requirements.txt (line 4)) (1.3.0)\r\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from thinc<8.4.0,>=8.3.4->spacy->-r requirements.txt (line 6)) (1.3.3)\r\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from thinc<8.4.0,>=8.3.4->spacy->-r requirements.txt (line 6)) (0.1.5)\r\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from weasel<0.5.0,>=0.4.2->spacy->-r requirements.txt (line 6)) (0.23.0)\r\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from weasel<0.5.0,>=0.4.2->spacy->-r requirements.txt (line 6)) (7.5.0)\r\n",
      "Requirement already satisfied: wrapt<3,>=1.10 in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from deprecated->pyterrier->-r requirements.txt (line 1)) (2.0.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from jinja2->pyterrier->-r requirements.txt (line 1)) (2.1.3)\r\n",
      "Requirement already satisfied: patsy>=0.5.6 in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from statsmodels->pyterrier->-r requirements.txt (line 1)) (1.0.2)\r\n",
      "Requirement already satisfied: cbor>=1.0.0 in /Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages (from trec-car-tools>=2.5.4->ir_datasets>=0.3.2->pyterrier->-r requirements.txt (line 1)) (1.0.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.3\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-10T11:53:07.708676Z",
     "start_time": "2026-01-10T11:53:07.268060Z"
    }
   },
   "source": [
    "# Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyterrier as pt\n",
    "import transformers\n",
    "import torch\n",
    "import nltk\n",
    "import spacy\n",
    "import shutil\n",
    "import matplotlib"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[36], line 10\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mspacy\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mshutil\u001B[39;00m\n\u001B[0;32m---> 10\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12366,
     "status": "ok",
     "timestamp": 1762962835550,
     "user": {
      "displayName": "Georgios Peikos",
      "userId": "04834132442165285194"
     },
     "user_tz": -60
    },
    "id": "4xBdfDsPYdLA",
    "outputId": "32103be7-1880-4ccb-ea70-6800e841dec1",
    "ExecuteTime": {
     "end_time": "2026-01-09T20:47:31.014515Z",
     "start_time": "2026-01-09T20:47:09.316198Z"
    }
   },
   "source": [
    "import os\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "!curl -L \"https://huggingface.co/datasets/Bhawna/ChroniclingAmericaQA/resolve/main/test.json?download=true\" -o data/test.json\n",
    "!curl -L \"https://huggingface.co/datasets/Bhawna/ChroniclingAmericaQA/resolve/main/train.json?download=true\" -o data/train.json\n",
    "!curl -L \"https://huggingface.co/datasets/Bhawna/ChroniclingAmericaQA/resolve/main/dev.json?download=true\" -o data/validation.json\n",
    "\n",
    "import json\n",
    "\n",
    "files = [\"data/train.json\", \"data/validation.json\", \"data/test.json\"]\n",
    "\n",
    "for path in files:\n",
    "    print(f\"\\n===== {path} =====\")\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            # Read a few hundred characters to see what kind of JSON it is\n",
    "            head = f.read(500)\n",
    "            print(\"Preview of first 500 characters:\\n\")\n",
    "            print(head[:500])\n",
    "        # Try to load only part of the file\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        if isinstance(data, list):\n",
    "            print(f\"\\nLoaded {len(data)} items (list).\")\n",
    "            print(\"Dictionary keys:\", list(data[0].keys()))\n",
    "            print(json.dumps(data[0], indent=2)[:600])\n",
    "        elif isinstance(data, dict):\n",
    "            print(\"\\nTop-level is a dictionary. Keys:\", list(data.keys()))\n",
    "            for k, v in data.items():\n",
    "                if isinstance(v, list):\n",
    "                    print(f\"Key '{k}' contains a list of {len(v)} items.\")\n",
    "                    if v:\n",
    "                        print(\"First item keys:\", list(v[0].keys()))\n",
    "                        print(json.dumps(v[0], indent=2)[:600])\n",
    "                        break\n",
    "        else:\n",
    "            print(f\"Unexpected top-level type: {type(data)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not parse {path} as JSON: {e}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\r\n",
      "100  1350  100  1350    0     0   4593      0 --:--:-- --:--:-- --:--:--  45910      0 --:--:-- --:--:-- --:--:--     0\r\n",
      "100 71.5M  100 71.5M    0     0  48.2M      0  0:00:01  0:00:01 --:--:-- 37.1M0:00:01 --:--:--  101M\r\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\r\n",
      "100  1358  100  1358    0     0   9738      0 --:--:-- --:--:-- --:--:--  9769\r\n",
      "100 1315M  100 1315M    0     0   104M      0  0:00:12  0:00:12 --:--:--  107M\r\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\r\n",
      "100  1350  100  1350    0     0   9903      0 --:--:-- --:--:-- --:--:--  9926\r\n",
      "100 71.8M  100 71.8M    0     0  86.2M      0 --:--:-- --:--:-- --:--:--  114M\r\n",
      "\n",
      "===== data/train.json =====\n",
      "Preview of first 500 characters:\n",
      "\n",
      "[\n",
      "    {\n",
      "        \"query_id\": \"train_1\",\n",
      "        \"question\": \"Who is the author of the book, \\\"Horrors of Slavery, or the American Turf in Tripoli\\\"?\",\n",
      "        \"answer\": \"WILLIAM RAY\",\n",
      "        \"org_answer\": \"WILLIAM RAY\",\n",
      "        \"para_id\": \"New_Hampshire_18070804_1\",\n",
      "        \"context\": \"Aiscellaneous Repository. From the Albany Register, WAR, OR A PROSPECT OF IT, From recent instances of British Outrage. BY: WILLIAM RAY, Author of the contemplated publication, entitled, \\u201cHorrors of Slavery, \n",
      "\n",
      "Loaded 439302 items (list).\n",
      "Dictionary keys: ['query_id', 'question', 'answer', 'org_answer', 'para_id', 'context', 'raw_ocr', 'publication_date', 'trans_que', 'trans_ans', 'url']\n",
      "{\n",
      "  \"query_id\": \"train_1\",\n",
      "  \"question\": \"Who is the author of the book, \\\"Horrors of Slavery, or the American Turf in Tripoli\\\"?\",\n",
      "  \"answer\": \"WILLIAM RAY\",\n",
      "  \"org_answer\": \"WILLIAM RAY\",\n",
      "  \"para_id\": \"New_Hampshire_18070804_1\",\n",
      "  \"context\": \"Aiscellaneous Repository. From the Albany Register, WAR, OR A PROSPECT OF IT, From recent instances of British Outrage. BY: WILLIAM RAY, Author of the contemplated publication, entitled, \\u201cHorrors of Slavery, or the American Turf in Tripoli,\\u201d VOTARIES of Freedom, arm! The British Lion roars! Legions of Valor, take th\\u2019 alarm\\u2014; Rash, ru\n",
      "\n",
      "===== data/validation.json =====\n",
      "Preview of first 500 characters:\n",
      "\n",
      "[\n",
      "    {\n",
      "        \"query_id\": \"val_1\",\n",
      "        \"question\": \"How much of the crew would Gerry want to shore up in a gale of wind?\",\n",
      "        \"answer\": \"half\",\n",
      "        \"org_answer\": \"half\",\n",
      "        \"para_id\": \"Maine_18100326_13\",\n",
      "        \"context\": \"But my lads, it was right to put in federal officers last year, and the crew knew it; they saw what was brewing well enough, and by the soul of me, if the federal men hadn't done what they did, Old Davy Jones would have had his clutches into our quarters \n",
      "\n",
      "Loaded 24111 items (list).\n",
      "Dictionary keys: ['query_id', 'question', 'answer', 'org_answer', 'para_id', 'context', 'raw_ocr', 'publication_date', 'trans_que', 'trans_ans', 'url']\n",
      "{\n",
      "  \"query_id\": \"val_1\",\n",
      "  \"question\": \"How much of the crew would Gerry want to shore up in a gale of wind?\",\n",
      "  \"answer\": \"half\",\n",
      "  \"org_answer\": \"half\",\n",
      "  \"para_id\": \"Maine_18100326_13\",\n",
      "  \"context\": \"But my lads, it was right to put in federal officers last year, and the crew knew it; they saw what was brewing well enough, and by the soul of me, if the federal men hadn't done what they did, Old Davy Jones would have had his clutches into our quarters long ago. And now who d'ye think they want to put in instead of our present Commander C. GORE, and the Chief Mate COBB, an old weather-beaten \n",
      "\n",
      "===== data/test.json =====\n",
      "Preview of first 500 characters:\n",
      "\n",
      "[\n",
      "    {\n",
      "        \"query_id\": \"test_1\",\n",
      "        \"question\": \"How many lots did Thomas Peirce have?\",\n",
      "        \"answer\": \"183\",\n",
      "        \"org_answer\": \"183\",\n",
      "        \"para_id\": \"New_Hampshire_18030125_16\",\n",
      "        \"context\": \"Axivil Roberts, part of lot 180 108 60 Capt. George Walker, 181 140 35 George Townson, 183 48 19 Samuel Snell, 184 36 9 Samuel Waterhouse, 185 24 6 John Parker, 186 36 10 John Davis, 187 45 20 John Cross, 188 15 4 Benjamin Cross, 189 50 13 Widow Gilman, 209 21 8 George Peirce, 2\n",
      "\n",
      "Loaded 24084 items (list).\n",
      "Dictionary keys: ['query_id', 'question', 'answer', 'org_answer', 'para_id', 'context', 'raw_ocr', 'publication_date', 'trans_que', 'trans_ans', 'url']\n",
      "{\n",
      "  \"query_id\": \"test_1\",\n",
      "  \"question\": \"How many lots did Thomas Peirce have?\",\n",
      "  \"answer\": \"183\",\n",
      "  \"org_answer\": \"183\",\n",
      "  \"para_id\": \"New_Hampshire_18030125_16\",\n",
      "  \"context\": \"Axivil Roberts, part of lot 180 108 60 Capt. George Walker, 181 140 35 George Townson, 183 48 19 Samuel Snell, 184 36 9 Samuel Waterhouse, 185 24 6 John Parker, 186 36 10 John Davis, 187 45 20 John Cross, 188 15 4 Benjamin Cross, 189 50 13 Widow Gilman, 209 21 8 George Peirce, 209 200 75 Thomas Peirce, 220 185 71 SIXTH RANGE: Col. Henry Sherburne, 241 552 150 Nathaniel Roberts, 249 30 8 Jonathan Patridge, 252 60 18 Jo\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mylmVIP9bu8y"
   },
   "source": [
    "# Create the Document Collection\n",
    "\n",
    "To do that, we create a new json file that contains the 'para_id', 'context', 'raw_ocr', 'publication_date' keys, for all para_id in the collection.\n",
    "\n",
    "para_id: is the id of a paragraph of a news paper page."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17568,
     "status": "ok",
     "timestamp": 1762962853135,
     "user": {
      "displayName": "Georgios Peikos",
      "userId": "04834132442165285194"
     },
     "user_tz": -60
    },
    "id": "nxch4FUUbxRw",
    "outputId": "d86e4179-defd-49d8-8b68-172c577ed825",
    "ExecuteTime": {
     "end_time": "2026-01-09T20:47:38.874339Z",
     "start_time": "2026-01-09T20:47:31.078468Z"
    }
   },
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "inputs = [\"data/train.json\", \"data/validation.json\", \"data/test.json\"]\n",
    "output = \"data/document_collection.json\"\n",
    "\n",
    "def load_list_or_empty(path):\n",
    "    if not os.path.exists(path) or os.path.getsize(path) == 0:\n",
    "        print(f\"Skipping {path} because it is missing or empty\")\n",
    "        return []\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        if isinstance(data, list):\n",
    "            return data\n",
    "        print(f\"Skipping {path} because it is not a list at the top level\")\n",
    "        return []\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Skipping {path} because it is not valid JSON\")\n",
    "        return []\n",
    "\n",
    "def project(recs):\n",
    "    out = []\n",
    "    for r in recs:\n",
    "        out.append({\n",
    "            \"para_id\": r.get(\"para_id\", \"\"),\n",
    "            \"context\": r.get(\"context\", \"\"),\n",
    "            \"raw_ocr\": r.get(\"raw_ocr\", \"\"),\n",
    "            \"publication_date\": r.get(\"publication_date\", \"\")\n",
    "        })\n",
    "    return out\n",
    "\n",
    "all_recs = []\n",
    "for p in inputs:\n",
    "    recs = load_list_or_empty(p)\n",
    "    print(f\"Loaded {len(recs)} records from {p}\")\n",
    "    all_recs.extend(project(recs))\n",
    "\n",
    "# deduplicate by para_id keeping the first one seen\n",
    "uniq = {}\n",
    "for rec in all_recs:\n",
    "    pid = rec.get(\"para_id\", \"\")\n",
    "    if pid and pid not in uniq:\n",
    "        uniq[pid] = rec\n",
    "\n",
    "result = list(uniq.values())\n",
    "\n",
    "with open(output, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Wrote {len(result)} records to {output}\")\n",
    "print(json.dumps(result[:3], indent=2))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 439302 records from data/train.json\n",
      "Loaded 24111 records from data/validation.json\n",
      "Loaded 24084 records from data/test.json\n",
      "Wrote 131921 records to data/document_collection.json\n",
      "[\n",
      "  {\n",
      "    \"para_id\": \"New_Hampshire_18070804_1\",\n",
      "    \"context\": \"Aiscellaneous Repository. From the Albany Register, WAR, OR A PROSPECT OF IT, From recent instances of British Outrage. BY: WILLIAM RAY, Author of the contemplated publication, entitled, \\u201cHorrors of Slavery, or the American Turf in Tripoli,\\u201d VOTARIES of Freedom, arm! The British Lion roars! Legions of Valor, take th\\u2019 alarm\\u2014; Rash, rush to guard our shores! Behold the horrid deed\\u2014 Your brethren gasping lie! Beneath a tyrant\\u2019s hand they bleed\\u2014 They groan\\u2014they faint\\u2014they die. Veterans of seventy-six, Awake the slumbering sword;\\u2014 Hearts of your murderous foes transfix\\u2014 'Tis vengeance gives the word. Remember Lexington, And Bunker\\u2019s tragic hill; \\u201cThe same who spilt your blood thereon, Your blood again would spill. Ye who have seen your wives, Your children, and your fires, Too British ruffians yield their lives, And roast in savage fires;\\u2014 Our cities lost in flames,\\u2014 Your mothers captive led; Rise and avenge their injured names, Ye kindred of the dead. But not Revenge alone, Should urge you to the field; Let Duty lead you firmly on, And justice be your shield. Sure as we fail to join And crush our impious foes, War, fire and sword, and death combine, And woes succeed to woes.\",\n",
      "    \"raw_ocr\": \"fAiscellancous Bepogitory.\\n. dvom the Albany Regifier,\\n. . WAR,OR A PROSPECT OF IT,\\nFrom recent inflances of Britifp Oulrage.\\n BY: WILLIAM RAY:\\nHAuthsr of the tontemplated publication, entitled,\\n\\u00ab Horrors of Slavery,or the American Turg\\nin Tripoli,\\u201d\\nVOT\\u2019RIES of Freedom, arm!\\n The British Lion roars !\\n Legions of Valor, take th\\u2019 alarm\\u2014 ;\\nRash, rush to guard our shores ! -\\n- Behold the horrid deed\\u2014 v\\n. Your brethren gasping lie!\\n Beneath a tyrant\\u2019s hand they bleed\\u2014 -\\nThey groan--they faint\\u2014they die.\\n _Vet\\u2019rans of seventy-six, i\\n. Awake the flumb\\u2019ring sword ;\\u2014 = *.\\n- Hearts of your murd\\u2019rous foes transfix\\u2014\\n*Tis vengeance gives the word.\\n Remember Lexington,\\nAnd Bunker\\u2019s tragic hill;\\n\\u201cThe fame who {pilt your blnod thereon,\\n- Your blnod again would spill,\\nYe who have fe\\u00e9n your wives,\\nYour childrea, and your fires,\\n7\\u2019oo British ruffians\\u2019 yield their lives, .\\n Apd roast in savage fires;\\u2014\\nQer cities loft in lames,\\u2014\\nYour mothers captive led;\\nRife and avenge their injur'd names, -\\nYe kindred of the dead: .\\n But not Revenge alone,\\nShould \\u2018grge you to the field ;\\n. Let Duty lead you firmly on, :\\n. Aond jultice be your shield.\\n Sure as we fail to join\\n\\u00a2 And eruih our impeous foes,\\n\\u00bb Woar, fire and sword, and dcath combie,\\n- And woes succeed to woes.\",\n",
      "    \"publication_date\": \"1807-08-04\"\n",
      "  },\n",
      "  {\n",
      "    \"para_id\": \"New_Hampshire_18070804_4\",\n",
      "    \"context\": \"Surely he above the rest of his fellow mortals, partakes of heaven here below, of bliss which none but the virtuous ever claim. \\u00a5 Obituary B In France, Gen. de Rosemberg, aged 83, formerly Marshal of France, Grand Officer of the Legion of Honor, and commander of the French troops in the United States during the Revolutionary war. \\u00a5 In Washington, Hon. Uriah Tracy, Esq. Senator of the United States from the State of Connecticut, aged 54\\u2014his pall was supported by the heads of department and officers of government\\u2014he had been sick at Washington since March last. In Baltimore, during the week ending the 18th ult. 15 adults and 23 children. In Philadelphia, during the week ending 18th ult.\\u201424 adults and 40 children. In New-York, during the week ending 18th ult, 2 men, 6 women, 10 boys and 7 girls. At Newark, (N. J.) the Rev. Dr. Alexander Macauley, aged 73. At Danvers, Dr. Amos Putnam, aged 83. At Andover, Mrs. Susanna Symmes, relict of the late Rev. Dr. Wm. Symmes, aged 79. At Salem, (Mass) Widow Margaret Swaffey, aged 100 years and 6 months. At Newburyport, Mr. Samuel Dexter, aged 36, only son of the late Lord T. Dexter. At Brunswick, Rev. Joseph McKean, D. D, late president of Bowdoin college. At Lancaster, Mr. Henry Haskell, aged 73, a Lt. Col. in the revolutionary army. At Chesterfield, Mrs. Louisa Parsons, wife of Benjamin Parsons, Esq. aged 39.\",\n",
      "    \"raw_ocr\": \"Surely he a\\nbove the rest of his fellow mortals, partakes\\nof heaven hcre below, of bliss which nene\\nbut the virtuous ever claim.\\n \\u00a5 OvoioDlEDoitry B\\n In France, Gen.de Rosbambesu, aged 8,\\nfarmerly s Marthall of France, Grand Ofa\\ncer of the Legion of Henor,and commander\\nof the Freach troops in the United States\\nduring the Revolutienary war. \\u00a5\\n In Wathington, Hon. Uriab Tracey, Esq.\\nYenator of the United States from-the State\\nof Connecticut, aged s4~\\u2014his pall was sup\\nported by the, heads of department and\\nofficers of goverament\\u2014he had been sick at\\nWashington since March lag.\\n In Baltumore, during the week ending the\\n18th uvit. 15 adults and 23 childiren.\\n In Philadelphia, during the week ending\\n18th u1t.~\\u201424 adults and 40 children.\\n ~ln New-York, during the week ending\\n18th nlt, 2 men, 6 womcen, 10 boys and 7\\ngirls, . : ' \\u00a2 \\u00a5 oo\\nAt Newark, (N. ].) the Rev, Dr. Alexander\\nMacauwgrther, aged 73. ; (\\n. At Daovers, Dt. Amos Putnam,aged 83.\\n At Andover, Mrs. Susanna Symmes, relict\\nof the late Rev. Dr. Wm. Symmes, aged 79.\\n At Salem, (Mass) Widow Margaret Swafey,\\naged 100 yearsand 6 months.\\n. At Newburyport, Mr.\\\" Samuel Dexter, aged\\n- 36, only son of the late Lord T. Dexter.\\n o At Bruafwick, Rev. Fo/eph McKean, D. D,\\nJate president of Bowdoin college.\\n At Lancaster, Mr. Henry Hajkell, aged 73,\\na Lt, Col. in the revolutionary army.\\n At Cheflterfield, Mrs Louisa Parsons, wifoy\\n-of Benjamin Parlons, Efq.aged 39. i\\n-\",\n",
      "    \"publication_date\": \"1807-08-04\"\n",
      "  },\n",
      "  {\n",
      "    \"para_id\": \"New_Hampshire_18070804_5\",\n",
      "    \"context\": \"At Westmoreland, Mrs. Sally Lincoln, wife of Mr. Spencer L. aged 28.  At Henrico, Mrs. Polly Adams, consort On Saturday, the 11th ult. Mr. Joseph Meyer, of Hampstead, was found dead in the road, (his horse standing by him) when oy e g e SMITH & RUST Pocket Book Lost.  \\\"LOST last Wednesday between 7 and 8 o\\u2019clock in the afternoon, either in the Globe Tavern at the Plains, or on the road leading from thence to Portsmouth, a new Red Morocco Pocket Book ; containing some Money, Notes of hand payable to the Subscriber, also, New Hampshire Fire and Marine Certificates, and other papers valuable to none but to the owner\\u2014Whoever shall find said Pocket Book, and re- turn it with its contents, with or without the money shall be handsomely rewarded, and the thanks of their humble servant EDWARD. PARRY.  TO BE LET, That Fireproof Store lately improved by Mr. Benjamin Swett, which must be allowed to be the best stand for business, either for English or West- India Goods in this town\\u2014Inquire of EDWARD PARRY, Who has a large assortment of the fashionable GOODS, for sale cheap for cash or short credit. July 28.  FOR SALE, A NEW GONDOLA, built of the best materials, and by an experienced workman, forty feet cor- ner piece,\\u2014For further particulars enquire of MICHAEL WIGGIN. Newmarket, July 27th, 1807 2. CHAISE.\",\n",
      "    \"raw_ocr\": \"At Weltmoreland, Mrs. Sally Liacoln, wife\\n~of Mr,Spencer L.aged 28. 77\\niAt Hennikee, Mrs. Polly Adams, consort\\n00 Baturday, the lith vit. Mr. Jo/iph\\nNeyer, of Hampftead, was found dead in the\\nroud. (his Borfe Randing by him) sbens oy\\ne g e\\nSMR R Y e sT R\\nPocket Book Loft.\\n i\\u201c OSS i'laft Wednesday between 7-\\nJM_4 and 8 o\\u2019clock in the afternoon,\\neither in the Globe Tavern at the\\nPlains, or cn the road leading from\\nthence to Portsmouth, a new j\\nRed Morscco Pocket Book ;\\ncontaining some Money, Notes of hand\\npayable to the Subscriber. alfs,\\nNewhampfhire Fire and Marine\\nCertificates, and other papers valuable\\nto no ore but to the owner\\u2014Whoev\\ner shall find said Pocket Book, and re-,\\nturn it wich its contents, with or with.\\n out the money {hall be hand{omely re\\nwarded, aand the thanks of their hum.\\nble servant EDWARD. PARRY.\\n 10 BE LET,\\nThat Fire proof Store lately improv\\ned by Mr. Benjamin Swett, which\\nmust be allowed to be the best stand\\nfor business, either for English or Welt-\\nIndia Goods in this town\\u2014llinquire\\nof \\\" EDWARD PARRY,\\nWho has a large assortment fieth\\nfathionable GOQODS, for {ale cheap\\nfor cath or short credit. Fuly 28.\\n FOR SALE,\\nANEW GUNDALO,\\nbuilt of the belt materials, and by an\\nexperienced workman, forty feet cor\\nner piece,\\u2014For further particulars\\nenquire of\\n\\\" MICHAEL WIGGIN.\\nNewmarket, July 27th, 1807\\n2 .\\nCHAISE.\\n\",\n",
      "    \"publication_date\": \"1807-08-04\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-9wljtri-XX"
   },
   "source": [
    "## You should check that the collection you have matches that of the paper!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-09T20:48:18.183091Z",
     "start_time": "2026-01-09T20:47:38.943826Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "for path in inputs:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        df_check = pd.read_json(path)\n",
    "        print(f'Shape of {path}: {df_check.shape}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data/train.json: (439302, 11)\n",
      "Shape of data/validation.json: (24111, 11)\n",
      "Shape of data/test.json: (24084, 11)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimensions match the ones of the paper at https://github.com/DataScienceUIBK/ChroniclingAmericaQA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "snY9dkltgMts"
   },
   "source": [
    "# Create the Test Queries Data Structure\n",
    "\n",
    "We keep the first 10.000 queries due to memory errors in the free colab version.\n",
    "\n",
    "To be comparable, please keep the top 10.000 queries for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1151,
     "status": "ok",
     "timestamp": 1762962872929,
     "user": {
      "displayName": "Georgios Peikos",
      "userId": "04834132442165285194"
     },
     "user_tz": -60
    },
    "id": "7ZOmr1qBgRxi",
    "outputId": "1a4cbaaa-2813-4814-e0de-aee5aab98f7c",
    "ExecuteTime": {
     "end_time": "2026-01-09T20:48:18.918821Z",
     "start_time": "2026-01-09T20:48:18.595046Z"
    }
   },
   "source": [
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "input_file = \"data/test.json\"\n",
    "output_file = \"data/test_queries.json\"\n",
    "\n",
    "# Load the data\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "def clean_question(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    text = re.sub(rf\"[{re.escape(string.punctuation)}]\", \" \", text)  # remove punctuation\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # collapse multiple spaces\n",
    "    return text.strip()\n",
    "\n",
    "# Extract and clean\n",
    "queries = [\n",
    "    {\n",
    "        \"query_id\": item.get(\"query_id\", \"\"),\n",
    "        \"question\": clean_question(item.get(\"question\", \"\")),\n",
    "    }\n",
    "    for item in data\n",
    "]\n",
    "\n",
    "# Sort by query_id (assuming numeric)\n",
    "queries = sorted(queries, key=lambda x: int(x[\"query_id\"]) if str(x[\"query_id\"]).isdigit() else x[\"query_id\"])\n",
    "\n",
    "# Keep only the first 10,000\n",
    "queries = queries[:10000]\n",
    "\n",
    "# Save new JSON\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(queries, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Saved {len(queries)} entries to {output_file}\")\n",
    "print(json.dumps(queries[:3], indent=2))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 10000 entries to data/test_queries.json\n",
      "[\n",
      "  {\n",
      "    \"query_id\": \"test_1\",\n",
      "    \"question\": \"How many lots did Thomas Peirce have\"\n",
      "  },\n",
      "  {\n",
      "    \"query_id\": \"test_10\",\n",
      "    \"question\": \"Who gave Hamilton the substance of what he had proposed on the part of General Hamilton\"\n",
      "  },\n",
      "  {\n",
      "    \"query_id\": \"test_100\",\n",
      "    \"question\": \"Who informs his FRIENDS and the PUBLIC that he has taken that justly celebrated INN in this city\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6NyCV6oqjFS0"
   },
   "source": [
    "# Create the Qrels for the test set"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 742,
     "status": "ok",
     "timestamp": 1762962873672,
     "user": {
      "displayName": "Georgios Peikos",
      "userId": "04834132442165285194"
     },
     "user_tz": -60
    },
    "id": "Lxms9bHpjIcn",
    "outputId": "26e9db71-b590-4f5f-94db-484d857db80c",
    "ExecuteTime": {
     "end_time": "2026-01-09T20:48:19.310464Z",
     "start_time": "2026-01-09T20:48:18.929060Z"
    }
   },
   "source": [
    "input_file = \"data/test.json\"\n",
    "qrels_file = \"data/test_qrels.json\"\n",
    "answers_file = \"data/test_query_answers.json\"\n",
    "\n",
    "# Load the data\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Build the qrels file: query_id, iteration=0, para_id, relevance=1\n",
    "qrels = [\n",
    "    {\n",
    "        \"query_id\": item.get(\"query_id\", \"\"),\n",
    "        \"iteration\": 0,\n",
    "        \"para_id\": item.get(\"para_id\", \"\"),\n",
    "        \"relevance\": 1\n",
    "    }\n",
    "    for item in data\n",
    "]\n",
    "\n",
    "# Build the query_answers file: same plus answer and org_answer\n",
    "query_answers = [\n",
    "    {\n",
    "        \"query_id\": item.get(\"query_id\", \"\"),\n",
    "        \"iteration\": 0,\n",
    "        \"para_id\": item.get(\"para_id\", \"\"),\n",
    "        \"relevance\": 1,\n",
    "        \"answer\": item.get(\"answer\", \"\"),\n",
    "        \"org_answer\": item.get(\"org_answer\", \"\")\n",
    "    }\n",
    "    for item in data\n",
    "]\n",
    "\n",
    "# Save both files\n",
    "with open(qrels_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(qrels, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(answers_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(query_answers, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Saved {len(qrels)} entries to {qrels_file}\")\n",
    "print(f\"Saved {len(query_answers)} entries to {answers_file}\")\n",
    "print(\"Sample qrels entry:\", qrels[0])\n",
    "print(\"Sample query_answers entry:\", query_answers[0])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 24084 entries to data/test_qrels.json\n",
      "Saved 24084 entries to data/test_query_answers.json\n",
      "Sample qrels entry: {'query_id': 'test_1', 'iteration': 0, 'para_id': 'New_Hampshire_18030125_16', 'relevance': 1}\n",
      "Sample query_answers entry: {'query_id': 'test_1', 'iteration': 0, 'para_id': 'New_Hampshire_18030125_16', 'relevance': 1, 'answer': '183', 'org_answer': '183'}\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7vkoP010nIF"
   },
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract data from json files"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-09T20:48:46.175534Z",
     "start_time": "2026-01-09T20:48:19.321089Z"
    }
   },
   "source": [
    "input_files = ['data/document_collection.json', 'data/test.json', 'data/test_qrels.json', 'data/test_queries.json', 'data/test_query_answers.json', 'data/train.json', 'data/validation.json']\n",
    "\n",
    "dataframes = {}\n",
    "for input_file in input_files:\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        dataframes[input_file] = pd.read_json(input_file)"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's visualize data and analyze them"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-09T20:48:46.677299Z",
     "start_time": "2026-01-09T20:48:46.589337Z"
    }
   },
   "source": [
    "dataframes['data/document_collection.json']"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                         para_id  \\\n",
       "0       New_Hampshire_18070804_1   \n",
       "1       New_Hampshire_18070804_4   \n",
       "2       New_Hampshire_18070804_5   \n",
       "3       New_Hampshire_18070804_8   \n",
       "4       New_Hampshire_18070804_9   \n",
       "...                          ...   \n",
       "131916       Nebraska_19130626_7   \n",
       "131917        Indiana_19170719_6   \n",
       "131918       Kentucky_19110727_5   \n",
       "131919  Rhode_Island_19140626_10   \n",
       "131920        Florida_19150128_7   \n",
       "\n",
       "                                                  context  \\\n",
       "0       Aiscellaneous Repository. From the Albany Regi...   \n",
       "1       Surely he above the rest of his fellow mortals...   \n",
       "2       At Westmoreland, Mrs. Sally Lincoln, wife of M...   \n",
       "3       Upon the correction of this remedy the stomach...   \n",
       "4       Also FOR SALE AS ABOVE, NEW GOODS, STEPHEN HAR...   \n",
       "...                                                   ...   \n",
       "131916  \"Did you?” said Fran politely. “So father grad...   \n",
       "131917  When a boy begins to learn a trade, the \"play ...   \n",
       "131918  It is situated in the valley of the great many...   \n",
       "131919  A PRACTICAL LESSON IN AGRICULTURE, MAY 1708, T...   \n",
       "131920  The plans call for the grading, smoothing, and...   \n",
       "\n",
       "                                                  raw_ocr publication_date  \n",
       "0       fAiscellancous Bepogitory.\\n. dvom the Albany ...       1807-08-04  \n",
       "1       Surely he a\\nbove the rest of his fellow morta...       1807-08-04  \n",
       "2       At Weltmoreland, Mrs. Sally Liacoln, wife\\n~of...       1807-08-04  \n",
       "3       tion of this remedy the flomach is invariably\\...       1807-08-04  \n",
       "4       *°\\n, ALSO POR SALE AS ABOVE,\\no NEW-GEODS, -\\...       1807-08-04  \n",
       "...                                                   ...              ...  \n",
       "131916  \"Did you?” said Fran politely. “So\\nfather gra...       1913-06-26  \n",
       "131917  When a boy begins to learn a trade, the \"play\\...       1917-07-19  \n",
       "131918  It is ftltuaiod In tho val\\nley of Uio great n...       1911-07-27  \n",
       "131919  A PRACITICA, ZESSQN 2V AGRICTZ,\\nLEFY 170 RIGH...       1914-06-26  \n",
       "131920  The\\nplans call for the grading, smoothing\\nan...       1915-01-28  \n",
       "\n",
       "[131921 rows x 4 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>para_id</th>\n",
       "      <th>context</th>\n",
       "      <th>raw_ocr</th>\n",
       "      <th>publication_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>New_Hampshire_18070804_1</td>\n",
       "      <td>Aiscellaneous Repository. From the Albany Regi...</td>\n",
       "      <td>fAiscellancous Bepogitory.\\n. dvom the Albany ...</td>\n",
       "      <td>1807-08-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>New_Hampshire_18070804_4</td>\n",
       "      <td>Surely he above the rest of his fellow mortals...</td>\n",
       "      <td>Surely he a\\nbove the rest of his fellow morta...</td>\n",
       "      <td>1807-08-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>New_Hampshire_18070804_5</td>\n",
       "      <td>At Westmoreland, Mrs. Sally Lincoln, wife of M...</td>\n",
       "      <td>At Weltmoreland, Mrs. Sally Liacoln, wife\\n~of...</td>\n",
       "      <td>1807-08-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>New_Hampshire_18070804_8</td>\n",
       "      <td>Upon the correction of this remedy the stomach...</td>\n",
       "      <td>tion of this remedy the flomach is invariably\\...</td>\n",
       "      <td>1807-08-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>New_Hampshire_18070804_9</td>\n",
       "      <td>Also FOR SALE AS ABOVE, NEW GOODS, STEPHEN HAR...</td>\n",
       "      <td>*°\\n, ALSO POR SALE AS ABOVE,\\no NEW-GEODS, -\\...</td>\n",
       "      <td>1807-08-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131916</th>\n",
       "      <td>Nebraska_19130626_7</td>\n",
       "      <td>\"Did you?” said Fran politely. “So father grad...</td>\n",
       "      <td>\"Did you?” said Fran politely. “So\\nfather gra...</td>\n",
       "      <td>1913-06-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131917</th>\n",
       "      <td>Indiana_19170719_6</td>\n",
       "      <td>When a boy begins to learn a trade, the \"play ...</td>\n",
       "      <td>When a boy begins to learn a trade, the \"play\\...</td>\n",
       "      <td>1917-07-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131918</th>\n",
       "      <td>Kentucky_19110727_5</td>\n",
       "      <td>It is situated in the valley of the great many...</td>\n",
       "      <td>It is ftltuaiod In tho val\\nley of Uio great n...</td>\n",
       "      <td>1911-07-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131919</th>\n",
       "      <td>Rhode_Island_19140626_10</td>\n",
       "      <td>A PRACTICAL LESSON IN AGRICULTURE, MAY 1708, T...</td>\n",
       "      <td>A PRACITICA, ZESSQN 2V AGRICTZ,\\nLEFY 170 RIGH...</td>\n",
       "      <td>1914-06-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131920</th>\n",
       "      <td>Florida_19150128_7</td>\n",
       "      <td>The plans call for the grading, smoothing, and...</td>\n",
       "      <td>The\\nplans call for the grading, smoothing\\nan...</td>\n",
       "      <td>1915-01-28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131921 rows × 4 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-09T20:48:47.583453Z",
     "start_time": "2026-01-09T20:48:47.574829Z"
    }
   },
   "source": [
    "dataframes['data/train.json']"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            query_id                                           question  \\\n",
       "0            train_1  Who is the author of the book, \"Horrors of Sla...   \n",
       "1            train_2  Who was the Grand Officer of the Legion of Honor?   \n",
       "2            train_3  What country was Gen. de Rosemberg formerly Ma...   \n",
       "3            train_4          What was the title of Rev. Joseph McKean?   \n",
       "4            train_5  Who was the wife of Mr. Spencer L. at Westmore...   \n",
       "...              ...                                                ...   \n",
       "439297  train_439298         Who is the pastor of First Baptist Church?   \n",
       "439298  train_439299  What day of the week is the school for Arthur ...   \n",
       "439299  train_439300  How long has CHICHESTER'S DIAMOND BRAND PILLS ...   \n",
       "439300  train_439301  On what day of the week is Trinity Lutheran sc...   \n",
       "439301  train_439302  Who is the rector of the Church of the Ascension?   \n",
       "\n",
       "                  answer      org_answer                    para_id  \\\n",
       "0            WILLIAM RAY     WILLIAM RAY   New_Hampshire_18070804_1   \n",
       "1           de Rosemberg    de Rosemberg   New_Hampshire_18070804_4   \n",
       "2                 France          France   New_Hampshire_18070804_4   \n",
       "3           de Rosemberg    de Rosemberg   New_Hampshire_18070804_4   \n",
       "4          Sally Lincoln   Sally Lincoln   New_Hampshire_18070804_5   \n",
       "...                  ...             ...                        ...   \n",
       "439297    W. R. Bradshaw  W. R. Bradshaw  North_Carolina_19181130_6   \n",
       "439298  November 24,1918          Sunday  North_Carolina_19181130_6   \n",
       "439299          25 years        25 years  North_Carolina_19181130_7   \n",
       "439300  November 24,1918          Sunday  North_Carolina_19181130_7   \n",
       "439301      S. B. Stroup    S. B. Stroup  North_Carolina_19181130_7   \n",
       "\n",
       "                                                  context  \\\n",
       "0       Aiscellaneous Repository. From the Albany Regi...   \n",
       "1       Surely he above the rest of his fellow mortals...   \n",
       "2       Surely he above the rest of his fellow mortals...   \n",
       "3       Surely he above the rest of his fellow mortals...   \n",
       "4       At Westmoreland, Mrs. Sally Lincoln, wife of M...   \n",
       "...                                                   ...   \n",
       "439297  He went to school in France, received his comm...   \n",
       "439298  He went to school in France, received his comm...   \n",
       "439299  There are plenty of such gifts; you'll find lo...   \n",
       "439300  There are plenty of such gifts; you'll find lo...   \n",
       "439301  There are plenty of such gifts; you'll find lo...   \n",
       "\n",
       "                                                  raw_ocr publication_date  \\\n",
       "0       fAiscellancous Bepogitory.\\n. dvom the Albany ...       1807-08-04   \n",
       "1       Surely he a\\nbove the rest of his fellow morta...       1807-08-04   \n",
       "2       Surely he a\\nbove the rest of his fellow morta...       1807-08-04   \n",
       "3       Surely he a\\nbove the rest of his fellow morta...       1807-08-04   \n",
       "4       At Weltmoreland, Mrs. Sally Liacoln, wife\\n~of...       1807-08-04   \n",
       "...                                                   ...              ...   \n",
       "439297  He went\\nto school in' France, received his\\nc...       1918-11-30   \n",
       "439298  He went\\nto school in' France, received his\\nc...       1918-11-30   \n",
       "439299  There are plenty of such gifts; yon'll find\\nl...       1918-11-30   \n",
       "439300  There are plenty of such gifts; yon'll find\\nl...       1918-11-30   \n",
       "439301  There are plenty of such gifts; yon'll find\\nl...       1918-11-30   \n",
       "\n",
       "        trans_que  trans_ans  \\\n",
       "0               0          0   \n",
       "1               0          0   \n",
       "2               0          0   \n",
       "3               0          0   \n",
       "4               0          0   \n",
       "...           ...        ...   \n",
       "439297          0          0   \n",
       "439298          0          1   \n",
       "439299          0          0   \n",
       "439300          0          1   \n",
       "439301          0          0   \n",
       "\n",
       "                                                      url  \n",
       "0       https://chroniclingamerica.loc.gov/lccn/sn8302...  \n",
       "1       https://chroniclingamerica.loc.gov/lccn/sn8302...  \n",
       "2       https://chroniclingamerica.loc.gov/lccn/sn8302...  \n",
       "3       https://chroniclingamerica.loc.gov/lccn/sn8302...  \n",
       "4       https://chroniclingamerica.loc.gov/lccn/sn8302...  \n",
       "...                                                   ...  \n",
       "439297  https://chroniclingamerica.loc.gov/lccn/sn9106...  \n",
       "439298  https://chroniclingamerica.loc.gov/lccn/sn9106...  \n",
       "439299  https://chroniclingamerica.loc.gov/lccn/sn9106...  \n",
       "439300  https://chroniclingamerica.loc.gov/lccn/sn9106...  \n",
       "439301  https://chroniclingamerica.loc.gov/lccn/sn9106...  \n",
       "\n",
       "[439302 rows x 11 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>org_answer</th>\n",
       "      <th>para_id</th>\n",
       "      <th>context</th>\n",
       "      <th>raw_ocr</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>trans_que</th>\n",
       "      <th>trans_ans</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_1</td>\n",
       "      <td>Who is the author of the book, \"Horrors of Sla...</td>\n",
       "      <td>WILLIAM RAY</td>\n",
       "      <td>WILLIAM RAY</td>\n",
       "      <td>New_Hampshire_18070804_1</td>\n",
       "      <td>Aiscellaneous Repository. From the Albany Regi...</td>\n",
       "      <td>fAiscellancous Bepogitory.\\n. dvom the Albany ...</td>\n",
       "      <td>1807-08-04</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://chroniclingamerica.loc.gov/lccn/sn8302...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_2</td>\n",
       "      <td>Who was the Grand Officer of the Legion of Honor?</td>\n",
       "      <td>de Rosemberg</td>\n",
       "      <td>de Rosemberg</td>\n",
       "      <td>New_Hampshire_18070804_4</td>\n",
       "      <td>Surely he above the rest of his fellow mortals...</td>\n",
       "      <td>Surely he a\\nbove the rest of his fellow morta...</td>\n",
       "      <td>1807-08-04</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://chroniclingamerica.loc.gov/lccn/sn8302...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_3</td>\n",
       "      <td>What country was Gen. de Rosemberg formerly Ma...</td>\n",
       "      <td>France</td>\n",
       "      <td>France</td>\n",
       "      <td>New_Hampshire_18070804_4</td>\n",
       "      <td>Surely he above the rest of his fellow mortals...</td>\n",
       "      <td>Surely he a\\nbove the rest of his fellow morta...</td>\n",
       "      <td>1807-08-04</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://chroniclingamerica.loc.gov/lccn/sn8302...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_4</td>\n",
       "      <td>What was the title of Rev. Joseph McKean?</td>\n",
       "      <td>de Rosemberg</td>\n",
       "      <td>de Rosemberg</td>\n",
       "      <td>New_Hampshire_18070804_4</td>\n",
       "      <td>Surely he above the rest of his fellow mortals...</td>\n",
       "      <td>Surely he a\\nbove the rest of his fellow morta...</td>\n",
       "      <td>1807-08-04</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://chroniclingamerica.loc.gov/lccn/sn8302...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_5</td>\n",
       "      <td>Who was the wife of Mr. Spencer L. at Westmore...</td>\n",
       "      <td>Sally Lincoln</td>\n",
       "      <td>Sally Lincoln</td>\n",
       "      <td>New_Hampshire_18070804_5</td>\n",
       "      <td>At Westmoreland, Mrs. Sally Lincoln, wife of M...</td>\n",
       "      <td>At Weltmoreland, Mrs. Sally Liacoln, wife\\n~of...</td>\n",
       "      <td>1807-08-04</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://chroniclingamerica.loc.gov/lccn/sn8302...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439297</th>\n",
       "      <td>train_439298</td>\n",
       "      <td>Who is the pastor of First Baptist Church?</td>\n",
       "      <td>W. R. Bradshaw</td>\n",
       "      <td>W. R. Bradshaw</td>\n",
       "      <td>North_Carolina_19181130_6</td>\n",
       "      <td>He went to school in France, received his comm...</td>\n",
       "      <td>He went\\nto school in' France, received his\\nc...</td>\n",
       "      <td>1918-11-30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://chroniclingamerica.loc.gov/lccn/sn9106...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439298</th>\n",
       "      <td>train_439299</td>\n",
       "      <td>What day of the week is the school for Arthur ...</td>\n",
       "      <td>November 24,1918</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>North_Carolina_19181130_6</td>\n",
       "      <td>He went to school in France, received his comm...</td>\n",
       "      <td>He went\\nto school in' France, received his\\nc...</td>\n",
       "      <td>1918-11-30</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>https://chroniclingamerica.loc.gov/lccn/sn9106...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439299</th>\n",
       "      <td>train_439300</td>\n",
       "      <td>How long has CHICHESTER'S DIAMOND BRAND PILLS ...</td>\n",
       "      <td>25 years</td>\n",
       "      <td>25 years</td>\n",
       "      <td>North_Carolina_19181130_7</td>\n",
       "      <td>There are plenty of such gifts; you'll find lo...</td>\n",
       "      <td>There are plenty of such gifts; yon'll find\\nl...</td>\n",
       "      <td>1918-11-30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://chroniclingamerica.loc.gov/lccn/sn9106...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439300</th>\n",
       "      <td>train_439301</td>\n",
       "      <td>On what day of the week is Trinity Lutheran sc...</td>\n",
       "      <td>November 24,1918</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>North_Carolina_19181130_7</td>\n",
       "      <td>There are plenty of such gifts; you'll find lo...</td>\n",
       "      <td>There are plenty of such gifts; yon'll find\\nl...</td>\n",
       "      <td>1918-11-30</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>https://chroniclingamerica.loc.gov/lccn/sn9106...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439301</th>\n",
       "      <td>train_439302</td>\n",
       "      <td>Who is the rector of the Church of the Ascension?</td>\n",
       "      <td>S. B. Stroup</td>\n",
       "      <td>S. B. Stroup</td>\n",
       "      <td>North_Carolina_19181130_7</td>\n",
       "      <td>There are plenty of such gifts; you'll find lo...</td>\n",
       "      <td>There are plenty of such gifts; yon'll find\\nl...</td>\n",
       "      <td>1918-11-30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://chroniclingamerica.loc.gov/lccn/sn9106...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>439302 rows × 11 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: in `data/document_collection.json` the rows are already deduplicated**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Preprocessing_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Linguistic Processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalization\n",
    "We lowercase everything and remove all special characters/tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> 1st step normalization"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-09T20:48:59.155590Z",
     "start_time": "2026-01-09T20:48:47.729563Z"
    }
   },
   "source": [
    "def normalize_text1(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    #text = text.lower()\n",
    "    text = re.sub(r'<[^>]+>', ' ', text) # HTML\n",
    "    # text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() # multiple white spaces\n",
    "    return text\n",
    "\n",
    "# in caso togliessimo la NER vanno tolti i commenti nella funzione qui sopra\n",
    "\n",
    "docColl = dataframes['data/document_collection.json']\n",
    "docColl_contNorm1 = docColl['context'].apply(normalize_text1)\n",
    "docColl_ocrNorm1 = docColl['raw_ocr'].apply(normalize_text1)\n",
    "docColl_Norm1 = docColl.copy()"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-09T20:48:59.504712Z",
     "start_time": "2026-01-09T20:48:59.321420Z"
    }
   },
   "source": [
    "docColl_Norm1['context'] = docColl_contNorm1\n",
    "docColl_Norm1['raw_ocr'] = docColl_ocrNorm1\n",
    "docColl_Norm1.head(25)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                      para_id  \\\n",
       "0    New_Hampshire_18070804_1   \n",
       "1    New_Hampshire_18070804_4   \n",
       "2    New_Hampshire_18070804_5   \n",
       "3    New_Hampshire_18070804_8   \n",
       "4    New_Hampshire_18070804_9   \n",
       "5   New_Hampshire_18070804_13   \n",
       "6   New_Hampshire_18070804_14   \n",
       "7   New_Hampshire_18070804_16   \n",
       "8   New_Hampshire_18070804_18   \n",
       "9    New_Hampshire_18060715_1   \n",
       "10   New_Hampshire_18060715_3   \n",
       "11   New_Hampshire_18060715_6   \n",
       "12   New_Hampshire_18060715_7   \n",
       "13  New_Hampshire_18060715_10   \n",
       "14  New_Hampshire_18060715_13   \n",
       "15  New_Hampshire_18060715_14   \n",
       "16  New_Hampshire_18060715_15   \n",
       "17  New_Hampshire_18060715_16   \n",
       "18  New_Hampshire_18060715_18   \n",
       "19   New_Hampshire_18070915_1   \n",
       "20   New_Hampshire_18070915_3   \n",
       "21   New_Hampshire_18070915_4   \n",
       "22   New_Hampshire_18070915_7   \n",
       "23   New_Hampshire_18070915_8   \n",
       "24   New_Hampshire_18070915_9   \n",
       "\n",
       "                                              context  \\\n",
       "0   Aiscellaneous Repository. From the Albany Regi...   \n",
       "1   Surely he above the rest of his fellow mortals...   \n",
       "2   At Westmoreland, Mrs. Sally Lincoln, wife of M...   \n",
       "3   Upon the correction of this remedy the stomach...   \n",
       "4   Also FOR SALE AS ABOVE, NEW GOODS, STEPHEN HAR...   \n",
       "5   At a meeting of the committee of the : subscri...   \n",
       "6   Notice is hereby given to the proprietors of t...   \n",
       "7   ‘ . LO, L, George Frost, Esq. X 30 I 14 2 1 3 ...   \n",
       "8   Swedes do. 150 bbls. fresh FLOUR. MOLASSES and...   \n",
       "9   The Portsmouthaathinna in the Miscellaneous Re...   \n",
       "10  From the above data, it is evident that the id...   \n",
       "11  A large assortment of Joiner’s Stock’t Tools; ...   \n",
       "12  \"170 be sold by order of the Judge of Probate ...   \n",
       "13  A variety of ELEGANT JEWELRY, and most other a...   \n",
       "14  1 & 2 containing 125 acres each, and No. 3, 15...   \n",
       "15  And your petitioner in fine fays, that it is n...   \n",
       "16  ing, dated June 1st, 1892, agreed to sell to J...   \n",
       "17  Apply the H. & B. PENHALLOW. NEW STORE. WILLIA...   \n",
       "18  Portsmouth, May 27, 1856. MW Daniel Clay’s Est...   \n",
       "19  Tuesday, September 15, 1807. Trial of Aaron Bu...   \n",
       "20  Warlike or threatening manner, and must displa...   \n",
       "21  He would therefore to be furnished with the op...   \n",
       "22  Unfortunately, Capt. C. brought no papers; and...   \n",
       "23  The crew consisted chiefly of deserters from L...   \n",
       "24  To this order no attention was paid; she conti...   \n",
       "\n",
       "                                              raw_ocr publication_date  \n",
       "0   fAiscellancous Bepogitory. . dvom the Albany R...       1807-08-04  \n",
       "1   Surely he a bove the rest of his fellow mortal...       1807-08-04  \n",
       "2   At Weltmoreland, Mrs. Sally Liacoln, wife ~of ...       1807-08-04  \n",
       "3   tion of this remedy the flomach is invariably ...       1807-08-04  \n",
       "4   *° , ALSO POR SALE AS ABOVE, o NEW-GEODS, - ST...       1807-08-04  \n",
       "5   At a meeting of the committee of the : fcfijcr...       1807-08-04  \n",
       "6   N OTICE is hereby given to these propri- X eto...       1807-08-04  \n",
       "7   ‘ . LO, L, Gegrge Frofl, Efg. X 30 ‘I 14 2 1 3...       1807-08-04  \n",
       "8   Swedes do. 150 bbls. freth FLOUR. MOI.ASSES an...       1807-08-04  \n",
       "9   P tsmnatahiniinni il z Miscellaneous Repositor...       1806-07-15  \n",
       "10  — Yrom the above data it is evident, that the ...       1806-07-15  \n",
       "11  a large assortment of Joiner’s Stock’t T'dols ...       1806-07-15  \n",
       "12  \"170 be fold by order of the Judge of Probate ...       1806-07-15  \n",
       "13  A 4 variety of ELEGANT JEWELRY, and moflt othe...       1806-07-15  \n",
       "14  1 & 2 containg 125 acres each, and No. 3, 150 ...       1806-07-15  \n",
       "15  —And your petitioner in fi& fays, that it is n...       1806-07-15  \n",
       "16  ing, dated June If}, 1892, agreed 10 {zil ove ...       1806-07-15  \n",
       "17  - Apply te ' H. & B. PENHALLOW. . #¥ NEW-STORE...       1806-07-15  \n",
       "18  Porifmonth, May 27, 1856. MW Danicl Clay’s Est...       1806-07-15  \n",
       "19  TUESDAY, SEPTEMBER 15,1807. TRIAL or AARON BUR...       1807-09-15  \n",
       "20  Wwarlike or threatening manner, ahd muit displ...       1807-09-15  \n",
       "21  He wilhgy therefore to be fucnithed with the o...       1807-09-15  \n",
       "22  Unfortunately Capt. C..brought no papers ; and...       1807-09-15  \n",
       "23  — The crew consisted chiefly of deserters from...       1807-09-15  \n",
       "24  To thi‘% order no attention was paid ; she ron...       1807-09-15  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>para_id</th>\n",
       "      <th>context</th>\n",
       "      <th>raw_ocr</th>\n",
       "      <th>publication_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>New_Hampshire_18070804_1</td>\n",
       "      <td>Aiscellaneous Repository. From the Albany Regi...</td>\n",
       "      <td>fAiscellancous Bepogitory. . dvom the Albany R...</td>\n",
       "      <td>1807-08-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>New_Hampshire_18070804_4</td>\n",
       "      <td>Surely he above the rest of his fellow mortals...</td>\n",
       "      <td>Surely he a bove the rest of his fellow mortal...</td>\n",
       "      <td>1807-08-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>New_Hampshire_18070804_5</td>\n",
       "      <td>At Westmoreland, Mrs. Sally Lincoln, wife of M...</td>\n",
       "      <td>At Weltmoreland, Mrs. Sally Liacoln, wife ~of ...</td>\n",
       "      <td>1807-08-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>New_Hampshire_18070804_8</td>\n",
       "      <td>Upon the correction of this remedy the stomach...</td>\n",
       "      <td>tion of this remedy the flomach is invariably ...</td>\n",
       "      <td>1807-08-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>New_Hampshire_18070804_9</td>\n",
       "      <td>Also FOR SALE AS ABOVE, NEW GOODS, STEPHEN HAR...</td>\n",
       "      <td>*° , ALSO POR SALE AS ABOVE, o NEW-GEODS, - ST...</td>\n",
       "      <td>1807-08-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>New_Hampshire_18070804_13</td>\n",
       "      <td>At a meeting of the committee of the : subscri...</td>\n",
       "      <td>At a meeting of the committee of the : fcfijcr...</td>\n",
       "      <td>1807-08-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>New_Hampshire_18070804_14</td>\n",
       "      <td>Notice is hereby given to the proprietors of t...</td>\n",
       "      <td>N OTICE is hereby given to these propri- X eto...</td>\n",
       "      <td>1807-08-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>New_Hampshire_18070804_16</td>\n",
       "      <td>‘ . LO, L, George Frost, Esq. X 30 I 14 2 1 3 ...</td>\n",
       "      <td>‘ . LO, L, Gegrge Frofl, Efg. X 30 ‘I 14 2 1 3...</td>\n",
       "      <td>1807-08-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>New_Hampshire_18070804_18</td>\n",
       "      <td>Swedes do. 150 bbls. fresh FLOUR. MOLASSES and...</td>\n",
       "      <td>Swedes do. 150 bbls. freth FLOUR. MOI.ASSES an...</td>\n",
       "      <td>1807-08-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>New_Hampshire_18060715_1</td>\n",
       "      <td>The Portsmouthaathinna in the Miscellaneous Re...</td>\n",
       "      <td>P tsmnatahiniinni il z Miscellaneous Repositor...</td>\n",
       "      <td>1806-07-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>New_Hampshire_18060715_3</td>\n",
       "      <td>From the above data, it is evident that the id...</td>\n",
       "      <td>— Yrom the above data it is evident, that the ...</td>\n",
       "      <td>1806-07-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>New_Hampshire_18060715_6</td>\n",
       "      <td>A large assortment of Joiner’s Stock’t Tools; ...</td>\n",
       "      <td>a large assortment of Joiner’s Stock’t T'dols ...</td>\n",
       "      <td>1806-07-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>New_Hampshire_18060715_7</td>\n",
       "      <td>\"170 be sold by order of the Judge of Probate ...</td>\n",
       "      <td>\"170 be fold by order of the Judge of Probate ...</td>\n",
       "      <td>1806-07-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>New_Hampshire_18060715_10</td>\n",
       "      <td>A variety of ELEGANT JEWELRY, and most other a...</td>\n",
       "      <td>A 4 variety of ELEGANT JEWELRY, and moflt othe...</td>\n",
       "      <td>1806-07-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>New_Hampshire_18060715_13</td>\n",
       "      <td>1 &amp; 2 containing 125 acres each, and No. 3, 15...</td>\n",
       "      <td>1 &amp; 2 containg 125 acres each, and No. 3, 150 ...</td>\n",
       "      <td>1806-07-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>New_Hampshire_18060715_14</td>\n",
       "      <td>And your petitioner in fine fays, that it is n...</td>\n",
       "      <td>—And your petitioner in fi&amp; fays, that it is n...</td>\n",
       "      <td>1806-07-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>New_Hampshire_18060715_15</td>\n",
       "      <td>ing, dated June 1st, 1892, agreed to sell to J...</td>\n",
       "      <td>ing, dated June If}, 1892, agreed 10 {zil ove ...</td>\n",
       "      <td>1806-07-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>New_Hampshire_18060715_16</td>\n",
       "      <td>Apply the H. &amp; B. PENHALLOW. NEW STORE. WILLIA...</td>\n",
       "      <td>- Apply te ' H. &amp; B. PENHALLOW. . #¥ NEW-STORE...</td>\n",
       "      <td>1806-07-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>New_Hampshire_18060715_18</td>\n",
       "      <td>Portsmouth, May 27, 1856. MW Daniel Clay’s Est...</td>\n",
       "      <td>Porifmonth, May 27, 1856. MW Danicl Clay’s Est...</td>\n",
       "      <td>1806-07-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>New_Hampshire_18070915_1</td>\n",
       "      <td>Tuesday, September 15, 1807. Trial of Aaron Bu...</td>\n",
       "      <td>TUESDAY, SEPTEMBER 15,1807. TRIAL or AARON BUR...</td>\n",
       "      <td>1807-09-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>New_Hampshire_18070915_3</td>\n",
       "      <td>Warlike or threatening manner, and must displa...</td>\n",
       "      <td>Wwarlike or threatening manner, ahd muit displ...</td>\n",
       "      <td>1807-09-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>New_Hampshire_18070915_4</td>\n",
       "      <td>He would therefore to be furnished with the op...</td>\n",
       "      <td>He wilhgy therefore to be fucnithed with the o...</td>\n",
       "      <td>1807-09-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>New_Hampshire_18070915_7</td>\n",
       "      <td>Unfortunately, Capt. C. brought no papers; and...</td>\n",
       "      <td>Unfortunately Capt. C..brought no papers ; and...</td>\n",
       "      <td>1807-09-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>New_Hampshire_18070915_8</td>\n",
       "      <td>The crew consisted chiefly of deserters from L...</td>\n",
       "      <td>— The crew consisted chiefly of deserters from...</td>\n",
       "      <td>1807-09-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>New_Hampshire_18070915_9</td>\n",
       "      <td>To this order no attention was paid; she conti...</td>\n",
       "      <td>To thi‘% order no attention was paid ; she ron...</td>\n",
       "      <td>1807-09-15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-09T20:49:02.461915Z",
     "start_time": "2026-01-09T20:49:01.923411Z"
    }
   },
   "source": [
    "docColl['context'].compare(docColl_Norm1['context'])"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                     self  \\\n",
       "2       At Westmoreland, Mrs. Sally Lincoln, wife of M...   \n",
       "6       Notice is hereby given to the proprietors of t...   \n",
       "7       ‘ . LO, L, George Frost, Esq. X 30 I 14 2 1 3 ...   \n",
       "9       The Portsmouthaathinna in the Miscellaneous Re...   \n",
       "16       ing, dated June 1st, 1892, agreed to sell to ...   \n",
       "...                                                   ...   \n",
       "131862  Hooray!\"  Thus Jerome S. McWade, in an after-d...   \n",
       "131877  This Is what our Lord has provided for all his...   \n",
       "131880  And there are teachers who are not true teache...   \n",
       "131881  Pursuant to a decree entered in the above styl...   \n",
       "131904  With the town of Grand Junction and other town...   \n",
       "\n",
       "                                                    other  \n",
       "2       At Westmoreland, Mrs. Sally Lincoln, wife of M...  \n",
       "6       Notice is hereby given to the proprietors of t...  \n",
       "7       ‘ . LO, L, George Frost, Esq. X 30 I 14 2 1 3 ...  \n",
       "9       The Portsmouthaathinna in the Miscellaneous Re...  \n",
       "16      ing, dated June 1st, 1892, agreed to sell to J...  \n",
       "...                                                   ...  \n",
       "131862  Hooray!\" Thus Jerome S. McWade, in an after-di...  \n",
       "131877  This Is what our Lord has provided for all his...  \n",
       "131880  And there are teachers who are not true teache...  \n",
       "131881  Pursuant to a decree entered in the above styl...  \n",
       "131904  With the town of Grand Junction and other town...  \n",
       "\n",
       "[15750 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>self</th>\n",
       "      <th>other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>At Westmoreland, Mrs. Sally Lincoln, wife of M...</td>\n",
       "      <td>At Westmoreland, Mrs. Sally Lincoln, wife of M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Notice is hereby given to the proprietors of t...</td>\n",
       "      <td>Notice is hereby given to the proprietors of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>‘ . LO, L, George Frost, Esq. X 30 I 14 2 1 3 ...</td>\n",
       "      <td>‘ . LO, L, George Frost, Esq. X 30 I 14 2 1 3 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Portsmouthaathinna in the Miscellaneous Re...</td>\n",
       "      <td>The Portsmouthaathinna in the Miscellaneous Re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ing, dated June 1st, 1892, agreed to sell to ...</td>\n",
       "      <td>ing, dated June 1st, 1892, agreed to sell to J...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131862</th>\n",
       "      <td>Hooray!\"  Thus Jerome S. McWade, in an after-d...</td>\n",
       "      <td>Hooray!\" Thus Jerome S. McWade, in an after-di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131877</th>\n",
       "      <td>This Is what our Lord has provided for all his...</td>\n",
       "      <td>This Is what our Lord has provided for all his...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131880</th>\n",
       "      <td>And there are teachers who are not true teache...</td>\n",
       "      <td>And there are teachers who are not true teache...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131881</th>\n",
       "      <td>Pursuant to a decree entered in the above styl...</td>\n",
       "      <td>Pursuant to a decree entered in the above styl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131904</th>\n",
       "      <td>With the town of Grand Junction and other town...</td>\n",
       "      <td>With the town of Grand Junction and other town...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15750 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-09T20:49:17.527481Z",
     "start_time": "2026-01-09T20:49:17.524774Z"
    }
   },
   "source": [
    "print(docColl['context'].iloc[2])\n",
    "print(docColl_Norm1['context'].iloc[2])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At Westmoreland, Mrs. Sally Lincoln, wife of Mr. Spencer L. aged 28.  At Henrico, Mrs. Polly Adams, consort On Saturday, the 11th ult. Mr. Joseph Meyer, of Hampstead, was found dead in the road, (his horse standing by him) when oy e g e SMITH & RUST Pocket Book Lost.  \"LOST last Wednesday between 7 and 8 o’clock in the afternoon, either in the Globe Tavern at the Plains, or on the road leading from thence to Portsmouth, a new Red Morocco Pocket Book ; containing some Money, Notes of hand payable to the Subscriber, also, New Hampshire Fire and Marine Certificates, and other papers valuable to none but to the owner—Whoever shall find said Pocket Book, and re- turn it with its contents, with or without the money shall be handsomely rewarded, and the thanks of their humble servant EDWARD. PARRY.  TO BE LET, That Fireproof Store lately improved by Mr. Benjamin Swett, which must be allowed to be the best stand for business, either for English or West- India Goods in this town—Inquire of EDWARD PARRY, Who has a large assortment of the fashionable GOODS, for sale cheap for cash or short credit. July 28.  FOR SALE, A NEW GONDOLA, built of the best materials, and by an experienced workman, forty feet cor- ner piece,—For further particulars enquire of MICHAEL WIGGIN. Newmarket, July 27th, 1807 2. CHAISE.\n",
      "At Westmoreland, Mrs. Sally Lincoln, wife of Mr. Spencer L. aged 28. At Henrico, Mrs. Polly Adams, consort On Saturday, the 11th ult. Mr. Joseph Meyer, of Hampstead, was found dead in the road, (his horse standing by him) when oy e g e SMITH & RUST Pocket Book Lost. \"LOST last Wednesday between 7 and 8 o’clock in the afternoon, either in the Globe Tavern at the Plains, or on the road leading from thence to Portsmouth, a new Red Morocco Pocket Book ; containing some Money, Notes of hand payable to the Subscriber, also, New Hampshire Fire and Marine Certificates, and other papers valuable to none but to the owner—Whoever shall find said Pocket Book, and re- turn it with its contents, with or without the money shall be handsomely rewarded, and the thanks of their humble servant EDWARD. PARRY. TO BE LET, That Fireproof Store lately improved by Mr. Benjamin Swett, which must be allowed to be the best stand for business, either for English or West- India Goods in this town—Inquire of EDWARD PARRY, Who has a large assortment of the fashionable GOODS, for sale cheap for cash or short credit. July 28. FOR SALE, A NEW GONDOLA, built of the best materials, and by an experienced workman, forty feet cor- ner piece,—For further particulars enquire of MICHAEL WIGGIN. Newmarket, July 27th, 1807 2. CHAISE.\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NER\n",
    "We want to identify named-entities before lemmatizing the text, so that we do not lose any entity by \"shrinking\" words to their base forms."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-10T05:13:52.306499Z",
     "start_time": "2026-01-09T21:21:00.262839Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Settings per far runnare su gpu (se possibile)\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "MODEL_NAME = \"impresso-project/ner-stacked-bert-multilingual-light\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "ner_pipeline = pipeline(\n",
    "    model=MODEL_NAME,\n",
    "    tokenizer=tokenizer,\n",
    "    trust_remote_code=True,\n",
    "    device=device)\n",
    "\n",
    "def run_impresso_ner(text_series):\n",
    "    results = []\n",
    "    for text in tqdm(text_series): # tqdm per vedere i progressi nelle ore di run\n",
    "        text_str = str(text)\n",
    "        if not text_str.strip(): # per testi vuoti\n",
    "            results.append([])\n",
    "            continue\n",
    "\n",
    "        words = text_str.split()\n",
    "\n",
    "        try:\n",
    "            entities = ner_pipeline(text_str, tokens=words)\n",
    "            results.append(entities)\n",
    "        except Exception as e:\n",
    "            print(f\"Errore su un documento: {e}\")\n",
    "            results.append([]) # per non farlo bloccare se ha un errore\n",
    "    return results\n",
    "\n",
    "OUTPUT_FILE = \"data/ner_results_cache.parquet\"\n",
    "if os.path.exists(OUTPUT_FILE):\n",
    "    cached_data = pd.read_parquet(OUTPUT_FILE)\n",
    "\n",
    "    docColl_Norm1['ner_entities_context'] = cached_data['ner_entities_context']\n",
    "    docColl_Norm1['ner_entities_ocr'] = cached_data['ner_entities_ocr']\n",
    "\n",
    "else:\n",
    "    # context\n",
    "    docColl_Norm1['ner_entities_context'] = run_impresso_ner(docColl_Norm1['context'])\n",
    "    # OCR\n",
    "    docColl_Norm1['ner_entities_ocr'] = run_impresso_ner(docColl_Norm1['raw_ocr'])\n",
    "    # salvataggio su file esterno\n",
    "    docColl_Norm1[['ner_entities_context', 'ner_entities_ocr']].to_parquet(OUTPUT_FILE)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gabrielepinelli/miniconda3/lib/python3.10/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "Device set to use mps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'data/ner_results_cache.parquet' non trovato. Inizio elaborazione NER (operazione lunga)...\n",
      "Analisi 'context'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131921/131921 [3:57:25<00:00,  9.26it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analisi 'raw_ocr'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131921/131921 [3:55:02<00:00,  9.35it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine! I risultati sono stati salvati in 'data/ner_results_cache.parquet'.\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-10T11:51:40.978787Z",
     "start_time": "2026-01-10T11:51:40.876373Z"
    }
   },
   "cell_type": "code",
   "source": "docColl_Norm1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                         para_id  \\\n",
       "0       New_Hampshire_18070804_1   \n",
       "1       New_Hampshire_18070804_4   \n",
       "2       New_Hampshire_18070804_5   \n",
       "3       New_Hampshire_18070804_8   \n",
       "4       New_Hampshire_18070804_9   \n",
       "...                          ...   \n",
       "131916       Nebraska_19130626_7   \n",
       "131917        Indiana_19170719_6   \n",
       "131918       Kentucky_19110727_5   \n",
       "131919  Rhode_Island_19140626_10   \n",
       "131920        Florida_19150128_7   \n",
       "\n",
       "                                                  context  \\\n",
       "0       Aiscellaneous Repository. From the Albany Regi...   \n",
       "1       Surely he above the rest of his fellow mortals...   \n",
       "2       At Westmoreland, Mrs. Sally Lincoln, wife of M...   \n",
       "3       Upon the correction of this remedy the stomach...   \n",
       "4       Also FOR SALE AS ABOVE, NEW GOODS, STEPHEN HAR...   \n",
       "...                                                   ...   \n",
       "131916  \"Did you?” said Fran politely. “So father grad...   \n",
       "131917  When a boy begins to learn a trade, the \"play ...   \n",
       "131918  It is situated in the valley of the great many...   \n",
       "131919  A PRACTICAL LESSON IN AGRICULTURE, MAY 1708, T...   \n",
       "131920  The plans call for the grading, smoothing, and...   \n",
       "\n",
       "                                                  raw_ocr publication_date  \\\n",
       "0       fAiscellancous Bepogitory. . dvom the Albany R...       1807-08-04   \n",
       "1       Surely he a bove the rest of his fellow mortal...       1807-08-04   \n",
       "2       At Weltmoreland, Mrs. Sally Liacoln, wife ~of ...       1807-08-04   \n",
       "3       tion of this remedy the flomach is invariably ...       1807-08-04   \n",
       "4       *° , ALSO POR SALE AS ABOVE, o NEW-GEODS, - ST...       1807-08-04   \n",
       "...                                                   ...              ...   \n",
       "131916  \"Did you?” said Fran politely. “So father grad...       1913-06-26   \n",
       "131917  When a boy begins to learn a trade, the \"play ...       1917-07-19   \n",
       "131918  It is ftltuaiod In tho val ley of Uio great ni...       1911-07-27   \n",
       "131919  A PRACITICA, ZESSQN 2V AGRICTZ, LEFY 170 RIGHT...       1914-06-26   \n",
       "131920  The plans call for the grading, smoothing and ...       1915-01-28   \n",
       "\n",
       "                                     ner_entities_context  \\\n",
       "0       [{'type': 'org', 'confidence_ner': 0.47, 'inde...   \n",
       "1       [{'type': 'pers', 'confidence_ner': 0.99, 'ind...   \n",
       "2       [{'type': 'loc', 'confidence_ner': 0.96, 'inde...   \n",
       "3       [{'type': 'loc', 'confidence_ner': 0.83, 'inde...   \n",
       "4       [{'type': 'org', 'confidence_ner': 0.47, 'inde...   \n",
       "...                                                   ...   \n",
       "131916  [{'type': 'pers', 'confidence_ner': 0.94, 'ind...   \n",
       "131917  [{'type': 'loc', 'confidence_ner': 0.96, 'inde...   \n",
       "131918  [{'type': 'loc', 'confidence_ner': 0.67, 'inde...   \n",
       "131919                                                 []   \n",
       "131920  [{'type': 'loc', 'confidence_ner': 0.65, 'inde...   \n",
       "\n",
       "                                         ner_entities_ocr  \n",
       "0       [{'type': 'pers', 'confidence_ner': 0.66, 'ind...  \n",
       "1       [{'type': 'pers', 'confidence_ner': 0.98, 'ind...  \n",
       "2       [{'type': 'loc', 'confidence_ner': 0.9, 'index...  \n",
       "3       [{'type': 'loc', 'confidence_ner': 0.92, 'inde...  \n",
       "4       [{'type': 'org', 'confidence_ner': 0.38, 'inde...  \n",
       "...                                                   ...  \n",
       "131916  [{'type': 'pers', 'confidence_ner': 0.94, 'ind...  \n",
       "131917  [{'type': 'loc', 'confidence_ner': 0.95, 'inde...  \n",
       "131918  [{'type': 'loc', 'confidence_ner': 0.83, 'inde...  \n",
       "131919  [{'type': 'org', 'confidence_ner': 0.29, 'inde...  \n",
       "131920  [{'type': 'loc', 'confidence_ner': 0.68, 'inde...  \n",
       "\n",
       "[131921 rows x 6 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>para_id</th>\n",
       "      <th>context</th>\n",
       "      <th>raw_ocr</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>ner_entities_context</th>\n",
       "      <th>ner_entities_ocr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>New_Hampshire_18070804_1</td>\n",
       "      <td>Aiscellaneous Repository. From the Albany Regi...</td>\n",
       "      <td>fAiscellancous Bepogitory. . dvom the Albany R...</td>\n",
       "      <td>1807-08-04</td>\n",
       "      <td>[{'type': 'org', 'confidence_ner': 0.47, 'inde...</td>\n",
       "      <td>[{'type': 'pers', 'confidence_ner': 0.66, 'ind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>New_Hampshire_18070804_4</td>\n",
       "      <td>Surely he above the rest of his fellow mortals...</td>\n",
       "      <td>Surely he a bove the rest of his fellow mortal...</td>\n",
       "      <td>1807-08-04</td>\n",
       "      <td>[{'type': 'pers', 'confidence_ner': 0.99, 'ind...</td>\n",
       "      <td>[{'type': 'pers', 'confidence_ner': 0.98, 'ind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>New_Hampshire_18070804_5</td>\n",
       "      <td>At Westmoreland, Mrs. Sally Lincoln, wife of M...</td>\n",
       "      <td>At Weltmoreland, Mrs. Sally Liacoln, wife ~of ...</td>\n",
       "      <td>1807-08-04</td>\n",
       "      <td>[{'type': 'loc', 'confidence_ner': 0.96, 'inde...</td>\n",
       "      <td>[{'type': 'loc', 'confidence_ner': 0.9, 'index...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>New_Hampshire_18070804_8</td>\n",
       "      <td>Upon the correction of this remedy the stomach...</td>\n",
       "      <td>tion of this remedy the flomach is invariably ...</td>\n",
       "      <td>1807-08-04</td>\n",
       "      <td>[{'type': 'loc', 'confidence_ner': 0.83, 'inde...</td>\n",
       "      <td>[{'type': 'loc', 'confidence_ner': 0.92, 'inde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>New_Hampshire_18070804_9</td>\n",
       "      <td>Also FOR SALE AS ABOVE, NEW GOODS, STEPHEN HAR...</td>\n",
       "      <td>*° , ALSO POR SALE AS ABOVE, o NEW-GEODS, - ST...</td>\n",
       "      <td>1807-08-04</td>\n",
       "      <td>[{'type': 'org', 'confidence_ner': 0.47, 'inde...</td>\n",
       "      <td>[{'type': 'org', 'confidence_ner': 0.38, 'inde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131916</th>\n",
       "      <td>Nebraska_19130626_7</td>\n",
       "      <td>\"Did you?” said Fran politely. “So father grad...</td>\n",
       "      <td>\"Did you?” said Fran politely. “So father grad...</td>\n",
       "      <td>1913-06-26</td>\n",
       "      <td>[{'type': 'pers', 'confidence_ner': 0.94, 'ind...</td>\n",
       "      <td>[{'type': 'pers', 'confidence_ner': 0.94, 'ind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131917</th>\n",
       "      <td>Indiana_19170719_6</td>\n",
       "      <td>When a boy begins to learn a trade, the \"play ...</td>\n",
       "      <td>When a boy begins to learn a trade, the \"play ...</td>\n",
       "      <td>1917-07-19</td>\n",
       "      <td>[{'type': 'loc', 'confidence_ner': 0.96, 'inde...</td>\n",
       "      <td>[{'type': 'loc', 'confidence_ner': 0.95, 'inde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131918</th>\n",
       "      <td>Kentucky_19110727_5</td>\n",
       "      <td>It is situated in the valley of the great many...</td>\n",
       "      <td>It is ftltuaiod In tho val ley of Uio great ni...</td>\n",
       "      <td>1911-07-27</td>\n",
       "      <td>[{'type': 'loc', 'confidence_ner': 0.67, 'inde...</td>\n",
       "      <td>[{'type': 'loc', 'confidence_ner': 0.83, 'inde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131919</th>\n",
       "      <td>Rhode_Island_19140626_10</td>\n",
       "      <td>A PRACTICAL LESSON IN AGRICULTURE, MAY 1708, T...</td>\n",
       "      <td>A PRACITICA, ZESSQN 2V AGRICTZ, LEFY 170 RIGHT...</td>\n",
       "      <td>1914-06-26</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'type': 'org', 'confidence_ner': 0.29, 'inde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131920</th>\n",
       "      <td>Florida_19150128_7</td>\n",
       "      <td>The plans call for the grading, smoothing, and...</td>\n",
       "      <td>The plans call for the grading, smoothing and ...</td>\n",
       "      <td>1915-01-28</td>\n",
       "      <td>[{'type': 'loc', 'confidence_ner': 0.65, 'inde...</td>\n",
       "      <td>[{'type': 'loc', 'confidence_ner': 0.68, 'inde...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131921 rows × 6 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "docColl_ner = docColl_Norm1.copy()\n",
    "docColl_ner[['context', 'raw_ocr', 'ner_entities_context', 'ner_entities_ocr']]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> 2nd step normalization"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def normalize_text2(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    text = text.lower() # lowercase\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text) # punctuations\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() # white spaces again\n",
    "    return text\n",
    "\n",
    "# da testare così,\n",
    "# se va: cambiare anche normalize_text1\n",
    "# se non va: scrivere questo apply(normalize_text2) diviso tra context e raw_ocr e poi riunire tutto su un dataframe unico\n",
    "docColl_Norm2 = docColl_ner[['context', 'raw_ocr']].apply(normalize_text2)\n",
    "#docColl_ocrNorm2 = docColl_ner['raw_ocr'].apply(normalize_text2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lemmatization\n",
    "Placed here to standardize semantically the sentences in the documents"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import spacy\n",
    "\n",
    "try:\n",
    "    if 'nlp' not in locals():\n",
    "        nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "except OSError:\n",
    "    from spacy.cli import download\n",
    "    download(\"en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "\n",
    "# Qui prendiamo i dati dall'ultimo step (docColl_Norm2)\n",
    "# Facciamo una copia per creare il nuovo dataframe 'docColl_Lemm'\n",
    "if 'docColl_Norm2' in locals():\n",
    "    docColl_Lemm = docColl_Norm2.copy()\n",
    "    print(\" DataFrame sorgente: 'docColl_Norm2' trovato e copiato in 'docColl_Lemm'.\")\n",
    "else:\n",
    "    print(\" Errore: 'docColl_Norm2' non trovato in memoria.\")\n",
    "\n",
    "columns_to_process = ['context', 'raw_ocr']\n",
    "\n",
    "print(f\" Avvio lemmatizzazione sulle colonne: {columns_to_process}\")\n",
    "\n",
    "for col in columns_to_process:\n",
    "    if col in docColl_Lemm.columns:\n",
    "        print(f\"\\n--- Elaborazione colonna: '{col}' ---\")\n",
    "        \n",
    "        texts = docColl_Lemm[col].astype(str).tolist()\n",
    "        processed_texts = []\n",
    "\n",
    "        print(f\"Processando {len(texts)} documenti da 'docColl_Norm2'...\")\n",
    "\n",
    "        for doc in nlp.pipe(texts, batch_size=2000, n_process=-1):\n",
    "            lemmas = [token.lemma_ for token in doc if not token.is_space]\n",
    "            processed_texts.append(\" \".join(lemmas))\n",
    "\n",
    "        new_col_name = f\"{col}_lemma\"\n",
    "        docColl_Lemm[new_col_name] = processed_texts\n",
    "\n",
    "        print(f\" Finito! Creata colonna: {new_col_name}\")\n",
    "    else:\n",
    "        print(f\" Errore: Colonna '{col}' non trovata nel dataframe.\")\n",
    "\n",
    "print(\"\\nDataFrame finale: docColl_Lemm\")\n",
    "print(docColl_Lemm[['context', 'context_lemma', 'raw_ocr', 'raw_ocr_lemma']].head())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### N-gram based tokenization\n",
    "Important to place it after normalization, in this tokenization can be integrated a NER-aware part so that \"the tokenization is also entity-guided\""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "# DA INTEGRARE PER FARGLI FARE IL LAVORO ANCHE SULLA COLONNA RAW_OCR\n",
    "def ner_aware_ngram_tokenizer(row, text_col='lemmatized_context', ner_col='ner_entities', n=2):\n",
    "    \"\"\"\n",
    "    1. Prende il testo lemmatizzato.\n",
    "    2. Usa le entità NER per 'incollare' le parole composte (New York -> new_york).\n",
    "    3. Genera N-grams dal testo modificato.\n",
    "    \"\"\"\n",
    "    text = row.get(text_col, \"\")\n",
    "    entities = row.get(ner_col, [])\n",
    "    \n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return []\n",
    "    \n",
    "    # Entity Glueing (Incollaggio Entità)\n",
    "    # Creiamo una versione del testo dove le entità sono unite da underscore.\n",
    "    \n",
    "    # Se abbiamo entità, proviamo a unirle nel testo\n",
    "    if isinstance(entities, list) and len(entities) > 0:\n",
    "        # Ordiniamo per lunghezza decrescente per evitare sostituzioni parziali\n",
    "        try:\n",
    "\n",
    "            entity_texts = []\n",
    "            for ent in entities:\n",
    "                if 'word' in ent:\n",
    "                    entity_texts.append(ent['word'])\n",
    "                elif 'entity_group' in ent:\n",
    "                    entity_texts.append(ent['entity_group'])\n",
    "                elif 'entity' in ent:\n",
    "                    entity_texts.append(ent['entity']) \n",
    "            \n",
    "            for ent_text in sorted(entity_texts, key=len, reverse=True):\n",
    "                clean_ent = ent_text.lower().strip()\n",
    "                if \" \" in clean_ent:\n",
    "                    merged_ent = clean_ent.replace(\" \", \"_\")\n",
    "                    text = text.replace(clean_ent, merged_merged_ent)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    # Tokenization Standard ---\n",
    "    tokens = text.split() \n",
    "    \n",
    "    # Generazione N-grams ---\n",
    "    if len(tokens) < n:\n",
    "        return []\n",
    "        \n",
    "    # Se n=2 (Bigrams): zip(tokens, tokens[1:])\n",
    "    n_grams_tuples = zip(*[tokens[i:] for i in range(n)])\n",
    "    \n",
    "    # Unisce le tuple in stringhe: (\"new_york\", \"is\") -> \"new_york is\"\n",
    "    n_grams_list = [\" \".join(ngram) for ngram in n_grams_tuples]\n",
    "    \n",
    "    return n_grams_list\n",
    "\n",
    "target_key = 'data/document_collection.json'\n",
    "text_column = 'lemmatized_context' \n",
    "ner_column = 'ner_entities' \n",
    "\n",
    "if target_key in dataframes:\n",
    "    print(f\"Initiating N-gram Tokenization (Entity-Aware) on: {target_key}...\")\n",
    "    df = dataframes[target_key]\n",
    "    \n",
    "    if text_column in df.columns and ner_column in df.columns:\n",
    "        \n",
    "        N_VALUE = 2 \n",
    "        \n",
    "        print(f\"Generating {N_VALUE}-grams...\")\n",
    "        \n",
    "        df['ngrams'] = df.apply(\n",
    "            lambda row: ner_aware_ngram_tokenizer(row, text_col=text_column, ner_col=ner_column, n=N_VALUE), \n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        dataframes[target_key] = df\n",
    "        \n",
    "        print(df[['lemmatized_context', 'ngrams']].head())\n",
    "        \n",
    "    else:\n",
    "        print(f\"Error: Columns '{text_column}' or '{ner_column}' missing. Check names.\")\n",
    "else:\n",
    "    print(f\"Error: {target_key} not found.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "da qui dovrebbe uscire il dataframe chiamato docColl_tok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Multi-field Indexing_"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def create_multi_field_index(df):\n",
    "    # The index structure: { field_name: { term: { doc_id: frequency } } }\n",
    "    inverted_index = {\n",
    "        \"raw\": defaultdict(lambda: defaultdict(int)),\n",
    "        \"clean\": defaultdict(lambda: defaultdict(int)),\n",
    "        \"entities\": defaultdict(lambda: defaultdict(int))\n",
    "    }\n",
    "    \n",
    "    # Track document frequency (how many docs a term appears in)\n",
    "    doc_counts = {\n",
    "        \"raw\": defaultdict(int),\n",
    "        \"clean\": defaultdict(int),\n",
    "        \"entities\": defaultdict(int)\n",
    "    }\n",
    "\n",
    "    num_docs = len(df)\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        doc_id = idx # Using dataframe index as Document ID\n",
    "        \n",
    "        # --- Field 1: Raw (from raw_ocr) ---\n",
    "        raw_tokens = str(row.get('raw_ocr', '')).lower().split()\n",
    "        for token in raw_tokens:\n",
    "            inverted_index[\"raw\"][token][doc_id] += 1\n",
    "            \n",
    "        # --- Field 2: Clean (from context / lemmatized_context) ---\n",
    "        clean_tokens = str(row.get('context', '')).lower().split()\n",
    "        for token in clean_tokens:\n",
    "            inverted_index[\"clean\"][token][doc_id] += 1\n",
    "            \n",
    "        # --- Field 3: Entities (from ner_entities) ---\n",
    "        # Extracts only the 'word' or 'entity' text from your NER results\n",
    "        entities_list = row.get('ner_entities', [])\n",
    "        if isinstance(entities_list, list):\n",
    "            for ent in entities_list:\n",
    "                # Handle different key structures found in your screenshots\n",
    "                ent_text = ent.get('word') or ent.get('entity_group') or ent.get('entity')\n",
    "                if ent_text:\n",
    "                    term = ent_text.lower().strip().replace(\" \", \"_\")\n",
    "                    inverted_index[\"entities\"][term][doc_id] += 1\n",
    "\n",
    "    return inverted_index, num_docs\n",
    "\n",
    "# Execute Indexing\n",
    "df_target = dataframes['data/document_collection.json']\n",
    "my_index, total_docs = create_multi_field_index(df_target)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> Indexing con PyTerrier usando un generator"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# qui assumiamo che le celle create dal NER siano oggetti di tipo dizionario\n",
    "def createGenerator(df, context=True):\n",
    "    # context\n",
    "    if context:\n",
    "        for _, row in df.iterrows():\n",
    "            # togliamo lOffset and rOffset\n",
    "            clean_ents = []\n",
    "            for ent in row['ner_entities_context']:\n",
    "                cleaned = {k: v for k, v in ent.items() if k not in ['lOffset', 'rOffset']}\n",
    "                clean_ents.append(cleaned)\n",
    "\n",
    "            search_terms = []\n",
    "            for e in clean_ents:\n",
    "                #search_terms.append(e.get('name', ''))\n",
    "                #search_terms.append(e.get('title', ''))\n",
    "                # da capire se vogliamo che siano searchable, dato che surface contiene già il testo a cui è associata la entity\n",
    "                search_terms.append(e.get('surface', ''))\n",
    "\n",
    "            ent_text = \" \".join(filter(None, search_terms)) # questa riga ha senso solo se prendiamo anche 'name' e 'title'\n",
    "                                                                                           # se no ent_text va assegnato a e.get('surface', ' ')\n",
    "\n",
    "            meta_json = json.dumps(clean_ents) # facciamo diventare tutti i metadati una stringa in forma json (non un oggetto dizionario, proprio una stringa)\n",
    "\n",
    "            yield { # serve per lo stream dei dati quando viene chiamata createGenerator dentro indexer.index(•)\n",
    "                \"docno\": str(row['para_id']),\n",
    "                \"text\": row['context'],\n",
    "                \"entities\": ent_text, # entità searchable\n",
    "                \"entity_json\": meta_json}\n",
    "    # OCR\n",
    "    if not context:\n",
    "        for _, row in df.iterrows():\n",
    "            # togliamo lOffset and rOffset\n",
    "            clean_ents = []\n",
    "            for ent in row['ner_entities_ocr']:\n",
    "                cleaned = {k: v for k, v in ent.items() if k not in ['lOffset', 'rOffset']}\n",
    "                clean_ents.append(cleaned)\n",
    "\n",
    "            search_terms = []\n",
    "            for e in clean_ents:\n",
    "                #search_terms.append(e.get('name', ''))\n",
    "                #search_terms.append(e.get('title', ''))\n",
    "                # da capire se vogliamo che siano searchable, dato che surface contiene già il testo a cui è associata la entity\n",
    "                search_terms.append(e.get('surface', ''))\n",
    "\n",
    "            ent_text = \" \".join(filter(None, search_terms)) # questa riga ha senso solo se prendiamo anche 'name' e 'title'\n",
    "                                                                                           # se no ent_text va assegnato a e.get('surface', ' ')\n",
    "\n",
    "            meta_json = json.dumps(clean_ents) # facciamo diventare tutti i metadati una stringa in forma json (non un oggetto dizionario, proprio una stringa)\n",
    "\n",
    "            yield { # serve per lo stream dei dati quando viene chiamata createGenerator dentro indexer.index(•)\n",
    "                \"docno\": str(row['para_id']),\n",
    "                \"text\": row['raw_ocr'],\n",
    "                \"entities\": ent_text, # entità searchable\n",
    "                \"entity_json\": meta_json}\n",
    "\n",
    "contextIndex_path = 'data/docColl_context-index'\n",
    "ocrIndex_path = 'data/docColl_ocr-index'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if os.path.exists(contextIndex_path):\n",
    "    shutil.rmtree(contextIndex_path)\n",
    "\n",
    "indexerCont = pt.IterDictIndexer(\n",
    "    'entity_index',\n",
    "    fields=['text', 'entities'],\n",
    "    meta={'docno', 'entity_json'})\n",
    "\n",
    "indexrefCont = indexerCont.index(createGenerator(docColl_tok, context=True))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if os.path.exists(ocrIndex_path):\n",
    "    shutil.rmtree(ocrIndex_path)\n",
    "\n",
    "indexerOCR = pt.IterDictIndexer(\n",
    "    'entity_index',\n",
    "    fields=['text', 'entities'],\n",
    "    meta={'docno', 'entity_json'})\n",
    "\n",
    "indexrefOCR = indexerOCR.index(createGenerator(docColl_tok, context=False))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Statistics about the indexed documents"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "indexCont = pt.IndexFactory.of(indexrefCont)\n",
    "stats = indexCont.getCollectionStatistics()\n",
    "print('Index folder:', contextIndex_path)\n",
    "print('Number of documents:', stats.getNumberOfDocuments())\n",
    "print('Number of postings:', stats.getNumberOfPostings())\n",
    "print('Number of tokens:', stats.getNumberOfTokens())\n",
    "print('Number of unique terms:', stats.getNumberOfUniqueTerms())\n",
    "print('Average document length:', stats.getAverageDocumentLength())"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "indexOCR = pt.IndexFactory.of(indexrefOCR)\n",
    "stats = indexOCR.getCollectionStatistics()\n",
    "print('Index folder:', contextIndex_path)\n",
    "print('Number of documents:', stats.getNumberOfDocuments())\n",
    "print('Number of postings:', stats.getNumberOfPostings())\n",
    "print('Number of tokens:', stats.getNumberOfTokens())\n",
    "print('Number of unique terms:', stats.getNumberOfUniqueTerms())\n",
    "print('Average document length:', stats.getAverageDocumentLength())"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Query analysis"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "display(queries.head(10))"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "--> da scrivere commento riguardo l'analisi delle queries"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Qrels analysis"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "display(qrels.sample(10))"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# stats for the qrels\n",
    "# Count how many relevance assessments each query has\n",
    "counts = qrels.groupby(\"query_id\")[\"para_id\"].count()  # group by query id and count documents\n",
    "print('Overall Statistics')\n",
    "print(counts.describe())  # show a summary of the count distribution\n",
    "\n",
    "import matplotlib.pyplot as plt  # plotting library\n",
    "\n",
    "# Plot how many relevance assessments each query received\n",
    "plt.figure()  # create a new figure\n",
    "counts.plot(kind='hist')  # histogram showing distribution of judgment counts\n",
    "plt.xlabel('Number of relevance assessments per query')  # label for x-axis\n",
    "plt.ylabel('Number of queries')  # label for y-axis\n",
    "plt.title('Relevance assessment distribution')  # title of the plot\n",
    "plt.show()  # display the plot\n",
    "\n",
    "# Show the queries with the highest number of relevance assessments\n",
    "counts.sort_values(ascending=False).head()  # top queries by number of judgments\n",
    "\n",
    "# Count how many times each relevance label occurs overall\n",
    "qrels['relevance'].value_counts()  # distribution of relevance scores (e.g., 0, 1, 2, etc.)\n",
    "\n",
    "# Plot the label distribution as a histogram\n",
    "plt.figure()  # create a new figure\n",
    "qrels['relevance'].plot(kind='hist')  # histogram of relevance labels\n",
    "plt.xlabel('Relevance score')  # label for x-axis\n",
    "plt.ylabel('Frequency')  # label for y-axis\n",
    "plt.title('Relevance score distribution')  # title of the plot\n",
    "plt.show()  # display the plot"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "--> commento riguardo l'analisi delle qrels"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Phase I - Topical relevance-based retrieval"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **BM25 Retrieval from raw OCR (baseline 1)**"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "bm25ocr = pt.terrier.Retriever(indexrefOCR, wmodel='BM25', ) # dovremmo usare un BM25F? per dividere i fields di ricerca (secondo me si)\n",
    "res_bm25ocr = bm25ocr.transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **BM25 Retrieval from corrected OCR (baseline 2)**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **BM25 Retrieval from both raw and corrected OCR using RRF formula (baseline 3)**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNUuc82OtGicqd8vHTH8YSN",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DRBwc_amYdiZ"
   },
   "source": [
    "# Download and Inspect the Collection\n",
    "\n",
    "The dataset was created from the Chronicling America collection ‚Äî over 21 million digitized newspaper pages (1756‚Äì1963) curated by the Library of Congress and NEH. They used 39,330 pages (1800‚Äì1920), representing 53 US states, to ensure wide geographic and temporal coverage.\n",
    "\n",
    "Source: https://dl.acm.org/doi/pdf/10.1145/3626772.3657891\n",
    "\n",
    "GitHub: https://github.com/DataScienceUIBK/ChroniclingAmericaQA?tab=readme-ov-file"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%pip install -r requirements.txt\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyterrier as pt\n",
    "import transformers\n",
    "import torch\n",
    "import nltk\n",
    "import spacy"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12366,
     "status": "ok",
     "timestamp": 1762962835550,
     "user": {
      "displayName": "Georgios Peikos",
      "userId": "04834132442165285194"
     },
     "user_tz": -60
    },
    "id": "4xBdfDsPYdLA",
    "outputId": "32103be7-1880-4ccb-ea70-6800e841dec1"
   },
   "source": [
    "import os\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "!curl -L \"https://huggingface.co/datasets/Bhawna/ChroniclingAmericaQA/resolve/main/test.json?download=true\" -o data/test.json\n",
    "!curl -L \"https://huggingface.co/datasets/Bhawna/ChroniclingAmericaQA/resolve/main/train.json?download=true\" -o data/train.json\n",
    "!curl -L \"https://huggingface.co/datasets/Bhawna/ChroniclingAmericaQA/resolve/main/dev.json?download=true\" -o data/validation.json\n",
    "\n",
    "import json\n",
    "\n",
    "files = [\"data/train.json\", \"data/validation.json\", \"data/test.json\"]\n",
    "\n",
    "for path in files:\n",
    "    print(f\"\\n===== {path} =====\")\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            # Read a few hundred characters to see what kind of JSON it is\n",
    "            head = f.read(500)\n",
    "            print(\"Preview of first 500 characters:\\n\")\n",
    "            print(head[:500])\n",
    "        # Try to load only part of the file\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        if isinstance(data, list):\n",
    "            print(f\"\\nLoaded {len(data)} items (list).\")\n",
    "            print(\"Dictionary keys:\", list(data[0].keys()))\n",
    "            print(json.dumps(data[0], indent=2)[:600])\n",
    "        elif isinstance(data, dict):\n",
    "            print(\"\\nTop-level is a dictionary. Keys:\", list(data.keys()))\n",
    "            for k, v in data.items():\n",
    "                if isinstance(v, list):\n",
    "                    print(f\"Key '{k}' contains a list of {len(v)} items.\")\n",
    "                    if v:\n",
    "                        print(\"First item keys:\", list(v[0].keys()))\n",
    "                        print(json.dumps(v[0], indent=2)[:600])\n",
    "                        break\n",
    "        else:\n",
    "            print(f\"Unexpected top-level type: {type(data)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not parse {path} as JSON: {e}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mylmVIP9bu8y"
   },
   "source": [
    "# Create the Document Collection\n",
    "\n",
    "To do that, we create a new json file that contains the 'para_id', 'context', 'raw_ocr', 'publication_date' keys, for all para_id in the collection.\n",
    "\n",
    "para_id: is the id of a paragraph of a news paper page."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17568,
     "status": "ok",
     "timestamp": 1762962853135,
     "user": {
      "displayName": "Georgios Peikos",
      "userId": "04834132442165285194"
     },
     "user_tz": -60
    },
    "id": "nxch4FUUbxRw",
    "outputId": "d86e4179-defd-49d8-8b68-172c577ed825"
   },
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "inputs = [\"data/train.json\", \"data/validation.json\", \"data/test.json\"]\n",
    "output = \"data/document_collection.json\"\n",
    "\n",
    "def load_list_or_empty(path):\n",
    "    if not os.path.exists(path) or os.path.getsize(path) == 0:\n",
    "        print(f\"Skipping {path} because it is missing or empty\")\n",
    "        return []\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        if isinstance(data, list):\n",
    "            return data\n",
    "        print(f\"Skipping {path} because it is not a list at the top level\")\n",
    "        return []\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Skipping {path} because it is not valid JSON\")\n",
    "        return []\n",
    "\n",
    "def project(recs):\n",
    "    out = []\n",
    "    for r in recs:\n",
    "        out.append({\n",
    "            \"para_id\": r.get(\"para_id\", \"\"),\n",
    "            \"context\": r.get(\"context\", \"\"),\n",
    "            \"raw_ocr\": r.get(\"raw_ocr\", \"\"),\n",
    "            \"publication_date\": r.get(\"publication_date\", \"\")\n",
    "        })\n",
    "    return out\n",
    "\n",
    "all_recs = []\n",
    "for p in inputs:\n",
    "    recs = load_list_or_empty(p)\n",
    "    print(f\"Loaded {len(recs)} records from {p}\")\n",
    "    all_recs.extend(project(recs))\n",
    "\n",
    "# deduplicate by para_id keeping the first one seen\n",
    "uniq = {}\n",
    "for rec in all_recs:\n",
    "    pid = rec.get(\"para_id\", \"\")\n",
    "    if pid and pid not in uniq:\n",
    "        uniq[pid] = rec\n",
    "\n",
    "result = list(uniq.values())\n",
    "\n",
    "with open(output, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Wrote {len(result)} records to {output}\")\n",
    "print(json.dumps(result[:3], indent=2))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-9wljtri-XX"
   },
   "source": [
    "## You should check that the collection you have matches that of the paper!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "for path in inputs:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        df_check = pd.read_json(path)\n",
    "        print(f'Shape of {path}: {df_check.shape}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimensions match the ones of the paper at https://github.com/DataScienceUIBK/ChroniclingAmericaQA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "snY9dkltgMts"
   },
   "source": [
    "# Create the Test Queries Data Structure\n",
    "\n",
    "We keep the first 10.000 queries due to memory errors in the free colab version.\n",
    "\n",
    "To be comparable, please keep the top 10.000 queries for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1151,
     "status": "ok",
     "timestamp": 1762962872929,
     "user": {
      "displayName": "Georgios Peikos",
      "userId": "04834132442165285194"
     },
     "user_tz": -60
    },
    "id": "7ZOmr1qBgRxi",
    "outputId": "1a4cbaaa-2813-4814-e0de-aee5aab98f7c"
   },
   "source": [
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "input_file = \"data/test.json\"\n",
    "output_file = \"data/test_queries.json\"\n",
    "\n",
    "# Load the data\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "def clean_question(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    text = re.sub(rf\"[{re.escape(string.punctuation)}]\", \" \", text)  # remove punctuation\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # collapse multiple spaces\n",
    "    return text.strip()\n",
    "\n",
    "# Extract and clean\n",
    "queries = [\n",
    "    {\n",
    "        \"query_id\": item.get(\"query_id\", \"\"),\n",
    "        \"question\": clean_question(item.get(\"question\", \"\")),\n",
    "    }\n",
    "    for item in data\n",
    "]\n",
    "\n",
    "# Sort by query_id (assuming numeric)\n",
    "queries = sorted(queries, key=lambda x: int(x[\"query_id\"]) if str(x[\"query_id\"]).isdigit() else x[\"query_id\"])\n",
    "\n",
    "# Keep only the first 10,000\n",
    "queries = queries[:10000]\n",
    "\n",
    "# Save new JSON\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(queries, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Saved {len(queries)} entries to {output_file}\")\n",
    "print(json.dumps(queries[:3], indent=2))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6NyCV6oqjFS0"
   },
   "source": [
    "# Create the Qrels for the test set"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 742,
     "status": "ok",
     "timestamp": 1762962873672,
     "user": {
      "displayName": "Georgios Peikos",
      "userId": "04834132442165285194"
     },
     "user_tz": -60
    },
    "id": "Lxms9bHpjIcn",
    "outputId": "26e9db71-b590-4f5f-94db-484d857db80c"
   },
   "source": [
    "input_file = \"data/test.json\"\n",
    "qrels_file = \"data/test_qrels.json\"\n",
    "answers_file = \"data/test_query_answers.json\"\n",
    "\n",
    "# Load the data\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Build the qrels file: query_id, iteration=0, para_id, relevance=1\n",
    "qrels = [\n",
    "    {\n",
    "        \"query_id\": item.get(\"query_id\", \"\"),\n",
    "        \"iteration\": 0,\n",
    "        \"para_id\": item.get(\"para_id\", \"\"),\n",
    "        \"relevance\": 1\n",
    "    }\n",
    "    for item in data\n",
    "]\n",
    "\n",
    "# Build the query_answers file: same plus answer and org_answer\n",
    "query_answers = [\n",
    "    {\n",
    "        \"query_id\": item.get(\"query_id\", \"\"),\n",
    "        \"iteration\": 0,\n",
    "        \"para_id\": item.get(\"para_id\", \"\"),\n",
    "        \"relevance\": 1,\n",
    "        \"answer\": item.get(\"answer\", \"\"),\n",
    "        \"org_answer\": item.get(\"org_answer\", \"\")\n",
    "    }\n",
    "    for item in data\n",
    "]\n",
    "\n",
    "# Save both files\n",
    "with open(qrels_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(qrels, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(answers_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(query_answers, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Saved {len(qrels)} entries to {qrels_file}\")\n",
    "print(f\"Saved {len(query_answers)} entries to {answers_file}\")\n",
    "print(\"Sample qrels entry:\", qrels[0])\n",
    "print(\"Sample query_answers entry:\", query_answers[0])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7vkoP010nIF"
   },
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract data from json files"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "input_files = ['data/document_collection.json', 'data/test.json', 'data/test_qrels.json', 'data/test_queries.json', 'data/test_query_answers.json', 'data/train.json', 'data/validation.json']\n",
    "\n",
    "dataframes = {}\n",
    "for input_file in input_files:\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        dataframes[input_file] = pd.read_json(input_file)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dataframes['data/document_collection.json']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dataframes['data/train.json']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: in `data/document_collection.json` the rows are already deduplicated**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Preprocessing_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Linguistic Processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalization\n",
    "We lowercase everything and remove all special characters/tags"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "--> 1st step normalization"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def normalize_text1(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    #text = text.lower()\n",
    "    text = re.sub(r'<[^>]+>', ' ', text) # HTML\n",
    "    # text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() # multiple white spaces\n",
    "    return text\n",
    "\n",
    "# in caso togliessimo la NER vanno tolti i commenti nella funzione qui sopra\n",
    "\n",
    "docColl = dataframes['data/document_collection.json']\n",
    "docColl_contNorm1 = docColl['context'].apply(normalize_text1)\n",
    "docColl_ocrNorm1 = docColl['raw_ocr'].apply(normalize_text1)\n",
    "docColl_Norm1 = docColl.copy()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "docColl_Norm1['context'] = docColl_contNorm1\n",
    "docColl_Norm1['raw_ocr'] = docColl_ocrNorm1\n",
    "docColl_Norm1.head(25)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "docColl['context'].compare(docColl_Norm1['context'])",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(docColl['context'].iloc[2])\n",
    "print(docColl_Norm1['context'].iloc[2])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NER\n",
    "We want to identify named-entities before lemmatizing the text, so that we do not lose any entity by \"shrinking\" words to their base forms."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "MODEL_NAME = \"impresso-project/ner-stacked-bert-multilingual-light\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "ner_pipeline = pipeline(model=MODEL_NAME, tokenizer=tokenizer, trust_remote_code=True, device=\"cpu\")\n",
    "\n",
    "results_cont = []\n",
    "results_ocr = []\n",
    "for index, row in docColl_Norm1.iterrows():\n",
    "    sentence_cont = str(row['context'])\n",
    "    sentence_ocr = str(row['raw_ocr'])\n",
    "    entities_cont = ner_pipeline(sentence_cont, tokens=sentence_cont.split())\n",
    "    entities_ocr = ner_pipeline(sentence_ocr, tokens=sentence_ocr.split())\n",
    "    results_cont.append(entities_cont)\n",
    "    results_ocr.append(entities_ocr)\n",
    "docColl_Norm1['ner_entities_context'] = results_cont\n",
    "docColl_Norm1['ner_entities_ocr'] = results_ocr"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "docColl_ner = docColl_Norm1.copy()\n",
    "docColl_Norm1[['context', 'raw_ocr', 'ner_entities_context', 'ner_entities_ocr']]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "--> 2nd step normalization"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def normalize_text2(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    text = text.lower() # lowercase\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text) # punctuations\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() # white spaces again\n",
    "    return text\n",
    "\n",
    "# da testare cos√¨,\n",
    "# se va: cambiare anche normalize_text1\n",
    "# se non va: scrivere questo apply(normalize_text2) diviso tra context e raw_ocr e poi riunire tutto su un dataframe unico\n",
    "docColl_Norm2 = docColl_ner[['context', 'raw_ocr']].apply(normalize_text2)\n",
    "#docColl_ocrNorm2 = docColl_ner['raw_ocr'].apply(normalize_text2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lemmatization\n",
    "Placed here to standardize semantically the sentences in the documents"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import spacy\n",
    "\n",
    "try:\n",
    "    if 'nlp' not in locals():\n",
    "        nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "except OSError:\n",
    "    from spacy.cli import download\n",
    "    download(\"en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "\n",
    "target_key = 'data/document_collection.json'\n",
    "target_column = 'context' \n",
    "\n",
    "if target_key in dataframes:\n",
    "    print(f\"üöÄ Avvio lemmatizzazione ottimizzata (Batch Processing) su: {target_key}\")\n",
    "    df = dataframes[target_key]\n",
    "    \n",
    "    if target_column in df.columns:\n",
    "        texts = df[target_column].astype(str).tolist()\n",
    "        \n",
    "        processed_texts = []\n",
    "        \n",
    "        print(f\"Elaborazione di {len(texts)} documenti...\")\n",
    "        \n",
    "        for doc in nlp.pipe(texts, batch_size=2000, n_process=-1):\n",
    "            lemmas = [token.lemma_ for token in doc if not token.is_space]\n",
    "            processed_texts.append(\" \".join(lemmas))\n",
    "            \n",
    "        new_col_name = f\"{target_column}_lemma\"\n",
    "        df[new_col_name] = processed_texts\n",
    "        \n",
    "        dataframes[target_key] = df\n",
    "        \n",
    "        print(f\"‚úÖ Finito! Creata colonna: {new_col_name}\")\n",
    "        print(df[[target_column, new_col_name]].head())\n",
    "        \n",
    "    else:\n",
    "        print(f\"Errore: Colonna '{target_column}' non trovata. Controlla il nome esatto.\")\n",
    "else:\n",
    "    print(\"DataFrame non trovato.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### N-gram based tokenization\n",
    "Important to place it after normalization, in this tokenization can be integrated a NER-aware part so that \"the tokenization is also entity-guided\""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "# DA INTEGRARE PER FARGLI FARE IL LAVORO ANCHE SULLA COLONNA RAW_OCR\n",
    "def ner_aware_ngram_tokenizer(row, text_col='lemmatized_context', ner_col='ner_entities', n=2):\n",
    "    \"\"\"\n",
    "    1. Prende il testo lemmatizzato.\n",
    "    2. Usa le entit√† NER per 'incollare' le parole composte (New York -> new_york).\n",
    "    3. Genera N-grams dal testo modificato.\n",
    "    \"\"\"\n",
    "    text = row.get(text_col, \"\")\n",
    "    entities = row.get(ner_col, [])\n",
    "    \n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return []\n",
    "    \n",
    "    # Entity Glueing (Incollaggio Entit√†)\n",
    "    # Creiamo una versione del testo dove le entit√† sono unite da underscore.\n",
    "    \n",
    "    # Se abbiamo entit√†, proviamo a unirle nel testo\n",
    "    if isinstance(entities, list) and len(entities) > 0:\n",
    "        # Ordiniamo per lunghezza decrescente per evitare sostituzioni parziali\n",
    "        try:\n",
    "\n",
    "            entity_texts = []\n",
    "            for ent in entities:\n",
    "                if 'word' in ent:\n",
    "                    entity_texts.append(ent['word'])\n",
    "                elif 'entity_group' in ent:\n",
    "                    entity_texts.append(ent['entity_group'])\n",
    "                elif 'entity' in ent:\n",
    "                    entity_texts.append(ent['entity']) \n",
    "            \n",
    "            for ent_text in sorted(entity_texts, key=len, reverse=True):\n",
    "                clean_ent = ent_text.lower().strip()\n",
    "                if \" \" in clean_ent:\n",
    "                    merged_ent = clean_ent.replace(\" \", \"_\")\n",
    "                    text = text.replace(clean_ent, merged_merged_ent)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    # Tokenization Standard ---\n",
    "    tokens = text.split() \n",
    "    \n",
    "    # Generazione N-grams ---\n",
    "    if len(tokens) < n:\n",
    "        return []\n",
    "        \n",
    "    # Se n=2 (Bigrams): zip(tokens, tokens[1:])\n",
    "    n_grams_tuples = zip(*[tokens[i:] for i in range(n)])\n",
    "    \n",
    "    # Unisce le tuple in stringhe: (\"new_york\", \"is\") -> \"new_york is\"\n",
    "    n_grams_list = [\" \".join(ngram) for ngram in n_grams_tuples]\n",
    "    \n",
    "    return n_grams_list\n",
    "\n",
    "target_key = 'data/document_collection.json'\n",
    "text_column = 'lemmatized_context' \n",
    "ner_column = 'ner_entities' \n",
    "\n",
    "if target_key in dataframes:\n",
    "    print(f\"Initiating N-gram Tokenization (Entity-Aware) on: {target_key}...\")\n",
    "    df = dataframes[target_key]\n",
    "    \n",
    "    if text_column in df.columns and ner_column in df.columns:\n",
    "        \n",
    "        N_VALUE = 2 \n",
    "        \n",
    "        print(f\"Generating {N_VALUE}-grams...\")\n",
    "        \n",
    "        df['ngrams'] = df.apply(\n",
    "            lambda row: ner_aware_ngram_tokenizer(row, text_col=text_column, ner_col=ner_column, n=N_VALUE), \n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        dataframes[target_key] = df\n",
    "        \n",
    "        print(df[['lemmatized_context', 'ngrams']].head())\n",
    "        \n",
    "    else:\n",
    "        print(f\"Error: Columns '{text_column}' or '{ner_column}' missing. Check names.\")\n",
    "else:\n",
    "    print(f\"Error: {target_key} not found.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "da qui dovrebbe uscire il dataframe chiamato docColl_tok"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Multi-field Indexing_"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def create_multi_field_index(df):\n",
    "    # The index structure: { field_name: { term: { doc_id: frequency } } }\n",
    "    inverted_index = {\n",
    "        \"raw\": defaultdict(lambda: defaultdict(int)),\n",
    "        \"clean\": defaultdict(lambda: defaultdict(int)),\n",
    "        \"entities\": defaultdict(lambda: defaultdict(int))\n",
    "    }\n",
    "    \n",
    "    # Track document frequency (how many docs a term appears in)\n",
    "    doc_counts = {\n",
    "        \"raw\": defaultdict(int),\n",
    "        \"clean\": defaultdict(int),\n",
    "        \"entities\": defaultdict(int)\n",
    "    }\n",
    "\n",
    "    num_docs = len(df)\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        doc_id = idx # Using dataframe index as Document ID\n",
    "        \n",
    "        # --- Field 1: Raw (from raw_ocr) ---\n",
    "        raw_tokens = str(row.get('raw_ocr', '')).lower().split()\n",
    "        for token in raw_tokens:\n",
    "            inverted_index[\"raw\"][token][doc_id] += 1\n",
    "            \n",
    "        # --- Field 2: Clean (from context / lemmatized_context) ---\n",
    "        clean_tokens = str(row.get('context', '')).lower().split()\n",
    "        for token in clean_tokens:\n",
    "            inverted_index[\"clean\"][token][doc_id] += 1\n",
    "            \n",
    "        # --- Field 3: Entities (from ner_entities) ---\n",
    "        # Extracts only the 'word' or 'entity' text from your NER results\n",
    "        entities_list = row.get('ner_entities', [])\n",
    "        if isinstance(entities_list, list):\n",
    "            for ent in entities_list:\n",
    "                # Handle different key structures found in your screenshots\n",
    "                ent_text = ent.get('word') or ent.get('entity_group') or ent.get('entity')\n",
    "                if ent_text:\n",
    "                    term = ent_text.lower().strip().replace(\" \", \"_\")\n",
    "                    inverted_index[\"entities\"][term][doc_id] += 1\n",
    "\n",
    "    return inverted_index, num_docs\n",
    "\n",
    "# Execute Indexing\n",
    "df_target = dataframes['data/document_collection.json']\n",
    "my_index, total_docs = create_multi_field_index(df_target)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "--> Indexing con PyTerrier usando un generator"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# qui assumiamo che le celle create dal NER siano oggetti di tipo dizionario\n",
    "def createGenerator(df):\n",
    "    for _, row in df.iterrows():\n",
    "        # togliamo lOffset and rOffset\n",
    "        clean_ents = []\n",
    "        for ent in row['entity_col']:\n",
    "            cleaned = {k: v for k, v in ent.items() if k not in ['lOffset', 'rOffset']}\n",
    "            clean_ents.append(cleaned)\n",
    "\n",
    "        search_terms = []\n",
    "        for e in clean_ents:\n",
    "            #search_terms.append(e.get('name', ''))\n",
    "            #search_terms.append(e.get('title', ''))\n",
    "            # da capire se vogliamo che siano searchable, dato che surface contiene gi√† il testo a cui √® associata la entity\n",
    "            search_terms.append(e.get('surface', ''))\n",
    "\n",
    "        ent_text = \" \".join(filter(None, search_terms)) # questa riga ha senso solo se prendiamo anche 'name' e 'title'\n",
    "                                                                                       # se no ent_text va assegnato a e.get('surface', ' ')\n",
    "\n",
    "        meta_json = json.dumps(clean_ents) # facciamo diventare tutti i metadati una stringa in forma json (non un oggetto dizionario, proprio una stringa)\n",
    "\n",
    "        yield { # serve per lo stream dei dati quando viene chiamata createGenerator dentro indexer.index(‚Ä¢)\n",
    "            \"docno\": str(row['docno']),\n",
    "            \"text\": row['text'],\n",
    "            \"entities\": ent_text, # entit√† searchable\n",
    "            \"entity_json\": meta_json}\n",
    "\n",
    "indexer = pt.IterDictIndexer(\n",
    "    \"entity_index\",\n",
    "    fields=[\"text\", \"entities\"],\n",
    "    meta=[\"docno\", \"entity_json\"])\n",
    "\n",
    "index_ref = indexer.index(createGenerator(docColl_tok))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **BM25 Retrieval from raw OCR (baseline 1)**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **BM25 Retrieval from corrected OCR (baseline 2)**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **BM25 Retrieval from both raw and corrected OCR using RRF formula (baseline 3)**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNUuc82OtGicqd8vHTH8YSN",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

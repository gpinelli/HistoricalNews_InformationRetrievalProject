{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true,
   "authorship_tag": "ABX9TyNUuc82OtGicqd8vHTH8YSN"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Download and Inspect the Collection\n",
    "\n",
    "The dataset was created from the Chronicling America collection — over 21 million digitized newspaper pages (1756–1963) curated by the Library of Congress and NEH. They used 39,330 pages (1800–1920), representing 53 US states, to ensure wide geographic and temporal coverage.\n",
    "\n",
    "Source: https://dl.acm.org/doi/pdf/10.1145/3626772.3657891\n",
    "\n",
    "GitHub: https://github.com/DataScienceUIBK/ChroniclingAmericaQA?tab=readme-ov-file"
   ],
   "metadata": {
    "id": "DRBwc_amYdiZ"
   }
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install -r requirements.txt\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4xBdfDsPYdLA",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762962835550,
     "user_tz": -60,
     "elapsed": 12366,
     "user": {
      "displayName": "Georgios Peikos",
      "userId": "04834132442165285194"
     }
    },
    "outputId": "32103be7-1880-4ccb-ea70-6800e841dec1"
   },
   "source": [
    "import os\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "!curl -L \"https://huggingface.co/datasets/Bhawna/ChroniclingAmericaQA/resolve/main/test.json?download=true\" -o data/test.json\n",
    "!curl -L \"https://huggingface.co/datasets/Bhawna/ChroniclingAmericaQA/resolve/main/train.json?download=true\" -o data/train.json\n",
    "!curl -L \"https://huggingface.co/datasets/Bhawna/ChroniclingAmericaQA/resolve/main/dev.json?download=true\" -o data/validation.json\n",
    "\n",
    "import json\n",
    "\n",
    "files = [\"data/train.json\", \"data/validation.json\", \"data/test.json\"]\n",
    "\n",
    "for path in files:\n",
    "    print(f\"\\n===== {path} =====\")\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            # Read a few hundred characters to see what kind of JSON it is\n",
    "            head = f.read(500)\n",
    "            print(\"Preview of first 500 characters:\\n\")\n",
    "            print(head[:500])\n",
    "        # Try to load only part of the file\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        if isinstance(data, list):\n",
    "            print(f\"\\nLoaded {len(data)} items (list).\")\n",
    "            print(\"Dictionary keys:\", list(data[0].keys()))\n",
    "            print(json.dumps(data[0], indent=2)[:600])\n",
    "        elif isinstance(data, dict):\n",
    "            print(\"\\nTop-level is a dictionary. Keys:\", list(data.keys()))\n",
    "            for k, v in data.items():\n",
    "                if isinstance(v, list):\n",
    "                    print(f\"Key '{k}' contains a list of {len(v)} items.\")\n",
    "                    if v:\n",
    "                        print(\"First item keys:\", list(v[0].keys()))\n",
    "                        print(json.dumps(v[0], indent=2)[:600])\n",
    "                        break\n",
    "        else:\n",
    "            print(f\"Unexpected top-level type: {type(data)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not parse {path} as JSON: {e}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create the Document Collection\n",
    "\n",
    "To do that, we create a new json file that contains the 'para_id', 'context', 'raw_ocr', 'publication_date' keys, for all para_id in the collection.\n",
    "\n",
    "para_id: is the id of a paragraph of a news paper page."
   ],
   "metadata": {
    "id": "mylmVIP9bu8y"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "inputs = [\"data/train.json\", \"data/validation.json\", \"data/test.json\"]\n",
    "output = \"data/document_collection.json\"\n",
    "\n",
    "def load_list_or_empty(path):\n",
    "    if not os.path.exists(path) or os.path.getsize(path) == 0:\n",
    "        print(f\"Skipping {path} because it is missing or empty\")\n",
    "        return []\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        if isinstance(data, list):\n",
    "            return data\n",
    "        print(f\"Skipping {path} because it is not a list at the top level\")\n",
    "        return []\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Skipping {path} because it is not valid JSON\")\n",
    "        return []\n",
    "\n",
    "def project(recs):\n",
    "    out = []\n",
    "    for r in recs:\n",
    "        out.append({\n",
    "            \"para_id\": r.get(\"para_id\", \"\"),\n",
    "            \"context\": r.get(\"context\", \"\"),\n",
    "            \"raw_ocr\": r.get(\"raw_ocr\", \"\"),\n",
    "            \"publication_date\": r.get(\"publication_date\", \"\")\n",
    "        })\n",
    "    return out\n",
    "\n",
    "all_recs = []\n",
    "for p in inputs:\n",
    "    recs = load_list_or_empty(p)\n",
    "    print(f\"Loaded {len(recs)} records from {p}\")\n",
    "    all_recs.extend(project(recs))\n",
    "\n",
    "# deduplicate by para_id keeping the first one seen\n",
    "uniq = {}\n",
    "for rec in all_recs:\n",
    "    pid = rec.get(\"para_id\", \"\")\n",
    "    if pid and pid not in uniq:\n",
    "        uniq[pid] = rec\n",
    "\n",
    "result = list(uniq.values())\n",
    "\n",
    "with open(output, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Wrote {len(result)} records to {output}\")\n",
    "print(json.dumps(result[:3], indent=2))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nxch4FUUbxRw",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762962853135,
     "user_tz": -60,
     "elapsed": 17568,
     "user": {
      "displayName": "Georgios Peikos",
      "userId": "04834132442165285194"
     }
    },
    "outputId": "d86e4179-defd-49d8-8b68-172c577ed825"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## You should check that the collection you have matches that of the paper!"
   ],
   "metadata": {
    "id": "O-9wljtri-XX"
   }
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "for path in inputs:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        df_check = pd.read_json(path)\n",
    "        print(f'Shape of {path}: {df_check.shape}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The dimensions match the ones of the paper at https://github.com/DataScienceUIBK/ChroniclingAmericaQA"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create the Test Queries Data Structure\n",
    "\n",
    "We keep the first 10.000 queries due to memory errors in the free colab version.\n",
    "\n",
    "To be comparable, please keep the top 10.000 queries for evaluation."
   ],
   "metadata": {
    "id": "snY9dkltgMts"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "input_file = \"data/test.json\"\n",
    "output_file = \"data/test_queries.json\"\n",
    "\n",
    "# Load the data\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "def clean_question(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    text = re.sub(rf\"[{re.escape(string.punctuation)}]\", \" \", text)  # remove punctuation\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # collapse multiple spaces\n",
    "    return text.strip()\n",
    "\n",
    "# Extract and clean\n",
    "queries = [\n",
    "    {\n",
    "        \"query_id\": item.get(\"query_id\", \"\"),\n",
    "        \"question\": clean_question(item.get(\"question\", \"\")),\n",
    "    }\n",
    "    for item in data\n",
    "]\n",
    "\n",
    "# Sort by query_id (assuming numeric)\n",
    "queries = sorted(queries, key=lambda x: int(x[\"query_id\"]) if str(x[\"query_id\"]).isdigit() else x[\"query_id\"])\n",
    "\n",
    "# Keep only the first 10,000\n",
    "queries = queries[:10000]\n",
    "\n",
    "# Save new JSON\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(queries, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Saved {len(queries)} entries to {output_file}\")\n",
    "print(json.dumps(queries[:3], indent=2))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ZOmr1qBgRxi",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762962872929,
     "user_tz": -60,
     "elapsed": 1151,
     "user": {
      "displayName": "Georgios Peikos",
      "userId": "04834132442165285194"
     }
    },
    "outputId": "1a4cbaaa-2813-4814-e0de-aee5aab98f7c"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create the Qrels for the test set"
   ],
   "metadata": {
    "id": "6NyCV6oqjFS0"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "input_file = \"data/test.json\"\n",
    "qrels_file = \"data/test_qrels.json\"\n",
    "answers_file = \"data/test_query_answers.json\"\n",
    "\n",
    "# Load the data\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Build the qrels file: query_id, iteration=0, para_id, relevance=1\n",
    "qrels = [\n",
    "    {\n",
    "        \"query_id\": item.get(\"query_id\", \"\"),\n",
    "        \"iteration\": 0,\n",
    "        \"para_id\": item.get(\"para_id\", \"\"),\n",
    "        \"relevance\": 1\n",
    "    }\n",
    "    for item in data\n",
    "]\n",
    "\n",
    "# Build the query_answers file: same plus answer and org_answer\n",
    "query_answers = [\n",
    "    {\n",
    "        \"query_id\": item.get(\"query_id\", \"\"),\n",
    "        \"iteration\": 0,\n",
    "        \"para_id\": item.get(\"para_id\", \"\"),\n",
    "        \"relevance\": 1,\n",
    "        \"answer\": item.get(\"answer\", \"\"),\n",
    "        \"org_answer\": item.get(\"org_answer\", \"\")\n",
    "    }\n",
    "    for item in data\n",
    "]\n",
    "\n",
    "# Save both files\n",
    "with open(qrels_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(qrels, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(answers_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(query_answers, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Saved {len(qrels)} entries to {qrels_file}\")\n",
    "print(f\"Saved {len(query_answers)} entries to {answers_file}\")\n",
    "print(\"Sample qrels entry:\", qrels[0])\n",
    "print(\"Sample query_answers entry:\", query_answers[0])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lxms9bHpjIcn",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762962873672,
     "user_tz": -60,
     "elapsed": 742,
     "user": {
      "displayName": "Georgios Peikos",
      "userId": "04834132442165285194"
     }
    },
    "outputId": "26e9db71-b590-4f5f-94db-484d857db80c"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# Retrieval",
   "metadata": {
    "id": "M7vkoP010nIF"
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Extract data from json files"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "input_files = ['data/document_collection.json', 'data/test.json', 'data/test_qrels.json', 'data/test_queries.json', 'data/test_query_answers.json', 'data/train.json', 'data/validation.json']\n",
    "\n",
    "dataframes = {}\n",
    "for input_file in input_files:\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        dataframes[input_file] = pd.read_json(input_file)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "dataframes['data/document_collection.json']",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "dataframes['data/train.json']",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**NOTE: in `data/document_collection.json` the rows are already deduplicated**"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### _Preprocessing_"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### **Linguistic Processing**"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Normalization\n",
    "We lowercase everything and remove all special characters/tags"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### NER\n",
    "We want to identify named-entities before lemmatizing the text, so that we do not lose any entity by \"shrinking\" words to their base forms."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "MODEL_NAME = \"impresso-project/ner-stacked-bert-multilingual-light\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "ner_pipeline = pipeline(model=MODEL_NAME, tokenizer=tokenizer, trust_remote_code=True, device=\"cpu\")\n",
    "\n",
    "doc_collection = dataframes['data/document_collection.json']\n",
    "\n",
    "test_subset = doc_collection.head(25).copy()\n",
    "\n",
    "results = []\n",
    "for index, row in test_subset.iterrows():\n",
    "    sentence = str(row['context'])\n",
    "    entities = ner_pipeline(sentence, tokens=sentence.split())\n",
    "    results.append(entities)\n",
    "test_subset['ner_entities'] = results"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "test_subset[['context', 'ner_entities']]",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Lemmatization\n",
    "Placed here to standardize semantically the sentences in the documents"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### N-gram based tokenization\n",
    "Important to place it after normalization, in this tokenization can be integrated a NER-aware part so that \"the tokenization is also entity-guided\""
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### _Multi-field Indexing_"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Phase I"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **BM25 Retrieval from raw OCR (baseline 1)**"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **BM25 Retrieval from corrected OCR (baseline 2)**"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **BM25 Retrieval from both raw and corrected OCR using RRF formula (baseline 3)**"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ]
}
